# Research Community Dynamics behind Popular AI Benchmarks

The widespread use of experimental benchmarks in AI research has created new competition and collaboration dynamics
that are still poorly understood. In this work we provide an innovative methodology to explore this dynamics and analyse
the way different entrants in these competitions, from academia to tech giants, behave and react depending on their own or
othersâ€™ achievements. We perform an analysis of twenty five popular benchmarks in AI from Papers With Code, with around
two thousand result entries overall, connected with their underlying research papers. We identify links between researchers
and institutions (i.e., communities) beyond the standard co-authorship relations, and we explore a series of hypotheses
about their behaviour as well as some aggregated results in terms of activity, performance jumps and efficiency. We detect
and characterise the dynamics of research communities at different levels of abstraction, including organisation, affiliation,
trajectories, results and activity. While the results cannot be extrapolated beyond our selection of popular machine learning
benchmarks, the methodology can be extended to other areas of AI or robotics, and combined with bibliometric studies.
