[{"title": "Language Models are Unsupervised Multitask Learners@@@GPT-2 (48 layers, h=1600)", "authors": ["Michael Wick", "Kate Silverstein", "Jean-Baptiste Tristan", "Adam Craig Pocock", "Mark Johnson"], "affiliations": ["Oracle Corporation", "Oracle Corporation", "Boston College", "Oracle Corporation", "Oracle Corporation"], "paper_date": "2019-02-14", "metric": "0.93"}, {"title": "Dynamic Evaluation of Transformer Language Models@@@Transformer-XL (24 layers, RMS dynamic eval, decay)", "authors": ["Ben Krause", "Emmanuel Kahembwe", "Iain Murray", "Steve Renals"], "affiliations": ["University of Edinburgh", "University of Edinburgh", "University of Edinburgh", "University of Edinburgh"], "paper_date": "2019-04-17", "metric": "0.940"}, {"title": "Accessing Higher-level Representations in Sequential Transformers with Feedback Memory@@@Feedback Transformer", "authors": ["Angela Fan", "Thibaut Lavril", "Edouard Grave", "Armand Joulin", "Sainbayar Sukhbaatar"], "affiliations": ["Facebook", "", "", "", ""], "paper_date": "2020-02-21", "metric": "0.96"}, {"title": "Improving Transformer Models by Reordering their Sublayers@@@Sandwich Transformer (adaptive span)", "authors": ["Ofir Press", "Noah A. Smith", "Omer Levy"], "affiliations": ["University of Washington", "Allen Institute for Artificial Intelligence", "University of Washington"], "paper_date": "2019-11-10", "metric": "0.968"}, {"title": "Compressive Transformers for Long-Range Sequence Modelling@@@Compressive Transformer (24 layers)", "authors": ["Jack W. Rae", "Anna Potapenko", "Siddhant M. Jayakumar", "Timothy P. Lillicrap"], "affiliations": ["", "", "", ""], "paper_date": "2019-11-13", "metric": "0.97"}, {"title": "Adaptive Attention Span in Transformers@@@Transformer (24 layers, 8k adaptive span)", "authors": ["Sainbayar Sukhbaatar", "Edouard Grave", "Piotr Bojanowski", "Armand Joulin"], "affiliations": ["Facebook", "Facebook", "Facebook", "Facebook"], "paper_date": "2019-05-19", "metric": "0.98"}, {"title": "Augmenting Self-attention with Persistent Memory@@@All-attention network (36 layers)", "authors": ["Sainbayar Sukhbaatar", "Edouard Grave", "Guillaume Lample", "Herv\u00e9 J\u00e9gou", "Armand Joulin"], "affiliations": ["Facebook", "Facebook", "Facebook", "Facebook", "Facebook"], "paper_date": "2019-07-02", "metric": "0.98"}, {"title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context@@@Transformer-XL (24 layers)", "authors": ["Zihang Dai", "Zhilin Yang", "Yiming Yang", "Jaime G. Carbonell", "Quoc V. Le", "Ruslan Salakhutdinov"], "affiliations": ["Carnegie Mellon University", "Carnegie Mellon University", "Carnegie Mellon University", "Carnegie Mellon University", "Carnegie Mellon University", "Google"], "paper_date": "2019-01-09", "metric": "0.99"}, {"title": "Longformer: The Long-Document Transformer@@@Longformer (30 layers, h=512)", "authors": ["Iz Beltagy", "Matthew E. Peters", "Arman Cohan"], "affiliations": ["Allen Institute for Artificial Intelligence", "Allen Institute for Artificial Intelligence", "Allen Institute for Artificial Intelligence"], "paper_date": "2020-04-10", "metric": "0.99"}, {"title": "Generating Long Sequences with Sparse Transformers@@@Sparse Transformer (30 layers, fixed attn)", "authors": ["Rewon Child", "Scott Gray", "Alec Radford", "Ilya Sutskever"], "affiliations": ["", "", "", ""], "paper_date": "2019-04-23", "metric": "0.99"}, {"title": "Efficient Content-Based Sparse Attention with Routing Transformers@@@Routing Transformer (12 layers)", "authors": ["Aurko Roy", "Mohammad Saffar", "Mohammad Taghi Saffar", "David Grangier", "Ashish Vaswani"], "affiliations": ["Google", "", "", "", ""], "paper_date": "2020-03-12", "metric": "0.99"}, {"title": "Longformer: The Long-Document Transformer@@@Longformer (12 layers, h=512)", "authors": ["Iz Beltagy", "Matthew E. Peters", "Arman Cohan"], "affiliations": ["Allen Institute for Artificial Intelligence", "Allen Institute for Artificial Intelligence", "Allen Institute for Artificial Intelligence"], "paper_date": "2020-04-10", "metric": "1.00"}, {"title": "Augmenting Self-attention with Persistent Memory@@@All-attention network (18 layers)", "authors": ["Sainbayar Sukhbaatar", "Edouard Grave", "Guillaume Lample", "Herv\u00e9 J\u00e9gou", "Armand Joulin"], "affiliations": ["Facebook", "Facebook", "Facebook", "Facebook", "Facebook"], "paper_date": "2019-07-02", "metric": "1.01"}, {"title": "Adaptive Attention Span in Transformers@@@Transformer (12 layers, 8k adaptive span)", "authors": ["Sainbayar Sukhbaatar", "Edouard Grave", "Piotr Bojanowski", "Armand Joulin"], "affiliations": ["Facebook", "Facebook", "Facebook", "Facebook"], "paper_date": "2019-05-19", "metric": "1.02"}, {"title": "BP-Transformer: Modelling Long-Range Context via Binary Partitioning@@@BP-Transformer (12 layers)", "authors": ["Zihao Ye", "Qipeng Guo", "Quan Gan", "Xipeng Qiu", "Zheng Zhang"], "affiliations": ["", "", "", "", ""], "paper_date": "2019-11-11", "metric": "1.02"}, {"title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context@@@Transformer-XL (18 layers)", "authors": ["Zihang Dai", "Zhilin Yang", "Yiming Yang", "Jaime G. Carbonell", "Quoc V. Le", "Ruslan Salakhutdinov"], "affiliations": ["Carnegie Mellon University", "Carnegie Mellon University", "Carnegie Mellon University", "Carnegie Mellon University", "Carnegie Mellon University", "Google"], "paper_date": "2019-01-09", "metric": "1.03"}, {"title": "Character-Level Language Modeling with Deeper Self-Attention@@@Transformer (64 layers)", "authors": ["Rami Al-Rfou", "Dokook Choe", "Noah Constant", "Mandy Guo", "Llion Jones"], "affiliations": ["Google", "Google", "Google", "Google", "Google"], "paper_date": "2018-08-09", "metric": "1.06"}, {"title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context@@@Transformer-XL (12 layers)", "authors": ["Zihang Dai", "Zhilin Yang", "Yiming Yang", "Jaime G. Carbonell", "Quoc V. Le", "Ruslan Salakhutdinov"], "affiliations": ["Carnegie Mellon University", "Carnegie Mellon University", "Carnegie Mellon University", "Carnegie Mellon University", "Carnegie Mellon University", "Google"], "paper_date": "2019-01-09", "metric": "1.06"}, {"title": "Single Headed Attention RNN: Stop Thinking With Your Head@@@SHA-RNN (4 layers, h=1024, attention head per layer)", "authors": ["Stephen Merity"], "affiliations": [""], "paper_date": "2019-11-26", "metric": "1.068"}, {"title": "Single Headed Attention RNN: Stop Thinking With Your Head@@@SHA-RNN (4 layers, h=1024, single attention head)", "authors": ["Stephen Merity"], "affiliations": [""], "paper_date": "2019-11-26", "metric": "1.076"}, {"title": "Character-Level Language Modeling with Deeper Self-Attention@@@Transformer (12 layers)", "authors": ["Rami Al-Rfou", "Dokook Choe", "Noah Constant", "Mandy Guo", "Llion Jones"], "affiliations": ["Google", "Google", "Google", "Google", "Google"], "paper_date": "2018-08-09", "metric": "1.11"}, {"title": "Mogrifier LSTM@@@Mogrifier LSTM", "authors": ["G\u00e1bor Melis", "Tom\u00e1s Kocisk\u00fd", "Tom\u00e1\u0161 Ko\u010disk\u00fd", "Phil Blunsom"], "affiliations": ["Google", "Google", "Google", "Google"], "paper_date": "2019-09-04", "metric": "1.146"}, {"title": "Mogrifier LSTM@@@LSTM", "authors": ["G\u00e1bor Melis", "Tom\u00e1s Kocisk\u00fd", "Tom\u00e1\u0161 Ko\u010disk\u00fd", "Phil Blunsom"], "affiliations": ["Google", "Google", "Google", "Google"], "paper_date": "2019-09-04", "metric": "1.195"}, {"title": "Cluster-Former: Clustering-based Sparse Transformer for Long-Range Dependency Encoding@@@Cluster-Former (#C=512)", "authors": ["Shuohang Wang", "Luowei Zhou", "Zhe Gan", "Yen-Chun Chen", "Yuwei Fang", "Siqi Sun", "Yu Cheng", "Jingjing Liu"], "affiliations": ["Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft"], "paper_date": "2020-09-13", "metric": "1.22"}, {"title": "An Analysis of Neural Language Modeling at Multiple Scales@@@AWD-LSTM (3 layers)", "authors": ["Stephen Merity", "Nitish Shirish Keskar", "Richard Socher"], "affiliations": ["", "", ""], "paper_date": "2018-03-22", "metric": "1.232"}, {"title": "Multiplicative LSTM for sequence modelling@@@Large mLSTM", "authors": ["Ben Krause", "Liang Lu", "Iain Murray", "Steve Renals"], "affiliations": ["", "", "", ""], "paper_date": "2016-09-26", "metric": "1.24"}, {"title": "Fast-Slow Recurrent Neural Networks@@@Large FS-LSTM-4", "authors": ["Asier Mujika", "Florian Meier", "Angelika Steger"], "affiliations": ["ETH Zurich", "ETH Zurich", "ETH Zurich"], "paper_date": "2017-05-24", "metric": " 1.25"}, {"title": "Recurrent Highway Networks@@@Recurrent Highway Networks", "authors": ["Julian G. Zilly", "Rupesh Kumar Srivastava", "Jan Koutn\u00edk", "Juergen Schmidhuber", "J\u00fcrgen Schmidhuber"], "affiliations": ["ETH Zurich", "Dalle Molle Institute for Artificial Intelligence Research", "Dalle Molle Institute for Artificial Intelligence Research", "Dalle Molle Institute for Artificial Intelligence Research", "Dalle Molle Institute for Artificial Intelligence Research"], "paper_date": "2016-07-12", "metric": "1.27"}, {"title": "Neural Machine Translation in Linear Time@@@ByteNet", "authors": ["Nal Kalchbrenner", "Lasse Espeholt", "Karen Simonyan", "Aaron van den Oord", "Alex Graves", "Koray Kavukcuoglu"], "affiliations": ["", "", "", "", "", ""], "paper_date": "2016-10-31", "metric": "1.31"}, {"title": "Hierarchical Multiscale Recurrent Neural Networks@@@LN HM-LSTM", "authors": ["Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio"], "affiliations": ["Universit\u00e9 de Montr\u00e9al", "Universit\u00e9 de Montr\u00e9al", "Universit\u00e9 de Montr\u00e9al"], "paper_date": "2016-09-06", "metric": "1.32"}, {"title": "Single Headed Attention RNN: Stop Thinking With Your Head@@@SHA-LSTM (4 layers, h=1024, no attention head)", "authors": ["Stephen Merity"], "affiliations": [""], "paper_date": "2019-11-26", "metric": "1.33"}, {"title": "HyperNetworks@@@Hypernetworks", "authors": ["David Ha", "Andrew M. Dai", "Quoc V. Le"], "affiliations": ["Google", "Google", "Google"], "paper_date": "2016-09-27", "metric": "1.34"}, {"title": "Generating Sequences With Recurrent Neural Networks@@@LSTM (7 layers)", "authors": ["Alex Graves"], "affiliations": ["University of Toronto"], "paper_date": "2013-08-04", "metric": "1.67"}]