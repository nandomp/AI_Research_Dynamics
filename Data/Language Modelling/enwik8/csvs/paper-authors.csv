"title","authors","affiliations","paper_date","metric","year"
"Language Models are Unsupervised Multitask Learners@@@GPT-2 (48 layers, h=1600)","Michael Wick","Oracle Corporation",2019-02-14,"0.93",2019
"Language Models are Unsupervised Multitask Learners@@@GPT-2 (48 layers, h=1600)","Kate Silverstein","Oracle Corporation",2019-02-14,"0.93",2019
"Language Models are Unsupervised Multitask Learners@@@GPT-2 (48 layers, h=1600)","Jean-Baptiste Tristan","Boston College",2019-02-14,"0.93",2019
"Language Models are Unsupervised Multitask Learners@@@GPT-2 (48 layers, h=1600)","Adam Craig Pocock","Oracle Corporation",2019-02-14,"0.93",2019
"Language Models are Unsupervised Multitask Learners@@@GPT-2 (48 layers, h=1600)","Mark Johnson","Oracle Corporation",2019-02-14,"0.93",2019
"Dynamic Evaluation of Transformer Language Models@@@Transformer-XL (24 layers, RMS dynamic eval, decay)","Ben Krause","University of Edinburgh",2019-04-17,"0.940",2019
"Dynamic Evaluation of Transformer Language Models@@@Transformer-XL (24 layers, RMS dynamic eval, decay)","Emmanuel Kahembwe","University of Edinburgh",2019-04-17,"0.940",2019
"Dynamic Evaluation of Transformer Language Models@@@Transformer-XL (24 layers, RMS dynamic eval, decay)","Iain Murray","University of Edinburgh",2019-04-17,"0.940",2019
"Dynamic Evaluation of Transformer Language Models@@@Transformer-XL (24 layers, RMS dynamic eval, decay)","Steve Renals","University of Edinburgh",2019-04-17,"0.940",2019
"Accessing Higher-level Representations in Sequential Transformers with Feedback Memory@@@Feedback Transformer","Angela Fan","Facebook",2020-02-21,"0.96",2020
"Accessing Higher-level Representations in Sequential Transformers with Feedback Memory@@@Feedback Transformer","Thibaut Lavril","",2020-02-21,"0.96",2020
"Accessing Higher-level Representations in Sequential Transformers with Feedback Memory@@@Feedback Transformer","Edouard Grave","",2020-02-21,"0.96",2020
"Accessing Higher-level Representations in Sequential Transformers with Feedback Memory@@@Feedback Transformer","Armand Joulin","",2020-02-21,"0.96",2020
"Accessing Higher-level Representations in Sequential Transformers with Feedback Memory@@@Feedback Transformer","Sainbayar Sukhbaatar","",2020-02-21,"0.96",2020
"Improving Transformer Models by Reordering their Sublayers@@@Sandwich Transformer (adaptive span)","Ofir Press","University of Washington",2019-11-10,"0.968",2019
"Improving Transformer Models by Reordering their Sublayers@@@Sandwich Transformer (adaptive span)","Noah A. Smith","Allen Institute for Artificial Intelligence",2019-11-10,"0.968",2019
"Improving Transformer Models by Reordering their Sublayers@@@Sandwich Transformer (adaptive span)","Omer Levy","University of Washington",2019-11-10,"0.968",2019
"Compressive Transformers for Long-Range Sequence Modelling@@@Compressive Transformer (24 layers)","Jack W. Rae","",2019-11-13,"0.97",2019
"Compressive Transformers for Long-Range Sequence Modelling@@@Compressive Transformer (24 layers)","Anna Potapenko","",2019-11-13,"0.97",2019
"Compressive Transformers for Long-Range Sequence Modelling@@@Compressive Transformer (24 layers)","Siddhant M. Jayakumar","",2019-11-13,"0.97",2019
"Compressive Transformers for Long-Range Sequence Modelling@@@Compressive Transformer (24 layers)","Timothy P. Lillicrap","",2019-11-13,"0.97",2019
"Adaptive Attention Span in Transformers@@@Transformer (24 layers, 8k adaptive span)","Sainbayar Sukhbaatar","Facebook",2019-05-19,"0.98",2019
"Adaptive Attention Span in Transformers@@@Transformer (24 layers, 8k adaptive span)","Edouard Grave","Facebook",2019-05-19,"0.98",2019
"Adaptive Attention Span in Transformers@@@Transformer (24 layers, 8k adaptive span)","Piotr Bojanowski","Facebook",2019-05-19,"0.98",2019
"Adaptive Attention Span in Transformers@@@Transformer (24 layers, 8k adaptive span)","Armand Joulin","Facebook",2019-05-19,"0.98",2019
"Augmenting Self-attention with Persistent Memory@@@All-attention network (36 layers)","Sainbayar Sukhbaatar","Facebook",2019-07-02,"0.98",2019
"Augmenting Self-attention with Persistent Memory@@@All-attention network (36 layers)","Edouard Grave","Facebook",2019-07-02,"0.98",2019
"Augmenting Self-attention with Persistent Memory@@@All-attention network (36 layers)","Guillaume Lample","Facebook",2019-07-02,"0.98",2019
"Augmenting Self-attention with Persistent Memory@@@All-attention network (36 layers)","Hervé Jégou","Facebook",2019-07-02,"0.98",2019
"Augmenting Self-attention with Persistent Memory@@@All-attention network (36 layers)","Armand Joulin","Facebook",2019-07-02,"0.98",2019
"Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context@@@Transformer-XL (24 layers)","Zihang Dai","Carnegie Mellon University",2019-01-09,"0.99",2019
"Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context@@@Transformer-XL (24 layers)","Zhilin Yang","Carnegie Mellon University",2019-01-09,"0.99",2019
"Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context@@@Transformer-XL (24 layers)","Yiming Yang","Carnegie Mellon University",2019-01-09,"0.99",2019
"Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context@@@Transformer-XL (24 layers)","Jaime G. Carbonell","Carnegie Mellon University",2019-01-09,"0.99",2019
"Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context@@@Transformer-XL (24 layers)","Quoc V. Le","Carnegie Mellon University",2019-01-09,"0.99",2019
"Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context@@@Transformer-XL (24 layers)","Ruslan Salakhutdinov","Google",2019-01-09,"0.99",2019
"Longformer: The Long-Document Transformer@@@Longformer (30 layers, h=512)","Iz Beltagy","Allen Institute for Artificial Intelligence",2020-04-10,"0.99",2020
"Longformer: The Long-Document Transformer@@@Longformer (30 layers, h=512)","Matthew E. Peters","Allen Institute for Artificial Intelligence",2020-04-10,"0.99",2020
"Longformer: The Long-Document Transformer@@@Longformer (30 layers, h=512)","Arman Cohan","Allen Institute for Artificial Intelligence",2020-04-10,"0.99",2020
"Generating Long Sequences with Sparse Transformers@@@Sparse Transformer (30 layers, fixed attn)","Rewon Child","",2019-04-23,"0.99",2019
"Generating Long Sequences with Sparse Transformers@@@Sparse Transformer (30 layers, fixed attn)","Scott Gray","",2019-04-23,"0.99",2019
"Generating Long Sequences with Sparse Transformers@@@Sparse Transformer (30 layers, fixed attn)","Alec Radford","",2019-04-23,"0.99",2019
"Generating Long Sequences with Sparse Transformers@@@Sparse Transformer (30 layers, fixed attn)","Ilya Sutskever","",2019-04-23,"0.99",2019
"Efficient Content-Based Sparse Attention with Routing Transformers@@@Routing Transformer (12 layers)","Aurko Roy","Google",2020-03-12,"0.99",2020
"Efficient Content-Based Sparse Attention with Routing Transformers@@@Routing Transformer (12 layers)","Mohammad Saffar","",2020-03-12,"0.99",2020
"Efficient Content-Based Sparse Attention with Routing Transformers@@@Routing Transformer (12 layers)","Mohammad Taghi Saffar","",2020-03-12,"0.99",2020
"Efficient Content-Based Sparse Attention with Routing Transformers@@@Routing Transformer (12 layers)","David Grangier","",2020-03-12,"0.99",2020
"Efficient Content-Based Sparse Attention with Routing Transformers@@@Routing Transformer (12 layers)","Ashish Vaswani","",2020-03-12,"0.99",2020
"Longformer: The Long-Document Transformer@@@Longformer (12 layers, h=512)","Iz Beltagy","Allen Institute for Artificial Intelligence",2020-04-10,"1.00",2020
"Longformer: The Long-Document Transformer@@@Longformer (12 layers, h=512)","Matthew E. Peters","Allen Institute for Artificial Intelligence",2020-04-10,"1.00",2020
"Longformer: The Long-Document Transformer@@@Longformer (12 layers, h=512)","Arman Cohan","Allen Institute for Artificial Intelligence",2020-04-10,"1.00",2020
"Augmenting Self-attention with Persistent Memory@@@All-attention network (18 layers)","Sainbayar Sukhbaatar","Facebook",2019-07-02,"1.01",2019
"Augmenting Self-attention with Persistent Memory@@@All-attention network (18 layers)","Edouard Grave","Facebook",2019-07-02,"1.01",2019
"Augmenting Self-attention with Persistent Memory@@@All-attention network (18 layers)","Guillaume Lample","Facebook",2019-07-02,"1.01",2019
"Augmenting Self-attention with Persistent Memory@@@All-attention network (18 layers)","Hervé Jégou","Facebook",2019-07-02,"1.01",2019
"Augmenting Self-attention with Persistent Memory@@@All-attention network (18 layers)","Armand Joulin","Facebook",2019-07-02,"1.01",2019
"Adaptive Attention Span in Transformers@@@Transformer (12 layers, 8k adaptive span)","Sainbayar Sukhbaatar","Facebook",2019-05-19,"1.02",2019
"Adaptive Attention Span in Transformers@@@Transformer (12 layers, 8k adaptive span)","Edouard Grave","Facebook",2019-05-19,"1.02",2019
"Adaptive Attention Span in Transformers@@@Transformer (12 layers, 8k adaptive span)","Piotr Bojanowski","Facebook",2019-05-19,"1.02",2019
"Adaptive Attention Span in Transformers@@@Transformer (12 layers, 8k adaptive span)","Armand Joulin","Facebook",2019-05-19,"1.02",2019
"BP-Transformer: Modelling Long-Range Context via Binary Partitioning@@@BP-Transformer (12 layers)","Zihao Ye","",2019-11-11,"1.02",2019
"BP-Transformer: Modelling Long-Range Context via Binary Partitioning@@@BP-Transformer (12 layers)","Qipeng Guo","",2019-11-11,"1.02",2019
"BP-Transformer: Modelling Long-Range Context via Binary Partitioning@@@BP-Transformer (12 layers)","Quan Gan","",2019-11-11,"1.02",2019
"BP-Transformer: Modelling Long-Range Context via Binary Partitioning@@@BP-Transformer (12 layers)","Xipeng Qiu","",2019-11-11,"1.02",2019
"BP-Transformer: Modelling Long-Range Context via Binary Partitioning@@@BP-Transformer (12 layers)","Zheng Zhang","",2019-11-11,"1.02",2019
"Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context@@@Transformer-XL (18 layers)","Zihang Dai","Carnegie Mellon University",2019-01-09,"1.03",2019
"Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context@@@Transformer-XL (18 layers)","Zhilin Yang","Carnegie Mellon University",2019-01-09,"1.03",2019
"Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context@@@Transformer-XL (18 layers)","Yiming Yang","Carnegie Mellon University",2019-01-09,"1.03",2019
"Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context@@@Transformer-XL (18 layers)","Jaime G. Carbonell","Carnegie Mellon University",2019-01-09,"1.03",2019
"Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context@@@Transformer-XL (18 layers)","Quoc V. Le","Carnegie Mellon University",2019-01-09,"1.03",2019
"Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context@@@Transformer-XL (18 layers)","Ruslan Salakhutdinov","Google",2019-01-09,"1.03",2019
"Character-Level Language Modeling with Deeper Self-Attention@@@Transformer (64 layers)","Rami Al-Rfou","Google",2018-08-09,"1.06",2018
"Character-Level Language Modeling with Deeper Self-Attention@@@Transformer (64 layers)","Dokook Choe","Google",2018-08-09,"1.06",2018
"Character-Level Language Modeling with Deeper Self-Attention@@@Transformer (64 layers)","Noah Constant","Google",2018-08-09,"1.06",2018
"Character-Level Language Modeling with Deeper Self-Attention@@@Transformer (64 layers)","Mandy Guo","Google",2018-08-09,"1.06",2018
"Character-Level Language Modeling with Deeper Self-Attention@@@Transformer (64 layers)","Llion Jones","Google",2018-08-09,"1.06",2018
"Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context@@@Transformer-XL (12 layers)","Zihang Dai","Carnegie Mellon University",2019-01-09,"1.06",2019
"Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context@@@Transformer-XL (12 layers)","Zhilin Yang","Carnegie Mellon University",2019-01-09,"1.06",2019
"Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context@@@Transformer-XL (12 layers)","Yiming Yang","Carnegie Mellon University",2019-01-09,"1.06",2019
"Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context@@@Transformer-XL (12 layers)","Jaime G. Carbonell","Carnegie Mellon University",2019-01-09,"1.06",2019
"Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context@@@Transformer-XL (12 layers)","Quoc V. Le","Carnegie Mellon University",2019-01-09,"1.06",2019
"Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context@@@Transformer-XL (12 layers)","Ruslan Salakhutdinov","Google",2019-01-09,"1.06",2019
"Single Headed Attention RNN: Stop Thinking With Your Head@@@SHA-RNN (4 layers, h=1024, attention head per layer)","Stephen Merity","",2019-11-26,"1.068",2019
"Single Headed Attention RNN: Stop Thinking With Your Head@@@SHA-RNN (4 layers, h=1024, single attention head)","Stephen Merity","",2019-11-26,"1.076",2019
"Character-Level Language Modeling with Deeper Self-Attention@@@Transformer (12 layers)","Rami Al-Rfou","Google",2018-08-09,"1.11",2018
"Character-Level Language Modeling with Deeper Self-Attention@@@Transformer (12 layers)","Dokook Choe","Google",2018-08-09,"1.11",2018
"Character-Level Language Modeling with Deeper Self-Attention@@@Transformer (12 layers)","Noah Constant","Google",2018-08-09,"1.11",2018
"Character-Level Language Modeling with Deeper Self-Attention@@@Transformer (12 layers)","Mandy Guo","Google",2018-08-09,"1.11",2018
"Character-Level Language Modeling with Deeper Self-Attention@@@Transformer (12 layers)","Llion Jones","Google",2018-08-09,"1.11",2018
"Mogrifier LSTM@@@Mogrifier LSTM","Gábor Melis","Google",2019-09-04,"1.146",2019
"Mogrifier LSTM@@@Mogrifier LSTM","Tomás Kociský","Google",2019-09-04,"1.146",2019
"Mogrifier LSTM@@@Mogrifier LSTM","Tomá Kociský","Google",2019-09-04,"1.146",2019
"Mogrifier LSTM@@@Mogrifier LSTM","Phil Blunsom","Google",2019-09-04,"1.146",2019
"Mogrifier LSTM@@@LSTM","Gábor Melis","Google",2019-09-04,"1.195",2019
"Mogrifier LSTM@@@LSTM","Tomás Kociský","Google",2019-09-04,"1.195",2019
"Mogrifier LSTM@@@LSTM","Tomá Kociský","Google",2019-09-04,"1.195",2019
"Mogrifier LSTM@@@LSTM","Phil Blunsom","Google",2019-09-04,"1.195",2019
"Cluster-Former: Clustering-based Sparse Transformer for Long-Range Dependency Encoding@@@Cluster-Former (#C=512)","Shuohang Wang","Microsoft",2020-09-13,"1.22",2020
"Cluster-Former: Clustering-based Sparse Transformer for Long-Range Dependency Encoding@@@Cluster-Former (#C=512)","Luowei Zhou","Microsoft",2020-09-13,"1.22",2020
"Cluster-Former: Clustering-based Sparse Transformer for Long-Range Dependency Encoding@@@Cluster-Former (#C=512)","Zhe Gan","Microsoft",2020-09-13,"1.22",2020
"Cluster-Former: Clustering-based Sparse Transformer for Long-Range Dependency Encoding@@@Cluster-Former (#C=512)","Yen-Chun Chen","Microsoft",2020-09-13,"1.22",2020
"Cluster-Former: Clustering-based Sparse Transformer for Long-Range Dependency Encoding@@@Cluster-Former (#C=512)","Yuwei Fang","Microsoft",2020-09-13,"1.22",2020
"Cluster-Former: Clustering-based Sparse Transformer for Long-Range Dependency Encoding@@@Cluster-Former (#C=512)","Siqi Sun","Microsoft",2020-09-13,"1.22",2020
"Cluster-Former: Clustering-based Sparse Transformer for Long-Range Dependency Encoding@@@Cluster-Former (#C=512)","Yu Cheng","Microsoft",2020-09-13,"1.22",2020
"Cluster-Former: Clustering-based Sparse Transformer for Long-Range Dependency Encoding@@@Cluster-Former (#C=512)","Jingjing Liu","Microsoft",2020-09-13,"1.22",2020
"An Analysis of Neural Language Modeling at Multiple Scales@@@AWD-LSTM (3 layers)","Stephen Merity","",2018-03-22,"1.232",2018
"An Analysis of Neural Language Modeling at Multiple Scales@@@AWD-LSTM (3 layers)","Nitish Shirish Keskar","",2018-03-22,"1.232",2018
"An Analysis of Neural Language Modeling at Multiple Scales@@@AWD-LSTM (3 layers)","Richard Socher","",2018-03-22,"1.232",2018
"Multiplicative LSTM for sequence modelling@@@Large mLSTM","Ben Krause","",2016-09-26,"1.24",2016
"Multiplicative LSTM for sequence modelling@@@Large mLSTM","Liang Lu","",2016-09-26,"1.24",2016
"Multiplicative LSTM for sequence modelling@@@Large mLSTM","Iain Murray","",2016-09-26,"1.24",2016
"Multiplicative LSTM for sequence modelling@@@Large mLSTM","Steve Renals","",2016-09-26,"1.24",2016
"Fast-Slow Recurrent Neural Networks@@@Large FS-LSTM-4","Asier Mujika","ETH Zurich",2017-05-24," 1.25",2017
"Fast-Slow Recurrent Neural Networks@@@Large FS-LSTM-4","Florian Meier","ETH Zurich",2017-05-24," 1.25",2017
"Fast-Slow Recurrent Neural Networks@@@Large FS-LSTM-4","Angelika Steger","ETH Zurich",2017-05-24," 1.25",2017
"Recurrent Highway Networks@@@Recurrent Highway Networks","Julian G. Zilly","ETH Zurich",2016-07-12,"1.27",2016
"Recurrent Highway Networks@@@Recurrent Highway Networks","Rupesh Kumar Srivastava","Dalle Molle Institute for Artificial Intelligence Research",2016-07-12,"1.27",2016
"Recurrent Highway Networks@@@Recurrent Highway Networks","Jan Koutník","Dalle Molle Institute for Artificial Intelligence Research",2016-07-12,"1.27",2016
"Recurrent Highway Networks@@@Recurrent Highway Networks","Juergen Schmidhuber","Dalle Molle Institute for Artificial Intelligence Research",2016-07-12,"1.27",2016
"Recurrent Highway Networks@@@Recurrent Highway Networks","Jürgen Schmidhuber","Dalle Molle Institute for Artificial Intelligence Research",2016-07-12,"1.27",2016
"Neural Machine Translation in Linear Time@@@ByteNet","Nal Kalchbrenner","",2016-10-31,"1.31",2016
"Neural Machine Translation in Linear Time@@@ByteNet","Lasse Espeholt","",2016-10-31,"1.31",2016
"Neural Machine Translation in Linear Time@@@ByteNet","Karen Simonyan","",2016-10-31,"1.31",2016
"Neural Machine Translation in Linear Time@@@ByteNet","Aaron van den Oord","",2016-10-31,"1.31",2016
"Neural Machine Translation in Linear Time@@@ByteNet","Alex Graves","",2016-10-31,"1.31",2016
"Neural Machine Translation in Linear Time@@@ByteNet","Koray Kavukcuoglu","",2016-10-31,"1.31",2016
"Hierarchical Multiscale Recurrent Neural Networks@@@LN HM-LSTM","Junyoung Chung","Université de Montréal",2016-09-06,"1.32",2016
"Hierarchical Multiscale Recurrent Neural Networks@@@LN HM-LSTM","Sungjin Ahn","Université de Montréal",2016-09-06,"1.32",2016
"Hierarchical Multiscale Recurrent Neural Networks@@@LN HM-LSTM","Yoshua Bengio","Université de Montréal",2016-09-06,"1.32",2016
"Single Headed Attention RNN: Stop Thinking With Your Head@@@SHA-LSTM (4 layers, h=1024, no attention head)","Stephen Merity","",2019-11-26,"1.33",2019
"HyperNetworks@@@Hypernetworks","David Ha","Google",2016-09-27,"1.34",2016
"HyperNetworks@@@Hypernetworks","Andrew M. Dai","Google",2016-09-27,"1.34",2016
"HyperNetworks@@@Hypernetworks","Quoc V. Le","Google",2016-09-27,"1.34",2016
"Generating Sequences With Recurrent Neural Networks@@@LSTM (7 layers)","Alex Graves","University of Toronto",2013-08-04,"1.67",2013
