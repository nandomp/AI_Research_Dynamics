"title","authors","affiliations","paper_date","metric","year"
"Twin Auxilary Classifiers GAN@@@TAC-GAN","Mingming Gong","University of Melbourne",2019-12-01,"not available",2019
"Twin Auxilary Classifiers GAN@@@TAC-GAN","Yanwu Xu","University of Pittsburgh",2019-12-01,"not available",2019
"Twin Auxilary Classifiers GAN@@@TAC-GAN","Chunyuan Li","Microsoft",2019-12-01,"not available",2019
"Twin Auxilary Classifiers GAN@@@TAC-GAN","Kun Zhang","Carnegie Mellon University",2019-12-01,"not available",2019
"Twin Auxilary Classifiers GAN@@@TAC-GAN","Kayhan Batmanghelich","University of Pittsburgh",2019-12-01,"not available",2019
"Self-supervised GAN: Analysis and Improvement with Multi-class Minimax Game@@@MSGAN","Ngoc-Trung Tran","Singapore University of Technology and Design",2019-11-16,"not available",2019
"Self-supervised GAN: Analysis and Improvement with Multi-class Minimax Game@@@MSGAN","Viet-Hung Tran","Singapore University of Technology and Design",2019-11-16,"not available",2019
"Self-supervised GAN: Analysis and Improvement with Multi-class Minimax Game@@@MSGAN","Bao-Ngoc Nguyen","",2019-11-16,"not available",2019
"Self-supervised GAN: Analysis and Improvement with Multi-class Minimax Game@@@MSGAN","Ngoc-Bao Nguyen","",2019-11-16,"not available",2019
"Self-supervised GAN: Analysis and Improvement with Multi-class Minimax Game@@@MSGAN","Linxiao Yang","University of Electronic Science and Technology of China",2019-11-16,"not available",2019
"Self-supervised GAN: Analysis and Improvement with Multi-class Minimax Game@@@MSGAN","Ngai-Man Cheung","Singapore University of Technology and Design",2019-11-16,"not available",2019
"Learnable Boundary Guided Adversarial Training@@@wideresnet-34-20","Xiaowei Zhou","University of Technology, Sydney",2020-11-23,"not available",2020
"Learnable Boundary Guided Adversarial Training@@@wideresnet-34-20","Ivor W. Tsang","University of Technology, Sydney",2020-11-23,"not available",2020
"Learnable Boundary Guided Adversarial Training@@@wideresnet-34-20","Jie Yin","University of Sydney",2020-11-23,"not available",2020
"Learnable Boundary Guided Adversarial Training@@@wideresnet-34-10","Xiaowei Zhou","University of Technology, Sydney",2020-11-23,"not available",2020
"Learnable Boundary Guided Adversarial Training@@@wideresnet-34-10","Ivor W. Tsang","University of Technology, Sydney",2020-11-23,"not available",2020
"Learnable Boundary Guided Adversarial Training@@@wideresnet-34-10","Jie Yin","University of Sydney",2020-11-23,"not available",2020
"Neural Architecture Transfer@@@NAT-M4","Catherine Wong","Massachusetts Institute of Technology",2020-05-12,"not available",2020
"Neural Architecture Transfer@@@NAT-M4","Neil Houlsby","Google",2020-05-12,"not available",2020
"Neural Architecture Transfer@@@NAT-M4","Yifeng Lu","",2020-05-12,"not available",2020
"Neural Architecture Transfer@@@NAT-M4","Andrea Gesmundo","Google",2020-05-12,"not available",2020
"Neural Architecture Transfer@@@NAT-M3","Catherine Wong","Massachusetts Institute of Technology",2020-05-12,"not available",2020
"Neural Architecture Transfer@@@NAT-M3","Neil Houlsby","Google",2020-05-12,"not available",2020
"Neural Architecture Transfer@@@NAT-M3","Yifeng Lu","",2020-05-12,"not available",2020
"Neural Architecture Transfer@@@NAT-M3","Andrea Gesmundo","Google",2020-05-12,"not available",2020
"Neural Architecture Transfer@@@NAT-M2","Catherine Wong","Massachusetts Institute of Technology",2020-05-12,"not available",2020
"Neural Architecture Transfer@@@NAT-M2","Neil Houlsby","Google",2020-05-12,"not available",2020
"Neural Architecture Transfer@@@NAT-M2","Yifeng Lu","",2020-05-12,"not available",2020
"Neural Architecture Transfer@@@NAT-M2","Andrea Gesmundo","Google",2020-05-12,"not available",2020
"MUXConv: Information Multiplexing in Convolutional Neural Networks@@@MUXNet-m","Zhichao Lu","Michigan State University",2020-03-31,"not available",2020
"MUXConv: Information Multiplexing in Convolutional Neural Networks@@@MUXNet-m","Kalyanmoy Deb","Michigan State University",2020-03-31,"not available",2020
"MUXConv: Information Multiplexing in Convolutional Neural Networks@@@MUXNet-m","Vishnu Naresh Boddeti","Michigan State University",2020-03-31,"not available",2020
"Neural Architecture Transfer@@@NAT-M1","Catherine Wong","Massachusetts Institute of Technology",2020-05-12,"not available",2020
"Neural Architecture Transfer@@@NAT-M1","Neil Houlsby","Google",2020-05-12,"not available",2020
"Neural Architecture Transfer@@@NAT-M1","Yifeng Lu","",2020-05-12,"not available",2020
"Neural Architecture Transfer@@@NAT-M1","Andrea Gesmundo","Google",2020-05-12,"not available",2020
"SCAN: Learning to Classify Images without Labels@@@SCAN","Wouter Van Gansbeke","",2020-05-25,"not available",2020
"SCAN: Learning to Classify Images without Labels@@@SCAN","Simon Vandenhende","",2020-05-25,"not available",2020
"SCAN: Learning to Classify Images without Labels@@@SCAN","Stamatios Georgoulis","",2020-05-25,"not available",2020
"SCAN: Learning to Classify Images without Labels@@@SCAN","Marc Proesmans","",2020-05-25,"not available",2020
"SCAN: Learning to Classify Images without Labels@@@SCAN","Luc Van Gool","",2020-05-25,"not available",2020
"SCAN: Learning to Classify Images without Labels@@@SCAN (Avg)","Wouter Van Gansbeke","",2020-05-25,"not available",2020
"SCAN: Learning to Classify Images without Labels@@@SCAN (Avg)","Simon Vandenhende","",2020-05-25,"not available",2020
"SCAN: Learning to Classify Images without Labels@@@SCAN (Avg)","Stamatios Georgoulis","",2020-05-25,"not available",2020
"SCAN: Learning to Classify Images without Labels@@@SCAN (Avg)","Marc Proesmans","",2020-05-25,"not available",2020
"SCAN: Learning to Classify Images without Labels@@@SCAN (Avg)","Luc Van Gool","",2020-05-25,"not available",2020
"Learning Diverse and Discriminative Representations via the Principle of Maximal Coding Rate Reduction@@@MCR2","Yaodong Yu","",2020-06-15,"not available",2020
"Learning Diverse and Discriminative Representations via the Principle of Maximal Coding Rate Reduction@@@MCR2","Kwan Ho Ryan Chan","",2020-06-15,"not available",2020
"Learning Diverse and Discriminative Representations via the Principle of Maximal Coding Rate Reduction@@@MCR2","Chong You","",2020-06-15,"not available",2020
"Learning Diverse and Discriminative Representations via the Principle of Maximal Coding Rate Reduction@@@MCR2","Chaobing Song","",2020-06-15,"not available",2020
"Learning Diverse and Discriminative Representations via the Principle of Maximal Coding Rate Reduction@@@MCR2","Yi Ma","",2020-06-15,"not available",2020
"Deep Comprehensive Correlation Mining for Image Clustering@@@DCCM","Jianlong Wu","Peking University",2019-04-15,"not available",2019
"Deep Comprehensive Correlation Mining for Image Clustering@@@DCCM","Keyu Long","Chinese Academy of Sciences",2019-04-15,"not available",2019
"Deep Comprehensive Correlation Mining for Image Clustering@@@DCCM","Fei Wang","",2019-04-15,"not available",2019
"Deep Comprehensive Correlation Mining for Image Clustering@@@DCCM","Chen Qian","",2019-04-15,"not available",2019
"Deep Comprehensive Correlation Mining for Image Clustering@@@DCCM","Cheng Li","SenseTime",2019-04-15,"not available",2019
"Deep Comprehensive Correlation Mining for Image Clustering@@@DCCM","Zhouchen Lin","Peking University",2019-04-15,"not available",2019
"Deep Comprehensive Correlation Mining for Image Clustering@@@DCCM","Hongbin Zha","Peking University",2019-04-15,"not available",2019
"Deep Adaptive Image Clustering@@@DAC","Jianlong Chang","Chinese Academy of Sciences",2017-10-01,"not available",2017
"Deep Adaptive Image Clustering@@@DAC","Lingfeng Wang","Chinese Academy of Sciences",2017-10-01,"not available",2017
"Deep Adaptive Image Clustering@@@DAC","Gaofeng Meng","Chinese Academy of Sciences",2017-10-01,"not available",2017
"Deep Adaptive Image Clustering@@@DAC","Shiming Xiang","Chinese Academy of Sciences",2017-10-01,"not available",2017
"Deep Adaptive Image Clustering@@@DAC","Chunhong Pan","Chinese Academy of Sciences",2017-10-01,"not available",2017
"Unsupervised Deep Embedding for Clustering Analysis@@@DEC","Junyuan Xie","University of Washington",2015-11-19,"not available",2015
"Unsupervised Deep Embedding for Clustering Analysis@@@DEC","Ross Girshick","Facebook",2015-11-19,"not available",2015
"Unsupervised Deep Embedding for Clustering Analysis@@@DEC","Ali Farhadi","University of Washington",2015-11-19,"not available",2015
"Auto-Encoding Variational Bayes@@@VAE","Romain Lopez","University of California, Berkeley",2013-12-20,"not available",2013
"Auto-Encoding Variational Bayes@@@VAE","Jeffrey Regier","University of California, Berkeley",2013-12-20,"not available",2013
"Auto-Encoding Variational Bayes@@@VAE","Michael I. Jordan","University of California, Berkeley",2013-12-20,"not available",2013
"Auto-Encoding Variational Bayes@@@VAE","Nir Yosef","University of California, Berkeley",2013-12-20,"not available",2013
"Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks@@@GAN","Alec Radford","",2015-11-19,"not available",2015
"Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks@@@GAN","Luke Metz","",2015-11-19,"not available",2015
"Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks@@@GAN","Soumith Chintala","Facebook",2015-11-19,"not available",2015
"Joint Unsupervised Learning of Deep Representations and Image Clusters@@@JULE","Jianwei Yang","Virginia Tech",2016-04-13,"not available",2016
"Joint Unsupervised Learning of Deep Representations and Image Clusters@@@JULE","Devi Parikh","Virginia Tech",2016-04-13,"not available",2016
"Joint Unsupervised Learning of Deep Representations and Image Clusters@@@JULE","Dhruv Batra","Virginia Tech",2016-04-13,"not available",2016
"Sharpness-Aware Minimization for Efficiently Improving Generalization@@@EffNet-L2 (SAM)","Pierre Foret","Google",2020-10-03,"96.08",2020
"Sharpness-Aware Minimization for Efficiently Improving Generalization@@@EffNet-L2 (SAM)","Ariel Kleiner","Google",2020-10-03,"96.08",2020
"Sharpness-Aware Minimization for Efficiently Improving Generalization@@@EffNet-L2 (SAM)","Hossein Mobahi","Google",2020-10-03,"96.08",2020
"Sharpness-Aware Minimization for Efficiently Improving Generalization@@@EffNet-L2 (SAM)","Behnam Neyshabur","Google",2020-10-03,"96.08",2020
"Big Transfer (BiT): General Visual Representation Learning@@@BiT-L (ResNet)","Alexander Kolesnikov","Google",2019-12-24,"93.51",2019
"Big Transfer (BiT): General Visual Representation Learning@@@BiT-L (ResNet)","Lucas Beyer","Google",2019-12-24,"93.51",2019
"Big Transfer (BiT): General Visual Representation Learning@@@BiT-L (ResNet)","Xiaohua Zhai","Google",2019-12-24,"93.51",2019
"Big Transfer (BiT): General Visual Representation Learning@@@BiT-L (ResNet)","Joan Puigcerver","Google",2019-12-24,"93.51",2019
"Big Transfer (BiT): General Visual Representation Learning@@@BiT-L (ResNet)","Jessica Yung","Google",2019-12-24,"93.51",2019
"Big Transfer (BiT): General Visual Representation Learning@@@BiT-L (ResNet)","Sylvain Gelly","Google",2019-12-24,"93.51",2019
"Big Transfer (BiT): General Visual Representation Learning@@@BiT-L (ResNet)","Neil Houlsby","Google",2019-12-24,"93.51",2019
"Big Transfer (BiT): General Visual Representation Learning@@@BiT-M (ResNet)","Alexander Kolesnikov","Google",2019-12-24,"92.17",2019
"Big Transfer (BiT): General Visual Representation Learning@@@BiT-M (ResNet)","Lucas Beyer","Google",2019-12-24,"92.17",2019
"Big Transfer (BiT): General Visual Representation Learning@@@BiT-M (ResNet)","Xiaohua Zhai","Google",2019-12-24,"92.17",2019
"Big Transfer (BiT): General Visual Representation Learning@@@BiT-M (ResNet)","Joan Puigcerver","Google",2019-12-24,"92.17",2019
"Big Transfer (BiT): General Visual Representation Learning@@@BiT-M (ResNet)","Jessica Yung","Google",2019-12-24,"92.17",2019
"Big Transfer (BiT): General Visual Representation Learning@@@BiT-M (ResNet)","Sylvain Gelly","Google",2019-12-24,"92.17",2019
"Big Transfer (BiT): General Visual Representation Learning@@@BiT-M (ResNet)","Neil Houlsby","Google",2019-12-24,"92.17",2019
"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks@@@EfficientNet-B7","Mingxing Tan","Google",2019-05-28,"91.7",2019
"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks@@@EfficientNet-B7","Quoc V. Le","Google",2019-05-28,"91.7",2019
"TResNet: High Performance GPU-Dedicated Architecture@@@TResNet-XL","Tal Ridnik","",2020-03-30,"91.5",2020
"TResNet: High Performance GPU-Dedicated Architecture@@@TResNet-XL","Hussam Lawen","",2020-03-30,"91.5",2020
"TResNet: High Performance GPU-Dedicated Architecture@@@TResNet-XL","Asaf Noy","",2020-03-30,"91.5",2020
"TResNet: High Performance GPU-Dedicated Architecture@@@TResNet-XL","Itamar Friedman","",2020-03-30,"91.5",2020
"GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism@@@GPIPE","Yanping Huang","Google",2018-11-16,"91.3",2018
"GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism@@@GPIPE","Youlong Cheng","Google",2018-11-16,"91.3",2018
"GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism@@@GPIPE","Ankur Bapna","Google",2018-11-16,"91.3",2018
"GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism@@@GPIPE","Orhan Firat","Google",2018-11-16,"91.3",2018
"GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism@@@GPIPE","Mia Xu Chen","Google",2018-11-16,"91.3",2018
"GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism@@@GPIPE","Dehao Chen","Google",2018-11-16,"91.3",2018
"GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism@@@GPIPE","HyoukJoong Lee","Google",2018-11-16,"91.3",2018
"GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism@@@GPIPE","Jiquan Ngiam","Google",2018-11-16,"91.3",2018
"GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism@@@GPIPE","Quoc V. Le","Carnegie Mellon University",2018-11-16,"91.3",2018
"GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism@@@GPIPE","Yonghui Wu","Google",2018-11-16,"91.3",2018
"GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism@@@GPIPE","Zhifeng Chen","Google",2018-11-16,"91.3",2018
"Sharpness-Aware Minimization for Efficiently Improving Generalization@@@PyramidNet (SAM)","Pierre Foret","Google",2020-10-03,"89.7",2020
"Sharpness-Aware Minimization for Efficiently Improving Generalization@@@PyramidNet (SAM)","Ariel Kleiner","Google",2020-10-03,"89.7",2020
"Sharpness-Aware Minimization for Efficiently Improving Generalization@@@PyramidNet (SAM)","Hossein Mobahi","Google",2020-10-03,"89.7",2020
"Sharpness-Aware Minimization for Efficiently Improving Generalization@@@PyramidNet (SAM)","Behnam Neyshabur","Google",2020-10-03,"89.7",2020
"SplitNet: Divide and Co-training@@@PyramidNet-272, S=4","Jianbo Dong","Alibaba Group",2020-11-30,"89.46",2020
"SplitNet: Divide and Co-training@@@PyramidNet-272, S=4","Zheng Cao","Alibaba Group",2020-11-30,"89.46",2020
"SplitNet: Divide and Co-training@@@PyramidNet-272, S=4","Tao Zhang","Alibaba Group",2020-11-30,"89.46",2020
"SplitNet: Divide and Co-training@@@PyramidNet-272, S=4","Jianxi Ye","Alibaba Group",2020-11-30,"89.46",2020
"SplitNet: Divide and Co-training@@@PyramidNet-272, S=4","Shaochuang Wang","Alibaba Group",2020-11-30,"89.46",2020
"SplitNet: Divide and Co-training@@@PyramidNet-272, S=4","Fei Feng","Alibaba Group",2020-11-30,"89.46",2020
"SplitNet: Divide and Co-training@@@PyramidNet-272, S=4","Li Zhao","Alibaba Group",2020-11-30,"89.46",2020
"SplitNet: Divide and Co-training@@@PyramidNet-272, S=4","Xiaoyong Liu","Alibaba Group",2020-11-30,"89.46",2020
"SplitNet: Divide and Co-training@@@PyramidNet-272, S=4","Liuyihan Song","Alibaba Group",2020-11-30,"89.46",2020
"SplitNet: Divide and Co-training@@@PyramidNet-272, S=4","Liwei Peng","Alibaba Group",2020-11-30,"89.46",2020
"SplitNet: Divide and Co-training@@@PyramidNet-272, S=4","Pan Pan","Alibaba Group",2020-11-30,"89.46",2020
"SplitNet: Divide and Co-training@@@PyramidNet-272, S=4","Yuan Xie","Alibaba Group",2020-11-30,"89.46",2020
"AutoAugment: Learning Augmentation Policies from Data@@@PyramidNet+ShakeDrop","Ekin D. Cubuk","",2018-05-24,"89.3",2018
"AutoAugment: Learning Augmentation Policies from Data@@@PyramidNet+ShakeDrop","Barret Zoph","",2018-05-24,"89.3",2018
"AutoAugment: Learning Augmentation Policies from Data@@@PyramidNet+ShakeDrop","Dandelion Mane","",2018-05-24,"89.3",2018
"AutoAugment: Learning Augmentation Policies from Data@@@PyramidNet+ShakeDrop","Vijay K. Vasudevan","",2018-05-24,"89.3",2018
"AutoAugment: Learning Augmentation Policies from Data@@@PyramidNet+ShakeDrop","Quoc V. Le","",2018-05-24,"89.3",2018
"ColorNet: Investigating the importance of color spaces for image classification@@@ColorNet","Shreyank N Gowda","Tsinghua University",2019-02-01,"88.4",2019
"ColorNet: Investigating the importance of color spaces for image classification@@@ColorNet","Shreyank N. Gowda","Tsinghua University",2019-02-01,"88.4",2019
"ColorNet: Investigating the importance of color spaces for image classification@@@ColorNet","Chun Yuan","Tsinghua University",2019-02-01,"88.4",2019
"SpinalNet: Deep Neural Network with Gradual Input@@@Wide-ResNet-101 (Spinal FC)","H M Dipu Kabir","",2020-07-07,"88.34",2020
"SpinalNet: Deep Neural Network with Gradual Input@@@Wide-ResNet-101 (Spinal FC)","Moloud Abdar","",2020-07-07,"88.34",2020
"SpinalNet: Deep Neural Network with Gradual Input@@@Wide-ResNet-101 (Spinal FC)","Seyed Mohammad Jafar Jalali","",2020-07-07,"88.34",2020
"SpinalNet: Deep Neural Network with Gradual Input@@@Wide-ResNet-101 (Spinal FC)","Abbas Khosravi","",2020-07-07,"88.34",2020
"SpinalNet: Deep Neural Network with Gradual Input@@@Wide-ResNet-101 (Spinal FC)","Amir F. Atiya","",2020-07-07,"88.34",2020
"SpinalNet: Deep Neural Network with Gradual Input@@@Wide-ResNet-101 (Spinal FC)","Saeid Nahavandi","",2020-07-07,"88.34",2020
"SpinalNet: Deep Neural Network with Gradual Input@@@Wide-ResNet-101 (Spinal FC)","Dipti Srinivasan","",2020-07-07,"88.34",2020
"Neural Architecture Transfer@@@NAT-M4","Catherine Wong","Massachusetts Institute of Technology",2020-05-12,"88.3",2020
"Neural Architecture Transfer@@@NAT-M4","Neil Houlsby","Google",2020-05-12,"88.3",2020
"Neural Architecture Transfer@@@NAT-M4","Yifeng Lu","",2020-05-12,"88.3",2020
"Neural Architecture Transfer@@@NAT-M4","Andrea Gesmundo","Google",2020-05-12,"88.3",2020
"Fast AutoAugment@@@PyramidNet+ShakeDrop (Fast AA)","Sungbin Lim","",2019-05-01,"88.3",2019
"Fast AutoAugment@@@PyramidNet+ShakeDrop (Fast AA)","Ildoo Kim","",2019-05-01,"88.3",2019
"Fast AutoAugment@@@PyramidNet+ShakeDrop (Fast AA)","Taesup Kim","",2019-05-01,"88.3",2019
"Fast AutoAugment@@@PyramidNet+ShakeDrop (Fast AA)","Chiheon Kim","",2019-05-01,"88.3",2019
"Fast AutoAugment@@@PyramidNet+ShakeDrop (Fast AA)","Sungwoong Kim","",2019-05-01,"88.3",2019
"Neural Architecture Transfer@@@NAT-M3","Catherine Wong","Massachusetts Institute of Technology",2020-05-12,"87.7",2020
"Neural Architecture Transfer@@@NAT-M3","Neil Houlsby","Google",2020-05-12,"87.7",2020
"Neural Architecture Transfer@@@NAT-M3","Yifeng Lu","",2020-05-12,"87.7",2020
"Neural Architecture Transfer@@@NAT-M3","Andrea Gesmundo","Google",2020-05-12,"87.7",2020
"Neural Architecture Transfer@@@NAT-M2","Catherine Wong","Massachusetts Institute of Technology",2020-05-12,"87.5",2020
"Neural Architecture Transfer@@@NAT-M2","Neil Houlsby","Google",2020-05-12,"87.5",2020
"Neural Architecture Transfer@@@NAT-M2","Yifeng Lu","",2020-05-12,"87.5",2020
"Neural Architecture Transfer@@@NAT-M2","Andrea Gesmundo","Google",2020-05-12,"87.5",2020
"SplitNet: Divide and Co-training@@@DenseNet-BC-190, S=4","Jianbo Dong","Alibaba Group",2020-11-30,"87.44",2020
"SplitNet: Divide and Co-training@@@DenseNet-BC-190, S=4","Zheng Cao","Alibaba Group",2020-11-30,"87.44",2020
"SplitNet: Divide and Co-training@@@DenseNet-BC-190, S=4","Tao Zhang","Alibaba Group",2020-11-30,"87.44",2020
"SplitNet: Divide and Co-training@@@DenseNet-BC-190, S=4","Jianxi Ye","Alibaba Group",2020-11-30,"87.44",2020
"SplitNet: Divide and Co-training@@@DenseNet-BC-190, S=4","Shaochuang Wang","Alibaba Group",2020-11-30,"87.44",2020
"SplitNet: Divide and Co-training@@@DenseNet-BC-190, S=4","Fei Feng","Alibaba Group",2020-11-30,"87.44",2020
"SplitNet: Divide and Co-training@@@DenseNet-BC-190, S=4","Li Zhao","Alibaba Group",2020-11-30,"87.44",2020
"SplitNet: Divide and Co-training@@@DenseNet-BC-190, S=4","Xiaoyong Liu","Alibaba Group",2020-11-30,"87.44",2020
"SplitNet: Divide and Co-training@@@DenseNet-BC-190, S=4","Liuyihan Song","Alibaba Group",2020-11-30,"87.44",2020
"SplitNet: Divide and Co-training@@@DenseNet-BC-190, S=4","Liwei Peng","Alibaba Group",2020-11-30,"87.44",2020
"SplitNet: Divide and Co-training@@@DenseNet-BC-190, S=4","Pan Pan","Alibaba Group",2020-11-30,"87.44",2020
"SplitNet: Divide and Co-training@@@DenseNet-BC-190, S=4","Yuan Xie","Alibaba Group",2020-11-30,"87.44",2020
"SplitNet: Divide and Co-training@@@WRN-40-10, S=4","Jianbo Dong","Alibaba Group",2020-11-30,"86.90",2020
"SplitNet: Divide and Co-training@@@WRN-40-10, S=4","Zheng Cao","Alibaba Group",2020-11-30,"86.90",2020
"SplitNet: Divide and Co-training@@@WRN-40-10, S=4","Tao Zhang","Alibaba Group",2020-11-30,"86.90",2020
"SplitNet: Divide and Co-training@@@WRN-40-10, S=4","Jianxi Ye","Alibaba Group",2020-11-30,"86.90",2020
"SplitNet: Divide and Co-training@@@WRN-40-10, S=4","Shaochuang Wang","Alibaba Group",2020-11-30,"86.90",2020
"SplitNet: Divide and Co-training@@@WRN-40-10, S=4","Fei Feng","Alibaba Group",2020-11-30,"86.90",2020
"SplitNet: Divide and Co-training@@@WRN-40-10, S=4","Li Zhao","Alibaba Group",2020-11-30,"86.90",2020
"SplitNet: Divide and Co-training@@@WRN-40-10, S=4","Xiaoyong Liu","Alibaba Group",2020-11-30,"86.90",2020
"SplitNet: Divide and Co-training@@@WRN-40-10, S=4","Liuyihan Song","Alibaba Group",2020-11-30,"86.90",2020
"SplitNet: Divide and Co-training@@@WRN-40-10, S=4","Liwei Peng","Alibaba Group",2020-11-30,"86.90",2020
"SplitNet: Divide and Co-training@@@WRN-40-10, S=4","Pan Pan","Alibaba Group",2020-11-30,"86.90",2020
"SplitNet: Divide and Co-training@@@WRN-40-10, S=4","Yuan Xie","Alibaba Group",2020-11-30,"86.90",2020
"CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features@@@PyramidNet-200 + Shakedrop + Cutmix","Sangdoo Yun","Naver Corporation",2019-05-13,"86.19",2019
"CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features@@@PyramidNet-200 + Shakedrop + Cutmix","Dongyoon Han","Naver Corporation",2019-05-13,"86.19",2019
"CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features@@@PyramidNet-200 + Shakedrop + Cutmix","Sanghyuk Chun","Naver Corporation",2019-05-13,"86.19",2019
"CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features@@@PyramidNet-200 + Shakedrop + Cutmix","Seong Joon Oh","",2019-05-13,"86.19",2019
"CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features@@@PyramidNet-200 + Shakedrop + Cutmix","Youngjoon Yoo","Naver Corporation",2019-05-13,"86.19",2019
"CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features@@@PyramidNet-200 + Shakedrop + Cutmix","Junsuk Choe","Yonsei University",2019-05-13,"86.19",2019
"MUXConv: Information Multiplexing in Convolutional Neural Networks@@@MUXNet-m","Zhichao Lu","Michigan State University",2020-03-31,"86.1",2020
"MUXConv: Information Multiplexing in Convolutional Neural Networks@@@MUXNet-m","Kalyanmoy Deb","Michigan State University",2020-03-31,"86.1",2020
"MUXConv: Information Multiplexing in Convolutional Neural Networks@@@MUXNet-m","Vishnu Naresh Boddeti","Michigan State University",2020-03-31,"86.1",2020
"Neural Architecture Transfer@@@NAT-M1","Catherine Wong","Massachusetts Institute of Technology",2020-05-12,"86.0",2020
"Neural Architecture Transfer@@@NAT-M1","Neil Houlsby","Google",2020-05-12,"86.0",2020
"Neural Architecture Transfer@@@NAT-M1","Yifeng Lu","",2020-05-12,"86.0",2020
"Neural Architecture Transfer@@@NAT-M1","Andrea Gesmundo","Google",2020-05-12,"86.0",2020
"SplitNet: Divide and Co-training@@@WRN-28-10, S=4","Jianbo Dong","Alibaba Group",2020-11-30,"85.74",2020
"SplitNet: Divide and Co-training@@@WRN-28-10, S=4","Zheng Cao","Alibaba Group",2020-11-30,"85.74",2020
"SplitNet: Divide and Co-training@@@WRN-28-10, S=4","Tao Zhang","Alibaba Group",2020-11-30,"85.74",2020
"SplitNet: Divide and Co-training@@@WRN-28-10, S=4","Jianxi Ye","Alibaba Group",2020-11-30,"85.74",2020
"SplitNet: Divide and Co-training@@@WRN-28-10, S=4","Shaochuang Wang","Alibaba Group",2020-11-30,"85.74",2020
"SplitNet: Divide and Co-training@@@WRN-28-10, S=4","Fei Feng","Alibaba Group",2020-11-30,"85.74",2020
"SplitNet: Divide and Co-training@@@WRN-28-10, S=4","Li Zhao","Alibaba Group",2020-11-30,"85.74",2020
"SplitNet: Divide and Co-training@@@WRN-28-10, S=4","Xiaoyong Liu","Alibaba Group",2020-11-30,"85.74",2020
"SplitNet: Divide and Co-training@@@WRN-28-10, S=4","Liuyihan Song","Alibaba Group",2020-11-30,"85.74",2020
"SplitNet: Divide and Co-training@@@WRN-28-10, S=4","Liwei Peng","Alibaba Group",2020-11-30,"85.74",2020
"SplitNet: Divide and Co-training@@@WRN-28-10, S=4","Pan Pan","Alibaba Group",2020-11-30,"85.74",2020
"SplitNet: Divide and Co-training@@@WRN-28-10, S=4","Yuan Xie","Alibaba Group",2020-11-30,"85.74",2020
"Squeeze-and-Excitation Networks@@@SENet + ShakeEven + Cutout","Jie Hu","",2017-09-05,"84.59",2017
"Squeeze-and-Excitation Networks@@@SENet + ShakeEven + Cutout","Li Shen","University of Oxford",2017-09-05,"84.59",2017
"Squeeze-and-Excitation Networks@@@SENet + ShakeEven + Cutout","Gang Sun","",2017-09-05,"84.59",2017
"Averaging Weights Leads to Wider Optima and Better Generalization@@@PyramidNet-272 + SWA","Pavel Izmailov","Cornell University",2018-03-14,"84.16",2018
"Averaging Weights Leads to Wider Optima and Better Generalization@@@PyramidNet-272 + SWA","Dmitrii Podoprikhin","National Research University – Higher School of Economics",2018-03-14,"84.16",2018
"Averaging Weights Leads to Wider Optima and Better Generalization@@@PyramidNet-272 + SWA","D. A. Podoprikhin","National Research University – Higher School of Economics",2018-03-14,"84.16",2018
"Averaging Weights Leads to Wider Optima and Better Generalization@@@PyramidNet-272 + SWA","Timur Garipov","Moscow State University",2018-03-14,"84.16",2018
"Averaging Weights Leads to Wider Optima and Better Generalization@@@PyramidNet-272 + SWA","Dmitry Vetrov","National Research University – Higher School of Economics",2018-03-14,"84.16",2018
"Averaging Weights Leads to Wider Optima and Better Generalization@@@PyramidNet-272 + SWA","Andrew Gordon Wilson","Cornell University",2018-03-14,"84.16",2018
"Puzzle Mix: Exploiting Saliency and Local Statistics for Optimal Mixup@@@WRN28-10","Janghyun Kim","Seoul National University",2020-09-15,"84.05",2020
"Puzzle Mix: Exploiting Saliency and Local Statistics for Optimal Mixup@@@WRN28-10","Jang Hyun Kim","Seoul National University",2020-09-15,"84.05",2020
"Puzzle Mix: Exploiting Saliency and Local Statistics for Optimal Mixup@@@WRN28-10","Wonho Choo","Seoul National University",2020-09-15,"84.05",2020
"Puzzle Mix: Exploiting Saliency and Local Statistics for Optimal Mixup@@@WRN28-10","Hyun Oh Song","Seoul National University",2020-09-15,"84.05",2020
"FMix: Enhancing Mixed Sample Data Augmentation@@@DenseNet-BC-190 + FMix","Ethan Harris","University of Southampton",2020-02-27,"83.95",2020
"FMix: Enhancing Mixed Sample Data Augmentation@@@DenseNet-BC-190 + FMix","Antonia Marcu","University of Southampton",2020-02-27,"83.95",2020
"FMix: Enhancing Mixed Sample Data Augmentation@@@DenseNet-BC-190 + FMix","Matthew Painter","University of Southampton",2020-02-27,"83.95",2020
"FMix: Enhancing Mixed Sample Data Augmentation@@@DenseNet-BC-190 + FMix","Mahesan Niranjan","University of Southampton",2020-02-27,"83.95",2020
"FMix: Enhancing Mixed Sample Data Augmentation@@@DenseNet-BC-190 + FMix","Adam Prügel-Bennett","University of Southampton",2020-02-27,"83.95",2020
"FMix: Enhancing Mixed Sample Data Augmentation@@@DenseNet-BC-190 + FMix","Jonathon S. Hare","University of Southampton",2020-02-27,"83.95",2020
"Grafit: Learning fine-grained image representations with coarse labels@@@Grafit (ResNet-50)","Jianlong Fu","Microsoft",2020-11-25,"83.7",2020
"Grafit: Learning fine-grained image representations with coarse labels@@@Grafit (ResNet-50)","Heliang Zheng","University of Science and Technology of China",2020-11-25,"83.7",2020
"Grafit: Learning fine-grained image representations with coarse labels@@@Grafit (ResNet-50)","Tao Mei","Microsoft",2020-11-25,"83.7",2020
"Res2Net: A New Multi-scale Backbone Architecture@@@Res2NeXt-29","Shanghua Gao","Nankai University",2019-04-02,"83.44",2019
"Res2Net: A New Multi-scale Backbone Architecture@@@Res2NeXt-29","Ming-Ming Cheng","Nankai University",2019-04-02,"83.44",2019
"Res2Net: A New Multi-scale Backbone Architecture@@@Res2NeXt-29","Kai Zhao","Nankai University",2019-04-02,"83.44",2019
"Res2Net: A New Multi-scale Backbone Architecture@@@Res2NeXt-29","Xin-Yu Zhang","Nankai University",2019-04-02,"83.44",2019
"Res2Net: A New Multi-scale Backbone Architecture@@@Res2NeXt-29","Ming-Hsuan Yang","University of California, Merced",2019-04-02,"83.44",2019
"Res2Net: A New Multi-scale Backbone Architecture@@@Res2NeXt-29","Philip H. S. Torr","University of Oxford",2019-04-02,"83.44",2019
"mixup: Beyond Empirical Risk Minimization@@@DenseNet-BC-190 + Mixup","Hongyi Zhang","Massachusetts Institute of Technology",2017-10-25,"83.20",2017
"mixup: Beyond Empirical Risk Minimization@@@DenseNet-BC-190 + Mixup","Moustapha Cisse","Facebook",2017-10-25,"83.20",2017
"mixup: Beyond Empirical Risk Minimization@@@DenseNet-BC-190 + Mixup","Yann N. Dauphin","Facebook",2017-10-25,"83.20",2017
"mixup: Beyond Empirical Risk Minimization@@@DenseNet-BC-190 + Mixup","David Lopez-Paz","Facebook",2017-10-25,"83.20",2017
"EnAET: Self-Trained Ensemble AutoEncoding Transformations for Semi-Supervised Learning@@@EnAET","Xiao Wang","",2019-11-21,"83.13",2019
"EnAET: Self-Trained Ensemble AutoEncoding Transformations for Semi-Supervised Learning@@@EnAET","Daisuke Kihara","",2019-11-21,"83.13",2019
"EnAET: Self-Trained Ensemble AutoEncoding Transformations for Semi-Supervised Learning@@@EnAET","Jiebo Luo","",2019-11-21,"83.13",2019
"EnAET: Self-Trained Ensemble AutoEncoding Transformations for Semi-Supervised Learning@@@EnAET","Guo-Jun Qi","",2019-11-21,"83.13",2019
"Single-bit-per-weight deep convolutional neural networks without batch-normalization layers for embedded systems@@@Wide ResNet+Cutout+no BN scale/offset learning","Mark D. McDonnell","University of South Australia",2019-07-16,"82.95",2019
"Single-bit-per-weight deep convolutional neural networks without batch-normalization layers for embedded systems@@@Wide ResNet+Cutout+no BN scale/offset learning","Hesham Mostafa","University of California, San Diego",2019-07-16,"82.95",2019
"Single-bit-per-weight deep convolutional neural networks without batch-normalization layers for embedded systems@@@Wide ResNet+Cutout+no BN scale/offset learning","Runchun Wang","University of Sydney",2019-07-16,"82.95",2019
"Single-bit-per-weight deep convolutional neural networks without batch-normalization layers for embedded systems@@@Wide ResNet+Cutout+no BN scale/offset learning","A. van Schaik","University of Sydney",2019-07-16,"82.95",2019
"Single-bit-per-weight deep convolutional neural networks without batch-normalization layers for embedded systems@@@Wide ResNet+Cutout+no BN scale/offset learning","André van Schaik","University of Sydney",2019-07-16,"82.95",2019
"Selective Kernel Networks@@@SKNet-29 (ResNeXt-29, 16×32d)","Xiang Li","Nanjing University of Science and Technology",2019-03-15,"82.67",2019
"Selective Kernel Networks@@@SKNet-29 (ResNeXt-29, 16×32d)","Wenhai Wang","Tsinghua University",2019-03-15,"82.67",2019
"Selective Kernel Networks@@@SKNet-29 (ResNeXt-29, 16×32d)","Xiaolin Hu","Nanjing University",2019-03-15,"82.67",2019
"Selective Kernel Networks@@@SKNet-29 (ResNeXt-29, 16×32d)","Jian Yang","Nanjing University of Science and Technology",2019-03-15,"82.67",2019
"Densely Connected Convolutional Networks@@@DenseNet","Gao Huang","Cornell University",2016-08-25,"82.62",2016
"Densely Connected Convolutional Networks@@@DenseNet","Zhuang Liu","Tsinghua University",2016-08-25,"82.62",2016
"Densely Connected Convolutional Networks@@@DenseNet","Laurens van der Maaten","Facebook",2016-08-25,"82.62",2016
"Densely Connected Convolutional Networks@@@DenseNet","Kilian Q. Weinberger","Cornell University",2016-08-25,"82.62",2016
"Attend and Rectify: a Gated Attention Mechanism for Fine-Grained Recovery@@@WARN","Pau Rodríguez","Autonomous University of Barcelona",2018-07-19,"82.18",2018
"Attend and Rectify: a Gated Attention Mechanism for Fine-Grained Recovery@@@WARN","Josep M. Gonfaus","",2018-07-19,"82.18",2018
"Attend and Rectify: a Gated Attention Mechanism for Fine-Grained Recovery@@@WARN","Guillem Cucurull","Autonomous University of Barcelona",2018-07-19,"82.18",2018
"Attend and Rectify: a Gated Attention Mechanism for Fine-Grained Recovery@@@WARN","F. Xavier Roca","Autonomous University of Barcelona",2018-07-19,"82.18",2018
"Attend and Rectify: a Gated Attention Mechanism for Fine-Grained Recovery@@@WARN","Francesc Xavier Roca i Marvà","Autonomous University of Barcelona",2018-07-19,"82.18",2018
"Attend and Rectify: a Gated Attention Mechanism for Fine-Grained Recovery@@@WARN","Jordi Pérez González","Autonomous University of Barcelona",2018-07-19,"82.18",2018
"Attend and Rectify: a Gated Attention Mechanism for Fine-Grained Recovery@@@WARN","Jordi Gonzàlez","Autonomous University of Barcelona",2018-07-19,"82.18",2018
"Averaging Weights Leads to Wider Optima and Better Generalization@@@WRN+SWA","Pavel Izmailov","Cornell University",2018-03-14,"82.15",2018
"Averaging Weights Leads to Wider Optima and Better Generalization@@@WRN+SWA","Dmitrii Podoprikhin","National Research University – Higher School of Economics",2018-03-14,"82.15",2018
"Averaging Weights Leads to Wider Optima and Better Generalization@@@WRN+SWA","D. A. Podoprikhin","National Research University – Higher School of Economics",2018-03-14,"82.15",2018
"Averaging Weights Leads to Wider Optima and Better Generalization@@@WRN+SWA","Timur Garipov","Moscow State University",2018-03-14,"82.15",2018
"Averaging Weights Leads to Wider Optima and Better Generalization@@@WRN+SWA","Dmitry Vetrov","National Research University – Higher School of Economics",2018-03-14,"82.15",2018
"Averaging Weights Leads to Wider Optima and Better Generalization@@@WRN+SWA","Andrew Gordon Wilson","Cornell University",2018-03-14,"82.15",2018
"Manifold Mixup: Better Representations by Interpolating Hidden States@@@Manifold Mixup","Vikas Verma","Aalto University",2018-06-13,"81.96",2018
"Manifold Mixup: Better Representations by Interpolating Hidden States@@@Manifold Mixup","Alex Lamb","Université de Montréal",2018-06-13,"81.96",2018
"Manifold Mixup: Better Representations by Interpolating Hidden States@@@Manifold Mixup","Christopher Beckham","",2018-06-13,"81.96",2018
"Manifold Mixup: Better Representations by Interpolating Hidden States@@@Manifold Mixup","Amir Abbas Najafi","Sharif University of Technology",2018-06-13,"81.96",2018
"Manifold Mixup: Better Representations by Interpolating Hidden States@@@Manifold Mixup","Amir Najafi","Sharif University of Technology",2018-06-13,"81.96",2018
"Manifold Mixup: Better Representations by Interpolating Hidden States@@@Manifold Mixup","Ioannis Mitliagkas","Université de Montréal",2018-06-13,"81.96",2018
"Manifold Mixup: Better Representations by Interpolating Hidden States@@@Manifold Mixup","David Lopez-Paz","Facebook",2018-06-13,"81.96",2018
"Manifold Mixup: Better Representations by Interpolating Hidden States@@@Manifold Mixup","Yoshua Bengio","",2018-06-13,"81.96",2018
"Learning Identity Mappings with Residual Gates@@@Residual Gates + WRN","Pedro Savarese","Federal University of Rio de Janeiro",2016-11-04,"81.73",2016
"Learning Identity Mappings with Residual Gates@@@Residual Gates + WRN","Leonardo O. Mazza","Federal University of Rio de Janeiro",2016-11-04,"81.73",2016
"Learning Identity Mappings with Residual Gates@@@Residual Gates + WRN","Daniel R. Figueiredo","",2016-11-04,"81.73",2016
"Attention Augmented Convolutional Networks@@@AA-Wide-ResNet","Irwan Bello","Google",2019-04-22,"81.6",2019
"Attention Augmented Convolutional Networks@@@AA-Wide-ResNet","Barret Zoph","Google",2019-04-22,"81.6",2019
"Attention Augmented Convolutional Networks@@@AA-Wide-ResNet","Quoc V. Le","Google",2019-04-22,"81.6",2019
"Attention Augmented Convolutional Networks@@@AA-Wide-ResNet","Ashish Vaswani","Google",2019-04-22,"81.6",2019
"Attention Augmented Convolutional Networks@@@AA-Wide-ResNet","Jonathon Shlens","Google",2019-04-22,"81.6",2019
"Wide Residual Networks@@@Wide ResNet","Sergey Zagoruyko","",2016-05-23,"81.15",2016
"Wide Residual Networks@@@Wide ResNet","Nikos Komodakis","",2016-05-23,"81.15",2016
"Deep Competitive Pathway Networks@@@CoPaNet-R-164","Jia-Ren Chang","National Chiao Tung University",2017-09-29,"81.10",2017
"Deep Competitive Pathway Networks@@@CoPaNet-R-164","Yong-Sheng Chen","",2017-09-29,"81.10",2017
"Towards Principled Design of Deep Convolutional Networks: Introducing SimpNet@@@SimpleNetv2","Seyyed Hossein HasanPour","",2018-02-17,"80.29",2018
"Towards Principled Design of Deep Convolutional Networks: Introducing SimpNet@@@SimpleNetv2","Mohammad Rouhani","",2018-02-17,"80.29",2018
"Towards Principled Design of Deep Convolutional Networks: Introducing SimpNet@@@SimpleNetv2","Mohammad Hossein Rouhani","",2018-02-17,"80.29",2018
"Towards Principled Design of Deep Convolutional Networks: Introducing SimpNet@@@SimpleNetv2","Mohsen Fayyaz","",2018-02-17,"80.29",2018
"Towards Principled Design of Deep Convolutional Networks: Introducing SimpNet@@@SimpleNetv2","Mohammad Sabokrou","",2018-02-17,"80.29",2018
"Towards Principled Design of Deep Convolutional Networks: Introducing SimpNet@@@SimpleNetv2","Ehsan Adeli","",2018-02-17,"80.29",2018
"Training Neural Networks with Local Error Signals@@@VGG11B(3x) + LocalLearning","James M Murray","Columbia University",2019-01-20,"79.9",2019
"Training Neural Networks with Local Error Signals@@@VGG11B(3x) + LocalLearning","James M. Murray","Columbia University",2019-01-20,"79.9",2019
"Regularizing Neural Networks via Adversarial Model Perturbation@@@PreActResNet18+AMP","Yaowei Zheng","",2020-10-10,"78.49",2020
"Regularizing Neural Networks via Adversarial Model Perturbation@@@PreActResNet18+AMP","Richong Zhang","",2020-10-10,"78.49",2020
"Regularizing Neural Networks via Adversarial Model Perturbation@@@PreActResNet18+AMP","Yongyi Mao","",2020-10-10,"78.49",2020
"Lets keep it simple, Using simple architectures to outperform deeper and more complex architectures@@@SimpleNetv1","Seyyed Hossein HasanPour","",2016-08-22,"78.37",2016
"Lets keep it simple, Using simple architectures to outperform deeper and more complex architectures@@@SimpleNetv1","Mohammad Rouhani","",2016-08-22,"78.37",2016
"Lets keep it simple, Using simple architectures to outperform deeper and more complex architectures@@@SimpleNetv1","Mohammad Hossein Rouhani","",2016-08-22,"78.37",2016
"Lets keep it simple, Using simple architectures to outperform deeper and more complex architectures@@@SimpleNetv1","Mohsen Fayyaz","",2016-08-22,"78.37",2016
"Lets keep it simple, Using simple architectures to outperform deeper and more complex architectures@@@SimpleNetv1","Mohammad Sabokrou","",2016-08-22,"78.37",2016
"Rethinking Depthwise Separable Convolutions: How Intra-Kernel Correlations Lead to Improved MobileNets@@@MobileNetV3-large x1.0 (BSConv-U)","Daniel Haase","University of Jena",2020-03-30,"77.7",2020
"Rethinking Depthwise Separable Convolutions: How Intra-Kernel Correlations Lead to Improved MobileNets@@@MobileNetV3-large x1.0 (BSConv-U)","Manuel Amthor","University of Jena",2020-03-30,"77.7",2020
"Identity Mappings in Deep Residual Networks@@@ResNet-1001","Kaiming He","Microsoft",2016-03-16,"77.3",2016
"Identity Mappings in Deep Residual Networks@@@ResNet-1001","Xiangyu Zhang","Microsoft",2016-03-16,"77.3",2016
"Identity Mappings in Deep Residual Networks@@@ResNet-1001","Shaoqing Ren","Microsoft",2016-03-16,"77.3",2016
"Identity Mappings in Deep Residual Networks@@@ResNet-1001","Jian Sun","Microsoft",2016-03-16,"77.3",2016
"Large-Scale Evolution of Image Classifiers@@@Evolution","Esteban Real","Google",2017-03-03,"77",2017
"Large-Scale Evolution of Image Classifiers@@@Evolution","Sherry Moore","Google",2017-03-03,"77",2017
"Large-Scale Evolution of Image Classifiers@@@Evolution","Andrew Selle","Google",2017-03-03,"77",2017
"Large-Scale Evolution of Image Classifiers@@@Evolution","Saurabh Saxena","Google",2017-03-03,"77",2017
"Large-Scale Evolution of Image Classifiers@@@Evolution","Yutaka Leon Suematsu","Google",2017-03-03,"77",2017
"Large-Scale Evolution of Image Classifiers@@@Evolution","Jie Tan","Google",2017-03-03,"77",2017
"Large-Scale Evolution of Image Classifiers@@@Evolution","Quoc V. Le","Google",2017-03-03,"77",2017
"Large-Scale Evolution of Image Classifiers@@@Evolution","Alexey Kurakin","Google",2017-03-03,"77",2017
"DIANet: Dense-and-Implicit Attention Network@@@DIANet","zhongzhan huang","Tsinghua University",2019-05-25,"76.98",2019
"DIANet: Dense-and-Implicit Attention Network@@@DIANet","Senwei Liang","National University of Singapore",2019-05-25,"76.98",2019
"DIANet: Dense-and-Implicit Attention Network@@@DIANet","Mingfu Liang","Northwestern University",2019-05-25,"76.98",2019
"DIANet: Dense-and-Implicit Attention Network@@@DIANet","Haizhao Yang","National University of Singapore",2019-05-25,"76.98",2019
"Encoding the latent posterior of Bayesian Neural Networks for uncertainty quantification@@@LP-BNN (ours) + cutout","Yinhao Zhu","University of Notre Dame",2020-12-04,"76.85",2020
"Encoding the latent posterior of Bayesian Neural Networks for uncertainty quantification@@@LP-BNN (ours) + cutout","Nicholas Zabaras","University of Notre Dame",2020-12-04,"76.85",2020
"Spatially-sparse convolutional neural networks@@@SSCNN","Benjamin Graham","University of Warwick",2014-09-22,"75.7",2014
"Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)@@@Exponential Linear Units","Djork-Arné Clevert","Johannes Kepler University of Linz",2015-11-23,"75.7",2015
"Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)@@@Exponential Linear Units","Thomas Unterthiner","Johannes Kepler University of Linz",2015-11-23,"75.7",2015
"Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)@@@Exponential Linear Units","Sepp Hochreiter","Johannes Kepler University of Linz",2015-11-23,"75.7",2015
"Deep Networks with Stochastic Depth@@@Stochastic Depth","Gao Huang","Cornell University",2016-03-30,"75.42",2016
"Deep Networks with Stochastic Depth@@@Stochastic Depth","Yu Sun","Cornell University",2016-03-30,"75.42",2016
"Deep Networks with Stochastic Depth@@@Stochastic Depth","Zhuang Liu","Tsinghua University",2016-03-30,"75.42",2016
"Deep Networks with Stochastic Depth@@@Stochastic Depth","Daniel Sedra","Cornell University",2016-03-30,"75.42",2016
"Deep Networks with Stochastic Depth@@@Stochastic Depth","Kilian Q. Weinberger","Cornell University",2016-03-30,"75.42",2016
"Mish: A Self Regularized Non-Monotonic Activation Function@@@ResNet v2-110 (Mish activation)","Diganta Misra","",2019-08-23,"74.41",2019
"MixMatch: A Holistic Approach to Semi-Supervised Learning@@@MixMatch","David Berthelot","Google",2019-05-06,"74.1",2019
"MixMatch: A Holistic Approach to Semi-Supervised Learning@@@MixMatch","Nicholas Carlini","Google",2019-05-06,"74.1",2019
"MixMatch: A Holistic Approach to Semi-Supervised Learning@@@MixMatch","Ian Goodfellow","Google",2019-05-06,"74.1",2019
"MixMatch: A Holistic Approach to Semi-Supervised Learning@@@MixMatch","Nicolas Papernot","Google",2019-05-06,"74.1",2019
"MixMatch: A Holistic Approach to Semi-Supervised Learning@@@MixMatch","Avital Oliver","Google",2019-05-06,"74.1",2019
"MixMatch: A Holistic Approach to Semi-Supervised Learning@@@MixMatch","Colin Raffel","Google",2019-05-06,"74.1",2019
"Fractional Max-Pooling@@@Fractional MP","Benjamin Graham","University of Warwick",2014-12-18,"73.6",2014
"Deep Residual Networks with Exponential Linear Unit@@@ResNet+ELU","Yang Li","Beijing University of Posts and Telecommunications",2016-04-14,"73.5",2016
"Deep Residual Networks with Exponential Linear Unit@@@ResNet+ELU","Chunxiao Fan","Beijing University of Posts and Telecommunications",2016-04-14,"73.5",2016
"Deep Residual Networks with Exponential Linear Unit@@@ResNet+ELU","Yong Li","Beijing University of Posts and Telecommunications",2016-04-14,"73.5",2016
"Deep Residual Networks with Exponential Linear Unit@@@ResNet+ELU","Qiong Wu","Beijing University of Posts and Telecommunications",2016-04-14,"73.5",2016
"Deep Residual Networks with Exponential Linear Unit@@@ResNet+ELU","Yue Ming","Beijing University of Posts and Telecommunications",2016-04-14,"73.5",2016
"Stochastic Optimization of Plain Convolutional Neural Networks with Simple methods@@@SOPCNN","Yahia Saeed Assiri","Pace University",2020-01-24,"72.96",2020
"@@@Deep Complex","","",NA,"72.9",NA
"Scalable Bayesian Optimization Using Deep Neural Networks@@@Tuned CNN","Jasper Snoek","Harvard University",2015-02-19,"72.6",2015
"Scalable Bayesian Optimization Using Deep Neural Networks@@@Tuned CNN","Oren Rippel","Massachusetts Institute of Technology",2015-02-19,"72.6",2015
"Scalable Bayesian Optimization Using Deep Neural Networks@@@Tuned CNN","Kevin Swersky","University of Toronto",2015-02-19,"72.6",2015
"Scalable Bayesian Optimization Using Deep Neural Networks@@@Tuned CNN","Ryan Kiros","University of Toronto",2015-02-19,"72.6",2015
"Scalable Bayesian Optimization Using Deep Neural Networks@@@Tuned CNN","Nadathur Satish","Intel",2015-02-19,"72.6",2015
"Scalable Bayesian Optimization Using Deep Neural Networks@@@Tuned CNN","Narayanan Sundaram","Intel",2015-02-19,"72.6",2015
"Scalable Bayesian Optimization Using Deep Neural Networks@@@Tuned CNN","Md. Mostofa Ali Patwary","Intel",2015-02-19,"72.6",2015
"Scalable Bayesian Optimization Using Deep Neural Networks@@@Tuned CNN","Prabhat","Lawrence Berkeley National Laboratory",2015-02-19,"72.6",2015
"Scalable Bayesian Optimization Using Deep Neural Networks@@@Tuned CNN","Ryan P. Adams","Harvard University",2015-02-19,"72.6",2015
"Competitive Multi-scale Convolution@@@CMsC","Zhibin Liao","University of Adelaide",2015-11-18,"72.4",2015
"Competitive Multi-scale Convolution@@@CMsC","Gustavo Carneiro","University of Adelaide",2015-11-18,"72.4",2015
"All you need is a good init@@@Fitnet4-LSUV","Dmytro Mishkin","Czech Technical University in Prague",2015-11-19,"72.3",2015
"All you need is a good init@@@Fitnet4-LSUV","Jiri Matas","Czech Technical University in Prague",2015-11-19,"72.3",2015
"Batch-normalized Maxout Network in Network@@@BNM NiN","Jia-Ren Chang","",2015-11-09,"71.1",2015
"Batch-normalized Maxout Network in Network@@@BNM NiN","Yong-Sheng Chen","",2015-11-09,"71.1",2015
"On the Importance of Normalisation Layers in Deep Learning with Piecewise Linear Activation Units@@@MIM","Zhibin Liao","University of Adelaide",2015-08-03,"70.8",2015
"On the Importance of Normalisation Layers in Deep Learning with Piecewise Linear Activation Units@@@MIM","Gustavo Carneiro","University of Adelaide",2015-08-03,"70.8",2015
"Learning Activation Functions to Improve Deep Neural Networks@@@NiN+APL","Forest Agostinelli","University of Michigan",2014-12-21,"69.2",2014
"Learning Activation Functions to Improve Deep Neural Networks@@@NiN+APL","Matthew D. Hoffman","Adobe Systems",2014-12-21,"69.2",2014
"Learning Activation Functions to Improve Deep Neural Networks@@@NiN+APL","Peter Sadowski","University of California, Irvine",2014-12-21,"69.2",2014
"Learning Activation Functions to Improve Deep Neural Networks@@@NiN+APL","Pierre Baldi","University of California, Irvine",2014-12-21,"69.2",2014
"Stacked What-Where Auto-encoders@@@SWWAE","Junbo Jake Zhao","",2015-06-08,"69.1",2015
"Stacked What-Where Auto-encoders@@@SWWAE","Michael Mathieu","",2015-06-08,"69.1",2015
"Stacked What-Where Auto-encoders@@@SWWAE","Ross Goroshin","",2015-06-08,"69.1",2015
"Stacked What-Where Auto-encoders@@@SWWAE","Yann LeCun","",2015-06-08,"69.1",2015
"Deep Convolutional Decision Jungle for Image Classification@@@NiN+Superclass+CDJ","Seungryul Baek","",2017-06-06,"69",2017
"Deep Convolutional Decision Jungle for Image Classification@@@NiN+Superclass+CDJ","Kwang In Kim","",2017-06-06,"69",2017
"Deep Convolutional Decision Jungle for Image Classification@@@NiN+Superclass+CDJ","Tae-Kyun Kim","",2017-06-06,"69",2017
"@@@MLR DNN","","",NA,"68.5",NA
"Spectral Representations for Convolutional Neural Networks@@@Spectral Representations for Convolutional Neural Networks","Oren Rippel","Massachusetts Institute of Technology",2015-06-11,"68.4",2015
"Spectral Representations for Convolutional Neural Networks@@@Spectral Representations for Convolutional Neural Networks","Jasper Snoek","Harvard University",2015-06-11,"68.4",2015
"Spectral Representations for Convolutional Neural Networks@@@Spectral Representations for Convolutional Neural Networks","Ryan P. Adams","Harvard University",2015-06-11,"68.4",2015
"@@@RCNN-96","","",NA,"68.3",NA
"Training Very Deep Networks@@@VDN","Rupesh Kumar Srivastava","Dalle Molle Institute for Artificial Intelligence Research",2015-07-22,"67.8",2015
"Training Very Deep Networks@@@VDN","Klaus Greff","Dalle Molle Institute for Artificial Intelligence Research",2015-07-22,"67.8",2015
"Training Very Deep Networks@@@VDN","Juergen Schmidhuber","Dalle Molle Institute for Artificial Intelligence Research",2015-07-22,"67.8",2015
"Training Very Deep Networks@@@VDN","Jürgen Schmidhuber","Dalle Molle Institute for Artificial Intelligence Research",2015-07-22,"67.8",2015
"Deep Convolutional Neural Networks as Generic Feature Extractors@@@DCNN+GFE","Lars Hertel","",2017-10-06,"67.7",2017
"Deep Convolutional Neural Networks as Generic Feature Extractors@@@DCNN+GFE","Erhardt Barth","",2017-10-06,"67.7",2017
"Deep Convolutional Neural Networks as Generic Feature Extractors@@@DCNN+GFE","Thomas Käster","",2017-10-06,"67.7",2017
"Deep Convolutional Neural Networks as Generic Feature Extractors@@@DCNN+GFE","Thomas Martinetz","",2017-10-06,"67.7",2017
"Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree@@@Tree+Max-Avg pooling","Chen-Yu Lee","University of California, San Diego",2015-09-30,"67.6",2015
"Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree@@@Tree+Max-Avg pooling","Patrick W. Gallagher","",2015-09-30,"67.6",2015
"Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree@@@Tree+Max-Avg pooling","Zhuowen Tu","University of California, San Diego",2015-09-30,"67.6",2015
"HD-CNN: Hierarchical Deep Convolutional Neural Network for Large Scale Visual Recognition@@@HD-CNN","Zhicheng Yan","",2014-10-03,"67.4",2014
"HD-CNN: Hierarchical Deep Convolutional Neural Network for Large Scale Visual Recognition@@@HD-CNN","Hao Zhang","",2014-10-03,"67.4",2014
"HD-CNN: Hierarchical Deep Convolutional Neural Network for Large Scale Visual Recognition@@@HD-CNN","Robinson Piramuthu","",2014-10-03,"67.4",2014
"HD-CNN: Hierarchical Deep Convolutional Neural Network for Large Scale Visual Recognition@@@HD-CNN","Vignesh Jagadeesh","",2014-10-03,"67.4",2014
"HD-CNN: Hierarchical Deep Convolutional Neural Network for Large Scale Visual Recognition@@@HD-CNN","Dennis DeCoste","",2014-10-03,"67.4",2014
"HD-CNN: Hierarchical Deep Convolutional Neural Network for Large Scale Visual Recognition@@@HD-CNN","Wei Di","",2014-10-03,"67.4",2014
"HD-CNN: Hierarchical Deep Convolutional Neural Network for Large Scale Visual Recognition@@@HD-CNN","Yizhou Yu","",2014-10-03,"67.4",2014
"Universum Prescription: Regularization using Unlabeled Data@@@Universum Prescription","X.-C. Zhang","New York University",2015-11-11,"67.2",2015
"Universum Prescription: Regularization using Unlabeled Data@@@Universum Prescription","Xiang Zhang","New York University",2015-11-11,"67.2",2015
"Universum Prescription: Regularization using Unlabeled Data@@@Universum Prescription","Yann LeCun","New York University",2015-11-11,"67.2",2015
"ResNet50_on_Cifar_100_Without_Transfer_Learning@@@ResNet50 Without Transfer Learning","Ekin D. Cubuk","Google",2020-08-03,"67.060",2020
"ResNet50_on_Cifar_100_Without_Transfer_Learning@@@ResNet50 Without Transfer Learning","Barret Zoph","Google",2020-08-03,"67.060",2020
"ResNet50_on_Cifar_100_Without_Transfer_Learning@@@ResNet50 Without Transfer Learning","Dandelion Mane","",2020-08-03,"67.060",2020
"ResNet50_on_Cifar_100_Without_Transfer_Learning@@@ResNet50 Without Transfer Learning","Vijay K. Vasudevan","Google",2020-08-03,"67.060",2020
"ResNet50_on_Cifar_100_Without_Transfer_Learning@@@ResNet50 Without Transfer Learning","Quoc V. Le","Google",2020-08-03,"67.060",2020
"Striving for Simplicity: The All Convolutional Net@@@ACN","Jost Tobias Springenberg","University of Freiburg",2014-12-21,"66.3",2014
"Striving for Simplicity: The All Convolutional Net@@@ACN","Alexey Dosovitskiy","University of Freiburg",2014-12-21,"66.3",2014
"Striving for Simplicity: The All Convolutional Net@@@ACN","Thomas Brox","University of Freiburg",2014-12-21,"66.3",2014
"Striving for Simplicity: The All Convolutional Net@@@ACN","Martin Riedmiller","University of Freiburg",2014-12-21,"66.3",2014
"Deeply-Supervised Nets@@@DSN","Chen-Yu Lee","University of California, San Diego",2014-09-18,"65.4",2014
"Deeply-Supervised Nets@@@DSN","Saining Xie","University of California, San Diego",2014-09-18,"65.4",2014
"Deeply-Supervised Nets@@@DSN","Patrick W. Gallagher","",2014-09-18,"65.4",2014
"Deeply-Supervised Nets@@@DSN","Zhengyou Zhang","Microsoft",2014-09-18,"65.4",2014
"Deeply-Supervised Nets@@@DSN","Zhuowen Tu","University of California, San Diego",2014-09-18,"65.4",2014
"SpinalNet: Deep Neural Network with Gradual Input@@@VGG-16(Spinal FC)","H M Dipu Kabir","",2020-07-07,"64.99",2020
"SpinalNet: Deep Neural Network with Gradual Input@@@VGG-16(Spinal FC)","Moloud Abdar","",2020-07-07,"64.99",2020
"SpinalNet: Deep Neural Network with Gradual Input@@@VGG-16(Spinal FC)","Seyed Mohammad Jafar Jalali","",2020-07-07,"64.99",2020
"SpinalNet: Deep Neural Network with Gradual Input@@@VGG-16(Spinal FC)","Abbas Khosravi","",2020-07-07,"64.99",2020
"SpinalNet: Deep Neural Network with Gradual Input@@@VGG-16(Spinal FC)","Amir F. Atiya","",2020-07-07,"64.99",2020
"SpinalNet: Deep Neural Network with Gradual Input@@@VGG-16(Spinal FC)","Saeid Nahavandi","",2020-07-07,"64.99",2020
"SpinalNet: Deep Neural Network with Gradual Input@@@VGG-16(Spinal FC)","Dipti Srinivasan","",2020-07-07,"64.99",2020
"@@@Deep Representation Learning with Target Coding","","",NA,"64.8",NA
"Network In Network@@@NiN","Ming Liu","University of Washington",2013-12-16,"64.3",2013
"Network In Network@@@NiN","Liang Luo","University of Washington",2013-12-16,"64.3",2013
"Network In Network@@@NiN","Jacob Nelson","Microsoft",2013-12-16,"64.3",2013
"Network In Network@@@NiN","Luis Ceze","University of Washington",2013-12-16,"64.3",2013
"Network In Network@@@NiN","Arvind Krishnamurthy","University of Washington",2013-12-16,"64.3",2013
"Network In Network@@@NiN","Kishore Atreya","",2013-12-16,"64.3",2013
"Network In Network@@@NiN","Kishore Badari Atreya","",2013-12-16,"64.3",2013
"Discriminative Transfer Learning with Tree-based Priors@@@Tree Priors","Nitish Srivastava","University of Toronto",2013-12-01,"63.2",2013
"Discriminative Transfer Learning with Tree-based Priors@@@Tree Priors","Ruslan Salakhutdinov","University of Toronto",2013-12-01,"63.2",2013
"Improving Deep Neural Networks with Probabilistic Maxout Units@@@DNN+Probabilistic Maxout","Jost Tobias Springenberg","University of Freiburg",2013-12-20,"61.9",2013
"Improving Deep Neural Networks with Probabilistic Maxout Units@@@DNN+Probabilistic Maxout","Martin Riedmiller","University of Freiburg",2013-12-20,"61.9",2013
"Maxout Networks@@@Maxout Network (k=2)","Ian Goodfellow","Université de Montréal",2013-02-18,"61.43",2013
"Maxout Networks@@@Maxout Network (k=2)","David Warde-Farley","Université de Montréal",2013-02-18,"61.43",2013
"Maxout Networks@@@Maxout Network (k=2)","Mehdi Mirza","Université de Montréal",2013-02-18,"61.43",2013
"Maxout Networks@@@Maxout Network (k=2)","Aaron Courville","Université de Montréal",2013-02-18,"61.43",2013
"Maxout Networks@@@Maxout Network (k=2)","Yoshua Bengio","Université de Montréal",2013-02-18,"61.43",2013
"@@@Stable and Efficient Representation Learning with Nonnegativity Constraints ","","",NA,"60.8",NA
"Unsharp Masking Layer: Injecting Prior Knowledge in Convolutional Networks for Image Classification@@@ResNet20+UnsharpMaskLayer","Jose Carranza-Rojas","Costa Rica Institute of Technology",2019-09-29,"60.36",2019
"Unsharp Masking Layer: Injecting Prior Knowledge in Convolutional Networks for Image Classification@@@ResNet20+UnsharpMaskLayer","Saul Calderon-Ramirez","Costa Rica Institute of Technology",2019-09-29,"60.36",2019
"Unsharp Masking Layer: Injecting Prior Knowledge in Convolutional Networks for Image Classification@@@ResNet20+UnsharpMaskLayer","Adán Mora-Fallas","Costa Rica Institute of Technology",2019-09-29,"60.36",2019
"Unsharp Masking Layer: Injecting Prior Knowledge in Convolutional Networks for Image Classification@@@ResNet20+UnsharpMaskLayer","Michael Granados-Menani","Costa Rica Institute of Technology",2019-09-29,"60.36",2019
"Unsharp Masking Layer: Injecting Prior Knowledge in Convolutional Networks for Image Classification@@@ResNet20+UnsharpMaskLayer","Jordina Torrents-Barrena","Rovira i Virgili University",2019-09-29,"60.36",2019
"Empirical Evaluation of Rectified Activations in Convolutional Network@@@RReLU","Bing Xu","",2015-05-05,"59.8",2015
"Empirical Evaluation of Rectified Activations in Convolutional Network@@@RReLU","Naiyan Wang","",2015-05-05,"59.8",2015
"Empirical Evaluation of Rectified Activations in Convolutional Network@@@RReLU","Tianqi Chen","",2015-05-05,"59.8",2015
"Empirical Evaluation of Rectified Activations in Convolutional Network@@@RReLU","Mu Li","",2015-05-05,"59.8",2015
"Stochastic Pooling for Regularization of Deep Convolutional Neural Networks@@@Stochastic Pooling","Matthew D. Zeiler","New York University",2013-01-16,"57.5",2013
"Stochastic Pooling for Regularization of Deep Convolutional Neural Networks@@@Stochastic Pooling","Rob Fergus","New York University",2013-01-16,"57.5",2013
"@@@Smooth Pooling Regions","","",NA,"56.3",NA
"@@@Receptive Field Learning","","",NA,"54.2",NA
"Improving Neural Architecture Search Image Classifiers via Ensemble Learning@@@ASANas","Vladimir Macko","",2019-03-14,"not available",2019
"Improving Neural Architecture Search Image Classifiers via Ensemble Learning@@@ASANas","Charles Weill","",2019-03-14,"not available",2019
"Improving Neural Architecture Search Image Classifiers via Ensemble Learning@@@ASANas","Hanna Mazzawi","",2019-03-14,"not available",2019
"Improving Neural Architecture Search Image Classifiers via Ensemble Learning@@@ASANas","Javier Gonzalvo","",2019-03-14,"not available",2019
"Densely Connected Convolutional Networks@@@DenseNet-BC","Gao Huang","Cornell University",2016-08-25,"not available",2016
"Densely Connected Convolutional Networks@@@DenseNet-BC","Zhuang Liu","Tsinghua University",2016-08-25,"not available",2016
"Densely Connected Convolutional Networks@@@DenseNet-BC","Laurens van der Maaten","Facebook",2016-08-25,"not available",2016
"Densely Connected Convolutional Networks@@@DenseNet-BC","Kilian Q. Weinberger","Cornell University",2016-08-25,"not available",2016
"Learning Implicitly Recurrent CNNs Through Parameter Sharing@@@Shared WRN","Pedro Savarese","Toyota Technological Institute at Chicago",2019-02-26,"not available",2019
"Learning Implicitly Recurrent CNNs Through Parameter Sharing@@@Shared WRN","Michael Maire","University of Chicago",2019-02-26,"not available",2019
"How Important is Weight Symmetry in Backpropagation?@@@Sign-symmetry","Qianli Liao","Massachusetts Institute of Technology",2015-10-17,"not available",2015
"How Important is Weight Symmetry in Backpropagation?@@@Sign-symmetry","Joel Z. Leibo","Massachusetts Institute of Technology",2015-10-17,"not available",2015
"How Important is Weight Symmetry in Backpropagation?@@@Sign-symmetry","Tomaso Poggio","Massachusetts Institute of Technology",2015-10-17,"not available",2015
"Outlier Exposure with Confidence Control for Out-of-Distribution Detection@@@WRN 40-2 + OECC","Aristotelis-Angelos Papadopoulos","University of Southern California",2019-06-08,"not available",2019
"Outlier Exposure with Confidence Control for Out-of-Distribution Detection@@@WRN 40-2 + OECC","Mohammad Reza Rajati","University of Southern California",2019-06-08,"not available",2019
"Outlier Exposure with Confidence Control for Out-of-Distribution Detection@@@WRN 40-2 + OECC","Nazim Shaikh","University of Southern California",2019-06-08,"not available",2019
"Outlier Exposure with Confidence Control for Out-of-Distribution Detection@@@WRN 40-2 + OECC","Jiamian Wang","University of Southern California",2019-06-08,"not available",2019
"Deep Anomaly Detection with Outlier Exposure@@@WRN 40-2 + OE","Dan Hendrycks","University of California, Berkeley",2018-12-11,"not available",2018
"Deep Anomaly Detection with Outlier Exposure@@@WRN 40-2 + OE","Mantas Mazeika","University of Chicago",2018-12-11,"not available",2018
"Deep Anomaly Detection with Outlier Exposure@@@WRN 40-2 + OE","Thomas G. Dietterich","Oregon State University",2018-12-11,"not available",2018
"Deep Anomaly Detection with Outlier Exposure@@@WRN 40-2 (MSP Baseline)","Dan Hendrycks","University of California, Berkeley",2018-12-11,"not available",2018
"Deep Anomaly Detection with Outlier Exposure@@@WRN 40-2 (MSP Baseline)","Mantas Mazeika","University of Chicago",2018-12-11,"not available",2018
"Deep Anomaly Detection with Outlier Exposure@@@WRN 40-2 (MSP Baseline)","Thomas G. Dietterich","Oregon State University",2018-12-11,"not available",2018
"@@@PreResNet-101","","",NA,"not available",NA
"Network Pruning via Transformable Architecture Search@@@TAS-pruned ResNet-110","Xuanyi Dong","University of Technology, Sydney",2019-05-23,"not available",2019
"Network Pruning via Transformable Architecture Search@@@TAS-pruned ResNet-110","Yi Yang","University of Technology, Sydney",2019-05-23,"not available",2019
