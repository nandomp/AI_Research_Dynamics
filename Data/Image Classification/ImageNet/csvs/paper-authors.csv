"title","authors","affiliations","paper_date","metric","year"
"Feature Denoising for Improving Adversarial Robustness@@@Feature Denoising","Cihang Xie","Johns Hopkins University",2018-12-09,"not available",2018
"Feature Denoising for Improving Adversarial Robustness@@@Feature Denoising","Yuxin Wu","Facebook",2018-12-09,"not available",2018
"Feature Denoising for Improving Adversarial Robustness@@@Feature Denoising","Laurens van der Maaten","Facebook",2018-12-09,"not available",2018
"Feature Denoising for Improving Adversarial Robustness@@@Feature Denoising","Alan L. Yuille","Johns Hopkins University",2018-12-09,"not available",2018
"Feature Denoising for Improving Adversarial Robustness@@@Feature Denoising","Kaiming He","Facebook",2018-12-09,"not available",2018
"Neural Architecture Transfer@@@NAT-M4","","",2020-05-12,"not available",2020
"NSGANetV2: Evolutionary Multi-Objective Surrogate-Assisted Neural Architecture Search@@@NSGANetV2-xl","Zhichao Lu","Michigan State University",2020-07-20,"not available",2020
"NSGANetV2: Evolutionary Multi-Objective Surrogate-Assisted Neural Architecture Search@@@NSGANetV2-xl","Kalyanmoy Deb","Michigan State University",2020-07-20,"not available",2020
"NSGANetV2: Evolutionary Multi-Objective Surrogate-Assisted Neural Architecture Search@@@NSGANetV2-xl","Erik D. Goodman","Michigan State University",2020-07-20,"not available",2020
"NSGANetV2: Evolutionary Multi-Objective Surrogate-Assisted Neural Architecture Search@@@NSGANetV2-xl","Wolfgang Banzhaf","Michigan State University",2020-07-20,"not available",2020
"NSGANetV2: Evolutionary Multi-Objective Surrogate-Assisted Neural Architecture Search@@@NSGANetV2-xl","Vishnu Naresh Boddeti","Michigan State University",2020-07-20,"not available",2020
"Neural Architecture Transfer@@@NAT-M3","Catherine Wong","Massachusetts Institute of Technology",2020-05-12,"not available",2020
"Neural Architecture Transfer@@@NAT-M3","Neil Houlsby","Google",2020-05-12,"not available",2020
"Neural Architecture Transfer@@@NAT-M3","Yifeng Lu","",2020-05-12,"not available",2020
"Neural Architecture Transfer@@@NAT-M3","Andrea Gesmundo","Google",2020-05-12,"not available",2020
"NSGANetV2: Evolutionary Multi-Objective Surrogate-Assisted Neural Architecture Search@@@NSGANetV2-l","Zhichao Lu","Michigan State University",2020-07-20,"not available",2020
"NSGANetV2: Evolutionary Multi-Objective Surrogate-Assisted Neural Architecture Search@@@NSGANetV2-l","Kalyanmoy Deb","Michigan State University",2020-07-20,"not available",2020
"NSGANetV2: Evolutionary Multi-Objective Surrogate-Assisted Neural Architecture Search@@@NSGANetV2-l","Erik D. Goodman","Michigan State University",2020-07-20,"not available",2020
"NSGANetV2: Evolutionary Multi-Objective Surrogate-Assisted Neural Architecture Search@@@NSGANetV2-l","Wolfgang Banzhaf","Michigan State University",2020-07-20,"not available",2020
"NSGANetV2: Evolutionary Multi-Objective Surrogate-Assisted Neural Architecture Search@@@NSGANetV2-l","Vishnu Naresh Boddeti","Michigan State University",2020-07-20,"not available",2020
"Neural Architecture Transfer@@@NAT-M2","Catherine Wong","Massachusetts Institute of Technology",2020-05-12,"not available",2020
"Neural Architecture Transfer@@@NAT-M2","Neil Houlsby","Google",2020-05-12,"not available",2020
"Neural Architecture Transfer@@@NAT-M2","Yifeng Lu","",2020-05-12,"not available",2020
"Neural Architecture Transfer@@@NAT-M2","Andrea Gesmundo","Google",2020-05-12,"not available",2020
"NSGANetV2: Evolutionary Multi-Objective Surrogate-Assisted Neural Architecture Search@@@NSGANetV2-m","Zhichao Lu","Michigan State University",2020-07-20,"not available",2020
"NSGANetV2: Evolutionary Multi-Objective Surrogate-Assisted Neural Architecture Search@@@NSGANetV2-m","Kalyanmoy Deb","Michigan State University",2020-07-20,"not available",2020
"NSGANetV2: Evolutionary Multi-Objective Surrogate-Assisted Neural Architecture Search@@@NSGANetV2-m","Erik D. Goodman","Michigan State University",2020-07-20,"not available",2020
"NSGANetV2: Evolutionary Multi-Objective Surrogate-Assisted Neural Architecture Search@@@NSGANetV2-m","Wolfgang Banzhaf","Michigan State University",2020-07-20,"not available",2020
"NSGANetV2: Evolutionary Multi-Objective Surrogate-Assisted Neural Architecture Search@@@NSGANetV2-m","Vishnu Naresh Boddeti","Michigan State University",2020-07-20,"not available",2020
"Noisy Differentiable Architecture Search@@@NoisyDARTS-A","Xiangxiang Chu","Xiaomi",2020-05-07,"not available",2020
"Noisy Differentiable Architecture Search@@@NoisyDARTS-A","Bo Zhang","Xiaomi",2020-05-07,"not available",2020
"Noisy Differentiable Architecture Search@@@NoisyDARTS-A","Xudong Li","",2020-05-07,"not available",2020
"Neural Architecture Search for Lightweight Non-Local Networks@@@AutoNL-L","Yingwei Li","Johns Hopkins University",2020-04-04,"not available",2020
"Neural Architecture Search for Lightweight Non-Local Networks@@@AutoNL-L","Xiaojie Jin","National University of Singapore",2020-04-04,"not available",2020
"Neural Architecture Search for Lightweight Non-Local Networks@@@AutoNL-L","Jieru Mei","Johns Hopkins University",2020-04-04,"not available",2020
"Neural Architecture Search for Lightweight Non-Local Networks@@@AutoNL-L","Xiaochen Lian","University of California, Los Angeles",2020-04-04,"not available",2020
"Neural Architecture Search for Lightweight Non-Local Networks@@@AutoNL-L","Linjie Yang","The Chinese University of Hong Kong",2020-04-04,"not available",2020
"Neural Architecture Search for Lightweight Non-Local Networks@@@AutoNL-L","Cihang Xie","Johns Hopkins University",2020-04-04,"not available",2020
"Neural Architecture Search for Lightweight Non-Local Networks@@@AutoNL-L","Qihang Yu","Johns Hopkins University",2020-04-04,"not available",2020
"Neural Architecture Search for Lightweight Non-Local Networks@@@AutoNL-L","Yuyin Zhou","Johns Hopkins University",2020-04-04,"not available",2020
"Neural Architecture Search for Lightweight Non-Local Networks@@@AutoNL-L","Song Bai","Huazhong University of Science and Technology",2020-04-04,"not available",2020
"Neural Architecture Search for Lightweight Non-Local Networks@@@AutoNL-L","Alan L. Yuille","Johns Hopkins University",2020-04-04,"not available",2020
"Neural Architecture Transfer@@@NAT-M1","Catherine Wong","Massachusetts Institute of Technology",2020-05-12,"not available",2020
"Neural Architecture Transfer@@@NAT-M1","Neil Houlsby","Google",2020-05-12,"not available",2020
"Neural Architecture Transfer@@@NAT-M1","Yifeng Lu","",2020-05-12,"not available",2020
"Neural Architecture Transfer@@@NAT-M1","Andrea Gesmundo","Google",2020-05-12,"not available",2020
"NSGANetV2: Evolutionary Multi-Objective Surrogate-Assisted Neural Architecture Search@@@NSGANetV2-s","Zhichao Lu","Michigan State University",2020-07-20,"not available",2020
"NSGANetV2: Evolutionary Multi-Objective Surrogate-Assisted Neural Architecture Search@@@NSGANetV2-s","Kalyanmoy Deb","Michigan State University",2020-07-20,"not available",2020
"NSGANetV2: Evolutionary Multi-Objective Surrogate-Assisted Neural Architecture Search@@@NSGANetV2-s","Erik D. Goodman","Michigan State University",2020-07-20,"not available",2020
"NSGANetV2: Evolutionary Multi-Objective Surrogate-Assisted Neural Architecture Search@@@NSGANetV2-s","Wolfgang Banzhaf","Michigan State University",2020-07-20,"not available",2020
"NSGANetV2: Evolutionary Multi-Objective Surrogate-Assisted Neural Architecture Search@@@NSGANetV2-s","Vishnu Naresh Boddeti","Michigan State University",2020-07-20,"not available",2020
"GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet@@@GreedyNAS-A","Shan You","SenseTime",2020-03-25,"not available",2020
"GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet@@@GreedyNAS-A","Tao Huang","",2020-03-25,"not available",2020
"GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet@@@GreedyNAS-A","Mingmin Yang","",2020-03-25,"not available",2020
"GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet@@@GreedyNAS-A","Fei Wang","Yale University",2020-03-25,"not available",2020
"GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet@@@GreedyNAS-A","Chen Qian","",2020-03-25,"not available",2020
"GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet@@@GreedyNAS-A","Changshui Zhang","Tsinghua University",2020-03-25,"not available",2020
"SCARLET-NAS: Bridging the gap between Stability and Scalability in Weight-sharing Neural Architecture Search@@@SCARLET-A","Xiangxiang Chu","Xiaomi",2019-08-16,"not available",2019
"SCARLET-NAS: Bridging the gap between Stability and Scalability in Weight-sharing Neural Architecture Search@@@SCARLET-A","Bo Zhang","",2019-08-16,"not available",2019
"SCARLET-NAS: Bridging the gap between Stability and Scalability in Weight-sharing Neural Architecture Search@@@SCARLET-A","Li Jixiang","Xiaomi",2019-08-16,"not available",2019
"SCARLET-NAS: Bridging the gap between Stability and Scalability in Weight-sharing Neural Architecture Search@@@SCARLET-A","Li Qingyuan","Xiaomi",2019-08-16,"not available",2019
"SCARLET-NAS: Bridging the gap between Stability and Scalability in Weight-sharing Neural Architecture Search@@@SCARLET-A","Xu Ruijun","Xiaomi",2019-08-16,"not available",2019
"GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet@@@GreedyNAS-B","Shan You","Tsinghua University",2020-03-25,"not available",2020
"GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet@@@GreedyNAS-B","Tao Huang","",2020-03-25,"not available",2020
"GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet@@@GreedyNAS-B","Mingmin Yang","",2020-03-25,"not available",2020
"GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet@@@GreedyNAS-B","Fei Wang","",2020-03-25,"not available",2020
"GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet@@@GreedyNAS-B","Chen Qian","",2020-03-25,"not available",2020
"GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet@@@GreedyNAS-B","Changshui Zhang","Tsinghua University",2020-03-25,"not available",2020
"MUXConv: Information Multiplexing in Convolutional Neural Networks@@@MUXNet-l","Zhichao Lu","Michigan State University",2020-03-31,"not available",2020
"MUXConv: Information Multiplexing in Convolutional Neural Networks@@@MUXNet-l","Kalyanmoy Deb","Michigan State University",2020-03-31,"not available",2020
"MUXConv: Information Multiplexing in Convolutional Neural Networks@@@MUXNet-l","Vishnu Naresh Boddeti","Michigan State University",2020-03-31,"not available",2020
"Neural Architecture Search with GBDT@@@GBDT-NAS-3S","Renqian Luo","University of Science and Technology of China",2020-07-09,"not available",2020
"Neural Architecture Search with GBDT@@@GBDT-NAS-3S","Xu Tan","Microsoft",2020-07-09,"not available",2020
"Neural Architecture Search with GBDT@@@GBDT-NAS-3S","Rui Wang","Microsoft",2020-07-09,"not available",2020
"Neural Architecture Search with GBDT@@@GBDT-NAS-3S","Tao Qin","Microsoft",2020-07-09,"not available",2020
"Neural Architecture Search with GBDT@@@GBDT-NAS-3S","Enhong Chen","University of Science and Technology of China",2020-07-09,"not available",2020
"Neural Architecture Search with GBDT@@@GBDT-NAS-3S","Tie-Yan Liu","Microsoft",2020-07-09,"not available",2020
"Semi-Supervised Neural Architecture Search@@@SemiNAS","Renqian Luo","",2020-02-24,"not available",2020
"Semi-Supervised Neural Architecture Search@@@SemiNAS","Xu Tan","",2020-02-24,"not available",2020
"Semi-Supervised Neural Architecture Search@@@SemiNAS","Rui Wang","",2020-02-24,"not available",2020
"Semi-Supervised Neural Architecture Search@@@SemiNAS","Tao Qin","Microsoft",2020-02-24,"not available",2020
"Semi-Supervised Neural Architecture Search@@@SemiNAS","Enhong Chen","",2020-02-24,"not available",2020
"Semi-Supervised Neural Architecture Search@@@SemiNAS","Tie-Yan Liu","",2020-02-24,"not available",2020
"Neural Architecture Search for Lightweight Non-Local Networks@@@AutoNL-S","Yingwei Li","Johns Hopkins University",2020-04-04,"not available",2020
"Neural Architecture Search for Lightweight Non-Local Networks@@@AutoNL-S","Xiaojie Jin","National University of Singapore",2020-04-04,"not available",2020
"Neural Architecture Search for Lightweight Non-Local Networks@@@AutoNL-S","Jieru Mei","Johns Hopkins University",2020-04-04,"not available",2020
"Neural Architecture Search for Lightweight Non-Local Networks@@@AutoNL-S","Xiaochen Lian","University of California, Los Angeles",2020-04-04,"not available",2020
"Neural Architecture Search for Lightweight Non-Local Networks@@@AutoNL-S","Linjie Yang","The Chinese University of Hong Kong",2020-04-04,"not available",2020
"Neural Architecture Search for Lightweight Non-Local Networks@@@AutoNL-S","Cihang Xie","Johns Hopkins University",2020-04-04,"not available",2020
"Neural Architecture Search for Lightweight Non-Local Networks@@@AutoNL-S","Qihang Yu","Johns Hopkins University",2020-04-04,"not available",2020
"Neural Architecture Search for Lightweight Non-Local Networks@@@AutoNL-S","Yuyin Zhou","Johns Hopkins University",2020-04-04,"not available",2020
"Neural Architecture Search for Lightweight Non-Local Networks@@@AutoNL-S","Song Bai","Huazhong University of Science and Technology",2020-04-04,"not available",2020
"Neural Architecture Search for Lightweight Non-Local Networks@@@AutoNL-S","Alan L. Yuille","Johns Hopkins University",2020-04-04,"not available",2020
"SCARLET-NAS: Bridging the gap between Stability and Scalability in Weight-sharing Neural Architecture Search@@@SCARLET-B","Xiangxiang Chu","Xiaomi",2019-08-16,"not available",2019
"SCARLET-NAS: Bridging the gap between Stability and Scalability in Weight-sharing Neural Architecture Search@@@SCARLET-B","Bo Zhang","Xiaomi",2019-08-16,"not available",2019
"SCARLET-NAS: Bridging the gap between Stability and Scalability in Weight-sharing Neural Architecture Search@@@SCARLET-B","Li Jixiang","Xiaomi",2019-08-16,"not available",2019
"SCARLET-NAS: Bridging the gap between Stability and Scalability in Weight-sharing Neural Architecture Search@@@SCARLET-B","Li Qingyuan","Xiaomi",2019-08-16,"not available",2019
"SCARLET-NAS: Bridging the gap between Stability and Scalability in Weight-sharing Neural Architecture Search@@@SCARLET-B","Xu Ruijun","Xiaomi",2019-08-16,"not available",2019
"GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet@@@GreedyNAS-C","Shan You","Tsinghua University",2020-03-25,"not available",2020
"GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet@@@GreedyNAS-C","Tao Huang","",2020-03-25,"not available",2020
"GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet@@@GreedyNAS-C","Mingmin Yang","",2020-03-25,"not available",2020
"GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet@@@GreedyNAS-C","Fei Wang","",2020-03-25,"not available",2020
"GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet@@@GreedyNAS-C","Chen Qian","",2020-03-25,"not available",2020
"GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet@@@GreedyNAS-C","Changshui Zhang","Tsinghua University",2020-03-25,"not available",2020
"A Generic Graph-based Neural Architecture Encoding Scheme for Predictor-based NAS@@@GATES","Xuefei Ning","",2020-04-04,"not available",2020
"A Generic Graph-based Neural Architecture Encoding Scheme for Predictor-based NAS@@@GATES","Yin Zheng","",2020-04-04,"not available",2020
"A Generic Graph-based Neural Architecture Encoding Scheme for Predictor-based NAS@@@GATES","Tianchen Zhao","",2020-04-04,"not available",2020
"A Generic Graph-based Neural Architecture Encoding Scheme for Predictor-based NAS@@@GATES","Yu Wang","",2020-04-04,"not available",2020
"A Generic Graph-based Neural Architecture Encoding Scheme for Predictor-based NAS@@@GATES","Huazhong Yang","",2020-04-04,"not available",2020
"SGAS: Sequential Greedy Architecture Search@@@SGAS","","",2019-11-30,"not available",2019
"PC-DARTS: Partial Channel Connections for Memory-Efficient Architecture Search@@@PC-DARTS-ImagNet","Yuhui Xu","Shanghai Jiao Tong University",2019-07-12,"not available",2019
"PC-DARTS: Partial Channel Connections for Memory-Efficient Architecture Search@@@PC-DARTS-ImagNet","Lingxi Xie","Huawei",2019-07-12,"not available",2019
"PC-DARTS: Partial Channel Connections for Memory-Efficient Architecture Search@@@PC-DARTS-ImagNet","Xiaopeng Zhang","",2019-07-12,"not available",2019
"PC-DARTS: Partial Channel Connections for Memory-Efficient Architecture Search@@@PC-DARTS-ImagNet","Xin Chen","Tongji University",2019-07-12,"not available",2019
"PC-DARTS: Partial Channel Connections for Memory-Efficient Architecture Search@@@PC-DARTS-ImagNet","Guo-Jun Qi","University of Central Florida",2019-07-12,"not available",2019
"PC-DARTS: Partial Channel Connections for Memory-Efficient Architecture Search@@@PC-DARTS-ImagNet","Qi Tian","Huawei",2019-07-12,"not available",2019
"PC-DARTS: Partial Channel Connections for Memory-Efficient Architecture Search@@@PC-DARTS-ImagNet","Hongkai Xiong","Shanghai Jiao Tong University",2019-07-12,"not available",2019
"Single Path One-Shot Neural Architecture Search with Uniform Sampling@@@OneShot-S+","Zichao Guo","",2019-03-31,"not available",2019
"Single Path One-Shot Neural Architecture Search with Uniform Sampling@@@OneShot-S+","Xiangyu Zhang","",2019-03-31,"not available",2019
"Single Path One-Shot Neural Architecture Search with Uniform Sampling@@@OneShot-S+","Haoyuan Mu","",2019-03-31,"not available",2019
"Single Path One-Shot Neural Architecture Search with Uniform Sampling@@@OneShot-S+","Wen Heng","",2019-03-31,"not available",2019
"Single Path One-Shot Neural Architecture Search with Uniform Sampling@@@OneShot-S+","Zechun Liu","",2019-03-31,"not available",2019
"Single Path One-Shot Neural Architecture Search with Uniform Sampling@@@OneShot-S+","Yichen Wei","",2019-03-31,"not available",2019
"Single Path One-Shot Neural Architecture Search with Uniform Sampling@@@OneShot-S+","Jian Sun","",2019-03-31,"not available",2019
"SCARLET-NAS: Bridging the gap between Stability and Scalability in Weight-sharing Neural Architecture Search@@@SCARLET-C","Xiangxiang Chu","Xiaomi",2019-08-16,"not available",2019
"SCARLET-NAS: Bridging the gap between Stability and Scalability in Weight-sharing Neural Architecture Search@@@SCARLET-C","Bo Zhang","Xiaomi",2019-08-16,"not available",2019
"SCARLET-NAS: Bridging the gap between Stability and Scalability in Weight-sharing Neural Architecture Search@@@SCARLET-C","Li Jixiang","Xiaomi",2019-08-16,"not available",2019
"SCARLET-NAS: Bridging the gap between Stability and Scalability in Weight-sharing Neural Architecture Search@@@SCARLET-C","Li Qingyuan","Xiaomi",2019-08-16,"not available",2019
"SCARLET-NAS: Bridging the gap between Stability and Scalability in Weight-sharing Neural Architecture Search@@@SCARLET-C","Xu Ruijun","Xiaomi",2019-08-16,"not available",2019
"AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search@@@AlphaX-1","Linnan Wang","",2019-03-26,"not available",2019
"AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search@@@AlphaX-1","Yiyang Zhao","",2019-03-26,"not available",2019
"AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search@@@AlphaX-1","Yuu Jinnai","",2019-03-26,"not available",2019
"AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search@@@AlphaX-1","Yuandong Tian","",2019-03-26,"not available",2019
"AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search@@@AlphaX-1","Rodrigo Fonseca","",2019-03-26,"not available",2019
"@@@Evo-NAS","","",NA,"not available",NA
"MUXConv: Information Multiplexing in Convolutional Neural Networks@@@MUXNet-m","Zhichao Lu","Michigan State University",2020-03-31,"not available",2020
"MUXConv: Information Multiplexing in Convolutional Neural Networks@@@MUXNet-m","Kalyanmoy Deb","Michigan State University",2020-03-31,"not available",2020
"MUXConv: Information Multiplexing in Convolutional Neural Networks@@@MUXNet-m","Vishnu Naresh Boddeti","Michigan State University",2020-03-31,"not available",2020
"ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware@@@ProxylesNAS","Han Cai","Shanghai Jiao Tong University",2018-12-02,"not available",2018
"ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware@@@ProxylesNAS","Ligeng Zhu","Massachusetts Institute of Technology",2018-12-02,"not available",2018
"ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware@@@ProxylesNAS","Song Han","Massachusetts Institute of Technology",2018-12-02,"not available",2018
"@@@GDAS","","",NA,"not available",NA
"DARTS: Differentiable Architecture Search@@@DARTS","Hanxiao Liu","Google",2018-06-24,"not available",2018
"DARTS: Differentiable Architecture Search@@@DARTS","Karen Simonyan","Google",2018-06-24,"not available",2018
"DARTS: Differentiable Architecture Search@@@DARTS","Yiming Yang","Carnegie Mellon University",2018-06-24,"not available",2018
"MUXConv: Information Multiplexing in Convolutional Neural Networks@@@MUXNet-s","Zhichao Lu","Michigan State University",2020-03-31,"not available",2020
"MUXConv: Information Multiplexing in Convolutional Neural Networks@@@MUXNet-s","Kalyanmoy Deb","Michigan State University",2020-03-31,"not available",2020
"MUXConv: Information Multiplexing in Convolutional Neural Networks@@@MUXNet-s","Vishnu Naresh Boddeti","Michigan State University",2020-03-31,"not available",2020
"MUXConv: Information Multiplexing in Convolutional Neural Networks@@@MUXNet-xs","Zhichao Lu","Michigan State University",2020-03-31,"not available",2020
"MUXConv: Information Multiplexing in Convolutional Neural Networks@@@MUXNet-xs","Kalyanmoy Deb","Michigan State University",2020-03-31,"not available",2020
"MUXConv: Information Multiplexing in Convolutional Neural Networks@@@MUXNet-xs","Vishnu Naresh Boddeti","Michigan State University",2020-03-31,"not available",2020
"Once-for-All: Train One Network and Specialize it for Efficient Deployment@@@OFA w/ PS #75","Han Cai","Massachusetts Institute of Technology",2019-08-26,"not available",2019
"Once-for-All: Train One Network and Specialize it for Efficient Deployment@@@OFA w/ PS #75","Chuang Gan","",2019-08-26,"not available",2019
"Once-for-All: Train One Network and Specialize it for Efficient Deployment@@@OFA w/ PS #75","Tianzhe Wang","Massachusetts Institute of Technology",2019-08-26,"not available",2019
"Once-for-All: Train One Network and Specialize it for Efficient Deployment@@@OFA w/ PS #75","Zhekai Zhang","",2019-08-26,"not available",2019
"Once-for-All: Train One Network and Specialize it for Efficient Deployment@@@OFA w/ PS #75","Song Han","Massachusetts Institute of Technology",2019-08-26,"not available",2019
"SCAN: Learning to Classify Images without Labels@@@SCAN","Wouter Van Gansbeke","",2020-05-25,"not available",2020
"SCAN: Learning to Classify Images without Labels@@@SCAN","Simon Vandenhende","",2020-05-25,"not available",2020
"SCAN: Learning to Classify Images without Labels@@@SCAN","Stamatios Georgoulis","",2020-05-25,"not available",2020
"SCAN: Learning to Classify Images without Labels@@@SCAN","Marc Proesmans","",2020-05-25,"not available",2020
"SCAN: Learning to Classify Images without Labels@@@SCAN","Luc Van Gool","",2020-05-25,"not available",2020
"Self-labelling via simultaneous clustering and representation learning@@@SeLa","Yuki M. Asano","University of Oxford",2019-11-13,"not available",2019
"Self-labelling via simultaneous clustering and representation learning@@@SeLa","Christian Rupprecht","University of Oxford",2019-11-13,"not available",2019
"Self-labelling via simultaneous clustering and representation learning@@@SeLa","Andrea Vedaldi","University of Oxford",2019-11-13,"not available",2019
"Sharpness-Aware Minimization for Efficiently Improving Generalization@@@EfficientNet-L2-475 + SAM","Pierre Foret","Google",2020-10-03,"88.61%",2020
"Sharpness-Aware Minimization for Efficiently Improving Generalization@@@EfficientNet-L2-475 + SAM","Ariel Kleiner","Google",2020-10-03,"88.61%",2020
"Sharpness-Aware Minimization for Efficiently Improving Generalization@@@EfficientNet-L2-475 + SAM","Hossein Mobahi","Google",2020-10-03,"88.61%",2020
"Sharpness-Aware Minimization for Efficiently Improving Generalization@@@EfficientNet-L2-475 + SAM","Behnam Neyshabur","Google",2020-10-03,"88.61%",2020
"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale@@@ViT-H/14","Alexey Dosovitskiy","",2020-10-22,"88.55%",2020
"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale@@@ViT-H/14","Lucas Beyer","",2020-10-22,"88.55%",2020
"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale@@@ViT-H/14","Alexander Kolesnikov","",2020-10-22,"88.55%",2020
"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale@@@ViT-H/14","Dirk Weissenborn","",2020-10-22,"88.55%",2020
"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale@@@ViT-H/14","Xiaohua Zhai","",2020-10-22,"88.55%",2020
"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale@@@ViT-H/14","Thomas Unterthiner","",2020-10-22,"88.55%",2020
"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale@@@ViT-H/14","Mostafa Dehghani","",2020-10-22,"88.55%",2020
"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale@@@ViT-H/14","Matthias Minderer","",2020-10-22,"88.55%",2020
"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale@@@ViT-H/14","Georg Heigold","",2020-10-22,"88.55%",2020
"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale@@@ViT-H/14","Sylvain Gelly","",2020-10-22,"88.55%",2020
"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale@@@ViT-H/14","Jakob Uszkoreit","",2020-10-22,"88.55%",2020
"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale@@@ViT-H/14","Neil Houlsby","",2020-10-22,"88.55%",2020
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNet-L2","Hugo Touvron","",2020-03-18,"88.5%",2020
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNet-L2","Andrea Vedaldi","",2020-03-18,"88.5%",2020
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNet-L2","Matthijs Douze","",2020-03-18,"88.5%",2020
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNet-L2","Hervé Jégou","",2020-03-18,"88.5%",2020
"Self-training with Noisy Student improves ImageNet classification@@@NoisyStudent (EfficientNet-L2)","Qizhe Xie","Carnegie Mellon University",2019-11-11,"88.4%",2019
"Self-training with Noisy Student improves ImageNet classification@@@NoisyStudent (EfficientNet-L2)","Minh-Thang Luong","Google",2019-11-11,"88.4%",2019
"Self-training with Noisy Student improves ImageNet classification@@@NoisyStudent (EfficientNet-L2)","Eduard Hovy","Carnegie Mellon University",2019-11-11,"88.4%",2019
"Self-training with Noisy Student improves ImageNet classification@@@NoisyStudent (EfficientNet-L2)","Quoc V. Le","Google",2019-11-11,"88.4%",2019
"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale@@@ViT-L/16","Alexey Dosovitskiy","",2020-10-22,"87.76%",2020
"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale@@@ViT-L/16","Lucas Beyer","",2020-10-22,"87.76%",2020
"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale@@@ViT-L/16","Alexander Kolesnikov","",2020-10-22,"87.76%",2020
"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale@@@ViT-L/16","Dirk Weissenborn","",2020-10-22,"87.76%",2020
"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale@@@ViT-L/16","Xiaohua Zhai","",2020-10-22,"87.76%",2020
"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale@@@ViT-L/16","Thomas Unterthiner","",2020-10-22,"87.76%",2020
"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale@@@ViT-L/16","Mostafa Dehghani","",2020-10-22,"87.76%",2020
"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale@@@ViT-L/16","Matthias Minderer","",2020-10-22,"87.76%",2020
"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale@@@ViT-L/16","Georg Heigold","",2020-10-22,"87.76%",2020
"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale@@@ViT-L/16","Sylvain Gelly","",2020-10-22,"87.76%",2020
"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale@@@ViT-L/16","Jakob Uszkoreit","",2020-10-22,"87.76%",2020
"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale@@@ViT-L/16","Neil Houlsby","",2020-10-22,"87.76%",2020
"Big Transfer (BiT): General Visual Representation Learning@@@BiT-L (ResNet)","Alexander Kolesnikov","Google",2019-12-24,"87.54%",2019
"Big Transfer (BiT): General Visual Representation Learning@@@BiT-L (ResNet)","Lucas Beyer","Google",2019-12-24,"87.54%",2019
"Big Transfer (BiT): General Visual Representation Learning@@@BiT-L (ResNet)","Xiaohua Zhai","Google",2019-12-24,"87.54%",2019
"Big Transfer (BiT): General Visual Representation Learning@@@BiT-L (ResNet)","Joan Puigcerver","Google",2019-12-24,"87.54%",2019
"Big Transfer (BiT): General Visual Representation Learning@@@BiT-L (ResNet)","Jessica Yung","Google",2019-12-24,"87.54%",2019
"Big Transfer (BiT): General Visual Representation Learning@@@BiT-L (ResNet)","Sylvain Gelly","Google",2019-12-24,"87.54%",2019
"Big Transfer (BiT): General Visual Representation Learning@@@BiT-L (ResNet)","Neil Houlsby","Google",2019-12-24,"87.54%",2019
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNet-B7","Hugo Touvron","",2020-03-18,"87.1%",2020
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNet-B7","Andrea Vedaldi","",2020-03-18,"87.1%",2020
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNet-B7","Matthijs Douze","",2020-03-18,"87.1%",2020
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNet-B7","Hervé Jégou","",2020-03-18,"87.1%",2020
"Self-training with Noisy Student improves ImageNet classification@@@NoisyStudent (EfficientNet-B7)","Qizhe Xie","Carnegie Mellon University",2019-11-11,"86.9%",2019
"Self-training with Noisy Student improves ImageNet classification@@@NoisyStudent (EfficientNet-B7)","Minh-Thang Luong","Google",2019-11-11,"86.9%",2019
"Self-training with Noisy Student improves ImageNet classification@@@NoisyStudent (EfficientNet-B7)","Eduard Hovy","Carnegie Mellon University",2019-11-11,"86.9%",2019
"Self-training with Noisy Student improves ImageNet classification@@@NoisyStudent (EfficientNet-B7)","Quoc V. Le","Google",2019-11-11,"86.9%",2019
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNet-B6","Hugo Touvron","",2020-03-18,"86.7%",2020
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNet-B6","Andrea Vedaldi","",2020-03-18,"86.7%",2020
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNet-B6","Matthijs Douze","",2020-03-18,"86.7%",2020
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNet-B6","Hervé Jégou","",2020-03-18,"86.7%",2020
"Fixing the train-test resolution discrepancy@@@FixResNeXt-101 32x48d","Hugo Touvron","Facebook",2019-06-14,"86.4%",2019
"Fixing the train-test resolution discrepancy@@@FixResNeXt-101 32x48d","Andrea Vedaldi","University of Oxford",2019-06-14,"86.4%",2019
"Fixing the train-test resolution discrepancy@@@FixResNeXt-101 32x48d","Matthijs Douze","Facebook",2019-06-14,"86.4%",2019
"Fixing the train-test resolution discrepancy@@@FixResNeXt-101 32x48d","Hervé Jégou","Facebook",2019-06-14,"86.4%",2019
"Self-training with Noisy Student improves ImageNet classification@@@NoisyStudent (EfficientNet-B6)","Qizhe Xie","Carnegie Mellon University",2019-11-11,"86.4%",2019
"Self-training with Noisy Student improves ImageNet classification@@@NoisyStudent (EfficientNet-B6)","Minh-Thang Luong","Google",2019-11-11,"86.4%",2019
"Self-training with Noisy Student improves ImageNet classification@@@NoisyStudent (EfficientNet-B6)","Eduard Hovy","Carnegie Mellon University",2019-11-11,"86.4%",2019
"Self-training with Noisy Student improves ImageNet classification@@@NoisyStudent (EfficientNet-B6)","Quoc V. Le","Google",2019-11-11,"86.4%",2019
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNet-B5","Hugo Touvron","",2020-03-18,"86.4%",2020
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNet-B5","Andrea Vedaldi","",2020-03-18,"86.4%",2020
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNet-B5","Matthijs Douze","",2020-03-18,"86.4%",2020
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNet-B5","Hervé Jégou","",2020-03-18,"86.4%",2020
"Self-training with Noisy Student improves ImageNet classification@@@NoisyStudent (EfficientNet-B5)","Qizhe Xie","Carnegie Mellon University",2019-11-11,"86.1%",2019
"Self-training with Noisy Student improves ImageNet classification@@@NoisyStudent (EfficientNet-B5)","Minh-Thang Luong","Google",2019-11-11,"86.1%",2019
"Self-training with Noisy Student improves ImageNet classification@@@NoisyStudent (EfficientNet-B5)","Eduard Hovy","Carnegie Mellon University",2019-11-11,"86.1%",2019
"Self-training with Noisy Student improves ImageNet classification@@@NoisyStudent (EfficientNet-B5)","Quoc V. Le","Google",2019-11-11,"86.1%",2019
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNet-B4","Hugo Touvron","",2020-03-18,"85.9%",2020
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNet-B4","Andrea Vedaldi","",2020-03-18,"85.9%",2020
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNet-B4","Matthijs Douze","",2020-03-18,"85.9%",2020
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNet-B4","Hervé Jégou","",2020-03-18,"85.9%",2020
"Circumventing Outliers of AutoAugment with Knowledge Distillation@@@KDforAA (EfficientNet-B8)","Longhui Wei","Huawei",2020-03-25,"85.8%",2020
"Circumventing Outliers of AutoAugment with Knowledge Distillation@@@KDforAA (EfficientNet-B8)","An Xiao","Huawei",2020-03-25,"85.8%",2020
"Circumventing Outliers of AutoAugment with Knowledge Distillation@@@KDforAA (EfficientNet-B8)","Lingxi Xie","",2020-03-25,"85.8%",2020
"Circumventing Outliers of AutoAugment with Knowledge Distillation@@@KDforAA (EfficientNet-B8)","Xin Chen","Tongji University",2020-03-25,"85.8%",2020
"Circumventing Outliers of AutoAugment with Knowledge Distillation@@@KDforAA (EfficientNet-B8)","Xiaopeng Zhang","",2020-03-25,"85.8%",2020
"Circumventing Outliers of AutoAugment with Knowledge Distillation@@@KDforAA (EfficientNet-B8)","Qi Tian","Huawei",2020-03-25,"85.8%",2020
"MaxUp: A Simple Way to Improve Generalization of Neural Network Training@@@Fix-EfficientNet-B8 (MaxUp + CutMix)","Chengyue Gong","University of Texas at Austin",2020-02-20,"85.8%",2020
"MaxUp: A Simple Way to Improve Generalization of Neural Network Training@@@Fix-EfficientNet-B8 (MaxUp + CutMix)","ChengYue Gong","University of Texas at Austin",2020-02-20,"85.8%",2020
"MaxUp: A Simple Way to Improve Generalization of Neural Network Training@@@Fix-EfficientNet-B8 (MaxUp + CutMix)","Tongzheng Ren","",2020-02-20,"85.8%",2020
"MaxUp: A Simple Way to Improve Generalization of Neural Network Training@@@Fix-EfficientNet-B8 (MaxUp + CutMix)","Mao Ye","",2020-02-20,"85.8%",2020
"MaxUp: A Simple Way to Improve Generalization of Neural Network Training@@@Fix-EfficientNet-B8 (MaxUp + CutMix)","Qiang Liu","",2020-02-20,"85.8%",2020
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNet-B8","Hugo Touvron","",2020-03-18,"85.7%",2020
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNet-B8","Andrea Vedaldi","",2020-03-18,"85.7%",2020
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNet-B8","Matthijs Douze","",2020-03-18,"85.7%",2020
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNet-B8","Hervé Jégou","",2020-03-18,"85.7%",2020
"Adversarial Examples Improve Image Recognition@@@AdvProp (EfficientNet-B8)","Cihang Xie","Johns Hopkins University",2019-11-21,"85.5%",2019
"Adversarial Examples Improve Image Recognition@@@AdvProp (EfficientNet-B8)","Mingxing Tan","Google",2019-11-21,"85.5%",2019
"Adversarial Examples Improve Image Recognition@@@AdvProp (EfficientNet-B8)","Boqing Gong","Google",2019-11-21,"85.5%",2019
"Adversarial Examples Improve Image Recognition@@@AdvProp (EfficientNet-B8)","Jiang Wang","Google",2019-11-21,"85.5%",2019
"Adversarial Examples Improve Image Recognition@@@AdvProp (EfficientNet-B8)","Alan L. Yuille","Johns Hopkins University",2019-11-21,"85.5%",2019
"Adversarial Examples Improve Image Recognition@@@AdvProp (EfficientNet-B8)","Quoc V. Le","Google",2019-11-21,"85.5%",2019
"Circumventing Outliers of AutoAugment with Knowledge Distillation@@@KDforAA (EfficientNet-B7)","Longhui Wei","Huawei",2020-03-25,"85.5%",2020
"Circumventing Outliers of AutoAugment with Knowledge Distillation@@@KDforAA (EfficientNet-B7)","An Xiao","Huawei",2020-03-25,"85.5%",2020
"Circumventing Outliers of AutoAugment with Knowledge Distillation@@@KDforAA (EfficientNet-B7)","Lingxi Xie","",2020-03-25,"85.5%",2020
"Circumventing Outliers of AutoAugment with Knowledge Distillation@@@KDforAA (EfficientNet-B7)","Xin Chen","Tongji University",2020-03-25,"85.5%",2020
"Circumventing Outliers of AutoAugment with Knowledge Distillation@@@KDforAA (EfficientNet-B7)","Xiaopeng Zhang","",2020-03-25,"85.5%",2020
"Circumventing Outliers of AutoAugment with Knowledge Distillation@@@KDforAA (EfficientNet-B7)","Qi Tian","Huawei",2020-03-25,"85.5%",2020
"Exploring the Limits of Weakly Supervised Pretraining@@@ResNeXt-101 32x48d","Dhruv Mahajan","Facebook",2018-05-02,"85.4%",2018
"Exploring the Limits of Weakly Supervised Pretraining@@@ResNeXt-101 32x48d","Ross Girshick","Facebook",2018-05-02,"85.4%",2018
"Exploring the Limits of Weakly Supervised Pretraining@@@ResNeXt-101 32x48d","Vignesh Ramanathan","Facebook",2018-05-02,"85.4%",2018
"Exploring the Limits of Weakly Supervised Pretraining@@@ResNeXt-101 32x48d","Kaiming He","Facebook",2018-05-02,"85.4%",2018
"Exploring the Limits of Weakly Supervised Pretraining@@@ResNeXt-101 32x48d","Manohar Paluri","Facebook",2018-05-02,"85.4%",2018
"Exploring the Limits of Weakly Supervised Pretraining@@@ResNeXt-101 32x48d","Yixuan Li","Facebook",2018-05-02,"85.4%",2018
"Exploring the Limits of Weakly Supervised Pretraining@@@ResNeXt-101 32x48d","Ashwin Bharambe","Facebook",2018-05-02,"85.4%",2018
"Exploring the Limits of Weakly Supervised Pretraining@@@ResNeXt-101 32x48d","Laurens van der Maaten","Facebook",2018-05-02,"85.4%",2018
"RandAugment: Practical automated data augmentation with a reduced search space@@@EfficientNet-B8 (RandAugment)","Ekin D. Cubuk","Google",2019-09-30,"85.4%",2019
"RandAugment: Practical automated data augmentation with a reduced search space@@@EfficientNet-B8 (RandAugment)","Barret Zoph","Google",2019-09-30,"85.4%",2019
"RandAugment: Practical automated data augmentation with a reduced search space@@@EfficientNet-B8 (RandAugment)","Jonathon Shlens","Google",2019-09-30,"85.4%",2019
"RandAugment: Practical automated data augmentation with a reduced search space@@@EfficientNet-B8 (RandAugment)","Quoc V. Le","Google",2019-09-30,"85.4%",2019
"Big Transfer (BiT): General Visual Representation Learning@@@BiT-M (ResNet)","Alexander Kolesnikov","Google",2019-12-24,"85.39%",2019
"Big Transfer (BiT): General Visual Representation Learning@@@BiT-M (ResNet)","Lucas Beyer","Google",2019-12-24,"85.39%",2019
"Big Transfer (BiT): General Visual Representation Learning@@@BiT-M (ResNet)","Xiaohua Zhai","Google",2019-12-24,"85.39%",2019
"Big Transfer (BiT): General Visual Representation Learning@@@BiT-M (ResNet)","Joan Puigcerver","Google",2019-12-24,"85.39%",2019
"Big Transfer (BiT): General Visual Representation Learning@@@BiT-M (ResNet)","Jessica Yung","Google",2019-12-24,"85.39%",2019
"Big Transfer (BiT): General Visual Representation Learning@@@BiT-M (ResNet)","Sylvain Gelly","Google",2019-12-24,"85.39%",2019
"Big Transfer (BiT): General Visual Representation Learning@@@BiT-M (ResNet)","Neil Houlsby","Google",2019-12-24,"85.39%",2019
"Self-training with Noisy Student improves ImageNet classification@@@NoisyStudent (EfficientNet-B4)","Qizhe Xie","Carnegie Mellon University",2019-11-11,"85.3%",2019
"Self-training with Noisy Student improves ImageNet classification@@@NoisyStudent (EfficientNet-B4)","Minh-Thang Luong","Google",2019-11-11,"85.3%",2019
"Self-training with Noisy Student improves ImageNet classification@@@NoisyStudent (EfficientNet-B4)","Eduard Hovy","Carnegie Mellon University",2019-11-11,"85.3%",2019
"Self-training with Noisy Student improves ImageNet classification@@@NoisyStudent (EfficientNet-B4)","Quoc V. Le","Google",2019-11-11,"85.3%",2019
"Adversarial Examples Improve Image Recognition@@@AdvProp (EfficientNet-B7)","Cihang Xie","Johns Hopkins University",2019-11-21,"85.2%",2019
"Adversarial Examples Improve Image Recognition@@@AdvProp (EfficientNet-B7)","Mingxing Tan","Google",2019-11-21,"85.2%",2019
"Adversarial Examples Improve Image Recognition@@@AdvProp (EfficientNet-B7)","Boqing Gong","Google",2019-11-21,"85.2%",2019
"Adversarial Examples Improve Image Recognition@@@AdvProp (EfficientNet-B7)","Jiang Wang","Google",2019-11-21,"85.2%",2019
"Adversarial Examples Improve Image Recognition@@@AdvProp (EfficientNet-B7)","Alan L. Yuille","Johns Hopkins University",2019-11-21,"85.2%",2019
"Adversarial Examples Improve Image Recognition@@@AdvProp (EfficientNet-B7)","Quoc V. Le","Google",2019-11-21,"85.2%",2019
"Exploring the Limits of Weakly Supervised Pretraining@@@ResNeXt-101 32x32d","Dhruv Mahajan","Facebook",2018-05-02,"85.1%",2018
"Exploring the Limits of Weakly Supervised Pretraining@@@ResNeXt-101 32x32d","Ross Girshick","Facebook",2018-05-02,"85.1%",2018
"Exploring the Limits of Weakly Supervised Pretraining@@@ResNeXt-101 32x32d","Vignesh Ramanathan","Facebook",2018-05-02,"85.1%",2018
"Exploring the Limits of Weakly Supervised Pretraining@@@ResNeXt-101 32x32d","Kaiming He","Facebook",2018-05-02,"85.1%",2018
"Exploring the Limits of Weakly Supervised Pretraining@@@ResNeXt-101 32x32d","Manohar Paluri","Facebook",2018-05-02,"85.1%",2018
"Exploring the Limits of Weakly Supervised Pretraining@@@ResNeXt-101 32x32d","Yixuan Li","Facebook",2018-05-02,"85.1%",2018
"Exploring the Limits of Weakly Supervised Pretraining@@@ResNeXt-101 32x32d","Ashwin Bharambe","Facebook",2018-05-02,"85.1%",2018
"Exploring the Limits of Weakly Supervised Pretraining@@@ResNeXt-101 32x32d","Laurens van der Maaten","Facebook",2018-05-02,"85.1%",2018
"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset@@@ResNet200_vd_26w_4s_ssld","Cheng Cui","",2020-06-18,"85.1%",2020
"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset@@@ResNet200_vd_26w_4s_ssld","Zhi Ye","",2020-06-18,"85.1%",2020
"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset@@@ResNet200_vd_26w_4s_ssld","Yangxi Li","",2020-06-18,"85.1%",2020
"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset@@@ResNet200_vd_26w_4s_ssld","Xinjian Li","",2020-06-18,"85.1%",2020
"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset@@@ResNet200_vd_26w_4s_ssld","Min Yang","",2020-06-18,"85.1%",2020
"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset@@@ResNet200_vd_26w_4s_ssld","Kai Wei","",2020-06-18,"85.1%",2020
"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset@@@ResNet200_vd_26w_4s_ssld","Bing Dai","",2020-06-18,"85.1%",2020
"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset@@@ResNet200_vd_26w_4s_ssld","Yanmei Zhao","",2020-06-18,"85.1%",2020
"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset@@@ResNet200_vd_26w_4s_ssld","Zhongji Liu","",2020-06-18,"85.1%",2020
"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset@@@ResNet200_vd_26w_4s_ssld","Rong Pang","",2020-06-18,"85.1%",2020
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNet-B3","Hugo Touvron","",2020-03-18,"85%",2020
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNet-B3","Andrea Vedaldi","",2020-03-18,"85%",2020
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNet-B3","Matthijs Douze","",2020-03-18,"85%",2020
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNet-B3","Hervé Jégou","",2020-03-18,"85%",2020
"RandAugment: Practical automated data augmentation with a reduced search space@@@EfficientNet-B7 (RandAugment)","Ekin D. Cubuk","Google",2019-09-30,"85%",2019
"RandAugment: Practical automated data augmentation with a reduced search space@@@EfficientNet-B7 (RandAugment)","Barret Zoph","Google",2019-09-30,"85%",2019
"RandAugment: Practical automated data augmentation with a reduced search space@@@EfficientNet-B7 (RandAugment)","Jonathon Shlens","Google",2019-09-30,"85%",2019
"RandAugment: Practical automated data augmentation with a reduced search space@@@EfficientNet-B7 (RandAugment)","Quoc V. Le","Google",2019-09-30,"85%",2019
"Billion-scale semi-supervised learning for image classification@@@ResNeXt-101 32x16d (semi-weakly sup.)","I. Zeki Yalniz","",2019-05-02,"84.8%",2019
"Billion-scale semi-supervised learning for image classification@@@ResNeXt-101 32x16d (semi-weakly sup.)","Hervé Jégou","",2019-05-02,"84.8%",2019
"Billion-scale semi-supervised learning for image classification@@@ResNeXt-101 32x16d (semi-weakly sup.)","Kan Chen","",2019-05-02,"84.8%",2019
"Billion-scale semi-supervised learning for image classification@@@ResNeXt-101 32x16d (semi-weakly sup.)","Manohar Paluri","",2019-05-02,"84.8%",2019
"Billion-scale semi-supervised learning for image classification@@@ResNeXt-101 32x16d (semi-weakly sup.)","Dhruv Mahajan","",2019-05-02,"84.8%",2019
"ResNeSt: Split-Attention Networks@@@ResNeSt-269","Hang Zhang","",2020-04-19,"84.5%",2020
"ResNeSt: Split-Attention Networks@@@ResNeSt-269","Chongruo Wu","",2020-04-19,"84.5%",2020
"ResNeSt: Split-Attention Networks@@@ResNeSt-269","Zhongyue Zhang","",2020-04-19,"84.5%",2020
"ResNeSt: Split-Attention Networks@@@ResNeSt-269","Yi Zhu","",2020-04-19,"84.5%",2020
"ResNeSt: Split-Attention Networks@@@ResNeSt-269","Zhi Zhang","",2020-04-19,"84.5%",2020
"ResNeSt: Split-Attention Networks@@@ResNeSt-269","Haibin Lin","",2020-04-19,"84.5%",2020
"ResNeSt: Split-Attention Networks@@@ResNeSt-269","Yue Sun","",2020-04-19,"84.5%",2020
"ResNeSt: Split-Attention Networks@@@ResNeSt-269","Tong He","",2020-04-19,"84.5%",2020
"ResNeSt: Split-Attention Networks@@@ResNeSt-269","Jonas Mueller","",2020-04-19,"84.5%",2020
"ResNeSt: Split-Attention Networks@@@ResNeSt-269","R. Manmatha","",2020-04-19,"84.5%",2020
"ResNeSt: Split-Attention Networks@@@ResNeSt-269","Mu Li","",2020-04-19,"84.5%",2020
"ResNeSt: Split-Attention Networks@@@ResNeSt-269","Alexander J. Smola","",2020-04-19,"84.5%",2020
"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks@@@EfficientNet-B7","Mingxing Tan","Google",2019-05-28,"84.4%",2019
"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks@@@EfficientNet-B7","Quoc V. Le","Google",2019-05-28,"84.4%",2019
"GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism@@@GPIPE","Yanping Huang","Google",2018-11-16,"84.4%",2018
"GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism@@@GPIPE","Youlong Cheng","Google",2018-11-16,"84.4%",2018
"GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism@@@GPIPE","Ankur Bapna","Google",2018-11-16,"84.4%",2018
"GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism@@@GPIPE","Orhan Firat","Google",2018-11-16,"84.4%",2018
"GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism@@@GPIPE","Mia Xu Chen","Google",2018-11-16,"84.4%",2018
"GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism@@@GPIPE","Dehao Chen","Google",2018-11-16,"84.4%",2018
"GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism@@@GPIPE","HyoukJoong Lee","Google",2018-11-16,"84.4%",2018
"GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism@@@GPIPE","Jiquan Ngiam","Google",2018-11-16,"84.4%",2018
"GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism@@@GPIPE","Quoc V. Le","Carnegie Mellon University",2018-11-16,"84.4%",2018
"GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism@@@GPIPE","Yonghui Wu","Google",2018-11-16,"84.4%",2018
"GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism@@@GPIPE","Zhifeng Chen","Google",2018-11-16,"84.4%",2018
"Billion-scale semi-supervised learning for image classification@@@ResNeXt-101 32x8d (semi-weakly sup.)","I. Zeki Yalniz","",2019-05-02,"84.3%",2019
"Billion-scale semi-supervised learning for image classification@@@ResNeXt-101 32x8d (semi-weakly sup.)","Hervé Jégou","",2019-05-02,"84.3%",2019
"Billion-scale semi-supervised learning for image classification@@@ResNeXt-101 32x8d (semi-weakly sup.)","Kan Chen","",2019-05-02,"84.3%",2019
"Billion-scale semi-supervised learning for image classification@@@ResNeXt-101 32x8d (semi-weakly sup.)","Manohar Paluri","",2019-05-02,"84.3%",2019
"Billion-scale semi-supervised learning for image classification@@@ResNeXt-101 32x8d (semi-weakly sup.)","Dhruv Mahajan","",2019-05-02,"84.3%",2019
"TResNet: High Performance GPU-Dedicated Architecture@@@TResNet-XL","Tal Ridnik","",2020-03-30,"84.3%",2020
"TResNet: High Performance GPU-Dedicated Architecture@@@TResNet-XL","Hussam Lawen","",2020-03-30,"84.3%",2020
"TResNet: High Performance GPU-Dedicated Architecture@@@TResNet-XL","Asaf Noy","",2020-03-30,"84.3%",2020
"TResNet: High Performance GPU-Dedicated Architecture@@@TResNet-XL","Itamar Friedman","",2020-03-30,"84.3%",2020
"LambdaNetworks: Modeling long-range Interactions without Attention@@@LambdaResNet200","Irwan Bello","Google",2021-01-01,"84.3%",2021
"LambdaNetworks: Modeling long-range Interactions without Attention@@@LambdaResNet200","Barret Zoph","Google",2021-01-01,"84.3%",2021
"LambdaNetworks: Modeling long-range Interactions without Attention@@@LambdaResNet200","Quoc V. Le","Google",2021-01-01,"84.3%",2021
"LambdaNetworks: Modeling long-range Interactions without Attention@@@LambdaResNet200","Ashish Vaswani","Google",2021-01-01,"84.3%",2021
"LambdaNetworks: Modeling long-range Interactions without Attention@@@LambdaResNet200","Jonathon Shlens","Google",2021-01-01,"84.3%",2021
"Exploring the Limits of Weakly Supervised Pretraining@@@ ResNeXt-101 32×16d","Dhruv Mahajan","Facebook",2018-05-02,"84.2%",2018
"Exploring the Limits of Weakly Supervised Pretraining@@@ ResNeXt-101 32×16d","Ross Girshick","Facebook",2018-05-02,"84.2%",2018
"Exploring the Limits of Weakly Supervised Pretraining@@@ ResNeXt-101 32×16d","Vignesh Ramanathan","Facebook",2018-05-02,"84.2%",2018
"Exploring the Limits of Weakly Supervised Pretraining@@@ ResNeXt-101 32×16d","Kaiming He","Facebook",2018-05-02,"84.2%",2018
"Exploring the Limits of Weakly Supervised Pretraining@@@ ResNeXt-101 32×16d","Manohar Paluri","Facebook",2018-05-02,"84.2%",2018
"Exploring the Limits of Weakly Supervised Pretraining@@@ ResNeXt-101 32×16d","Yixuan Li","Facebook",2018-05-02,"84.2%",2018
"Exploring the Limits of Weakly Supervised Pretraining@@@ ResNeXt-101 32×16d","Ashwin Bharambe","Facebook",2018-05-02,"84.2%",2018
"Exploring the Limits of Weakly Supervised Pretraining@@@ ResNeXt-101 32×16d","Laurens van der Maaten","Facebook",2018-05-02,"84.2%",2018
"Compounding the Performance Improvements of Assembled Techniques in a Convolutional Neural Network@@@Assemble-ResNet152","Jungkyu Lee","",2020-01-17,"84.2%",2020
"Compounding the Performance Improvements of Assembled Techniques in a Convolutional Neural Network@@@Assemble-ResNet152","Taeryun Won","",2020-01-17,"84.2%",2020
"Compounding the Performance Improvements of Assembled Techniques in a Convolutional Neural Network@@@Assemble-ResNet152","Kiho Hong","Naver Corporation",2020-01-17,"84.2%",2020
"Self-training with Noisy Student improves ImageNet classification@@@NoisyStudent (EfficientNet-B3)","","",2019-11-11,"84.1%",2019
"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset@@@Fix_ResNet50_vd_ssld","Cheng Cui","",2020-06-18,"84.0%",2020
"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset@@@Fix_ResNet50_vd_ssld","Zhi Ye","",2020-06-18,"84.0%",2020
"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset@@@Fix_ResNet50_vd_ssld","Yangxi Li","",2020-06-18,"84.0%",2020
"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset@@@Fix_ResNet50_vd_ssld","Xinjian Li","",2020-06-18,"84.0%",2020
"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset@@@Fix_ResNet50_vd_ssld","Min Yang","",2020-06-18,"84.0%",2020
"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset@@@Fix_ResNet50_vd_ssld","Kai Wei","",2020-06-18,"84.0%",2020
"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset@@@Fix_ResNet50_vd_ssld","Bing Dai","",2020-06-18,"84.0%",2020
"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset@@@Fix_ResNet50_vd_ssld","Yanmei Zhao","",2020-06-18,"84.0%",2020
"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset@@@Fix_ResNet50_vd_ssld","Zhongji Liu","",2020-06-18,"84.0%",2020
"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset@@@Fix_ResNet50_vd_ssld","Rong Pang","",2020-06-18,"84.0%",2020
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNetB4","Hugo Touvron","",2020-03-18,"84.0%",2020
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNetB4","Andrea Vedaldi","",2020-03-18,"84.0%",2020
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNetB4","Matthijs Douze","",2020-03-18,"84.0%",2020
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNetB4","Hervé Jégou","",2020-03-18,"84.0%",2020
"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks@@@EfficientNet-B6","Mingxing Tan","Google",2019-05-28,"84%",2019
"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks@@@EfficientNet-B6","Quoc V. Le","Google",2019-05-28,"84%",2019
"LambdaNetworks: Modeling long-range Interactions without Attention@@@LambdaResNet152","Irwan Bello","Google",2021-01-01,"84.0%",2021
"LambdaNetworks: Modeling long-range Interactions without Attention@@@LambdaResNet152","Barret Zoph","Google",2021-01-01,"84.0%",2021
"LambdaNetworks: Modeling long-range Interactions without Attention@@@LambdaResNet152","Quoc V. Le","Google",2021-01-01,"84.0%",2021
"LambdaNetworks: Modeling long-range Interactions without Attention@@@LambdaResNet152","Ashish Vaswani","Google",2021-01-01,"84.0%",2021
"LambdaNetworks: Modeling long-range Interactions without Attention@@@LambdaResNet152","Jonathon Shlens","Google",2021-01-01,"84.0%",2021
"Regularized Evolution for Image Classifier Architecture Search@@@AmoebaNet-A","Esteban Real","Google",2018-02-05,"83.9%",2018
"Regularized Evolution for Image Classifier Architecture Search@@@AmoebaNet-A","Alok Aggarwal","Google",2018-02-05,"83.9%",2018
"Regularized Evolution for Image Classifier Architecture Search@@@AmoebaNet-A","Yanping Huang","Google",2018-02-05,"83.9%",2018
"Regularized Evolution for Image Classifier Architecture Search@@@AmoebaNet-A","Quoc V. Le","Google",2018-02-05,"83.9%",2018
"ResNeSt: Split-Attention Networks@@@ResNeSt-200","","",2020-04-19,"83.9%",2020
"Fixing the train-test resolution discrepancy@@@FixPNASNet-5","Hugo Touvron","Facebook",2019-06-14,"83.7%",2019
"Fixing the train-test resolution discrepancy@@@FixPNASNet-5","Andrea Vedaldi","University of Oxford",2019-06-14,"83.7%",2019
"Fixing the train-test resolution discrepancy@@@FixPNASNet-5","Matthijs Douze","Facebook",2019-06-14,"83.7%",2019
"Fixing the train-test resolution discrepancy@@@FixPNASNet-5","Hervé Jégou","Facebook",2019-06-14,"83.7%",2019
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNet-B2","Hugo Touvron","",2020-03-18,"83.6%",2020
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNet-B2","Andrea Vedaldi","",2020-03-18,"83.6%",2020
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNet-B2","Matthijs Douze","",2020-03-18,"83.6%",2020
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNet-B2","Hervé Jégou","",2020-03-18,"83.6%",2020
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain PNASNet (500px)","Maxim Berman","",2019-02-14,"83.6%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain PNASNet (500px)","Hervé Jégou","",2019-02-14,"83.6%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain PNASNet (500px)","Andrea Vedaldi","",2019-02-14,"83.6%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain PNASNet (500px)","Iasonas Kokkinos","",2019-02-14,"83.6%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain PNASNet (500px)","Matthijs Douze","",2019-02-14,"83.6%",2019
"Billion-scale semi-supervised learning for image classification@@@ResNeXt-101 32x4d (semi-weakly sup.)","I. Zeki Yalniz","",2019-05-02,"83.4%",2019
"Billion-scale semi-supervised learning for image classification@@@ResNeXt-101 32x4d (semi-weakly sup.)","Hervé Jégou","",2019-05-02,"83.4%",2019
"Billion-scale semi-supervised learning for image classification@@@ResNeXt-101 32x4d (semi-weakly sup.)","Kan Chen","",2019-05-02,"83.4%",2019
"Billion-scale semi-supervised learning for image classification@@@ResNeXt-101 32x4d (semi-weakly sup.)","Manohar Paluri","",2019-05-02,"83.4%",2019
"Billion-scale semi-supervised learning for image classification@@@ResNeXt-101 32x4d (semi-weakly sup.)","Dhruv Mahajan","",2019-05-02,"83.4%",2019
"SplitNet: Divide and Co-training@@@SE-ResNeXt-101, 64x4d, S=2(416px)","Jianbo Dong","Alibaba Group",2020-11-30,"83.34%",2020
"SplitNet: Divide and Co-training@@@SE-ResNeXt-101, 64x4d, S=2(416px)","Zheng Cao","Alibaba Group",2020-11-30,"83.34%",2020
"SplitNet: Divide and Co-training@@@SE-ResNeXt-101, 64x4d, S=2(416px)","Tao Zhang","Alibaba Group",2020-11-30,"83.34%",2020
"SplitNet: Divide and Co-training@@@SE-ResNeXt-101, 64x4d, S=2(416px)","Jianxi Ye","Alibaba Group",2020-11-30,"83.34%",2020
"SplitNet: Divide and Co-training@@@SE-ResNeXt-101, 64x4d, S=2(416px)","Shaochuang Wang","Alibaba Group",2020-11-30,"83.34%",2020
"SplitNet: Divide and Co-training@@@SE-ResNeXt-101, 64x4d, S=2(416px)","Fei Feng","Alibaba Group",2020-11-30,"83.34%",2020
"SplitNet: Divide and Co-training@@@SE-ResNeXt-101, 64x4d, S=2(416px)","Li Zhao","Academia Sinica",2020-11-30,"83.34%",2020
"SplitNet: Divide and Co-training@@@SE-ResNeXt-101, 64x4d, S=2(416px)","Xiaoyong Liu","Alibaba Group",2020-11-30,"83.34%",2020
"SplitNet: Divide and Co-training@@@SE-ResNeXt-101, 64x4d, S=2(416px)","Liuyihan Song","Alibaba Group",2020-11-30,"83.34%",2020
"SplitNet: Divide and Co-training@@@SE-ResNeXt-101, 64x4d, S=2(416px)","Liwei Peng","",2020-11-30,"83.34%",2020
"SplitNet: Divide and Co-training@@@SE-ResNeXt-101, 64x4d, S=2(416px)","Pan Pan","Alibaba Group",2020-11-30,"83.34%",2020
"SplitNet: Divide and Co-training@@@SE-ResNeXt-101, 64x4d, S=2(416px)","Yuan Xie","Alibaba Group",2020-11-30,"83.34%",2020
"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks@@@EfficientNet-B5","Mingxing Tan","Google",2019-05-28,"83.3%",2019
"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks@@@EfficientNet-B5","Quoc V. Le","Google",2019-05-28,"83.3%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain PNASNet (450px)","Maxim Berman","",2019-02-14,"83.2%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain PNASNet (450px)","Hervé Jégou","",2019-02-14,"83.2%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain PNASNet (450px)","Andrea Vedaldi","",2019-02-14,"83.2%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain PNASNet (450px)","Iasonas Kokkinos","",2019-02-14,"83.2%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain PNASNet (450px)","Matthijs Douze","",2019-02-14,"83.2%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain SENet154 (450px)","Maxim Berman","",2019-02-14,"83.1%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain SENet154 (450px)","Hervé Jégou","",2019-02-14,"83.1%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain SENet154 (450px)","Andrea Vedaldi","",2019-02-14,"83.1%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain SENet154 (450px)","Iasonas Kokkinos","",2019-02-14,"83.1%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain SENet154 (450px)","Matthijs Douze","",2019-02-14,"83.1%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain SENet154 (400px)","Maxim Berman","",2019-02-14,"83.0%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain SENet154 (400px)","Hervé Jégou","",2019-02-14,"83.0%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain SENet154 (400px)","Andrea Vedaldi","",2019-02-14,"83.0%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain SENet154 (400px)","Iasonas Kokkinos","",2019-02-14,"83.0%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain SENet154 (400px)","Matthijs Douze","",2019-02-14,"83.0%",2019
"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset@@@ResNet50_vd_ssld","Cheng Cui","",2020-06-18,"83.0%",2020
"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset@@@ResNet50_vd_ssld","Zhi Ye","",2020-06-18,"83.0%",2020
"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset@@@ResNet50_vd_ssld","Yangxi Li","",2020-06-18,"83.0%",2020
"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset@@@ResNet50_vd_ssld","Xinjian Li","",2020-06-18,"83.0%",2020
"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset@@@ResNet50_vd_ssld","Min Yang","",2020-06-18,"83.0%",2020
"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset@@@ResNet50_vd_ssld","Kai Wei","",2020-06-18,"83.0%",2020
"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset@@@ResNet50_vd_ssld","Bing Dai","",2020-06-18,"83.0%",2020
"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset@@@ResNet50_vd_ssld","Yanmei Zhao","",2020-06-18,"83.0%",2020
"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset@@@ResNet50_vd_ssld","Zhongji Liu","",2020-06-18,"83.0%",2020
"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset@@@ResNet50_vd_ssld","Rong Pang","",2020-06-18,"83.0%",2020
"ResNeSt: Split-Attention Networks@@@ResNeSt-101","Hang Zhang","",2020-04-19,"83.0%",2020
"ResNeSt: Split-Attention Networks@@@ResNeSt-101","Chongruo Wu","",2020-04-19,"83.0%",2020
"ResNeSt: Split-Attention Networks@@@ResNeSt-101","Zhongyue Zhang","",2020-04-19,"83.0%",2020
"ResNeSt: Split-Attention Networks@@@ResNeSt-101","Yi Zhu","",2020-04-19,"83.0%",2020
"ResNeSt: Split-Attention Networks@@@ResNeSt-101","Zhi Zhang","",2020-04-19,"83.0%",2020
"ResNeSt: Split-Attention Networks@@@ResNeSt-101","Haibin Lin","",2020-04-19,"83.0%",2020
"ResNeSt: Split-Attention Networks@@@ResNeSt-101","Yue Sun","",2020-04-19,"83.0%",2020
"ResNeSt: Split-Attention Networks@@@ResNeSt-101","Tong He","",2020-04-19,"83.0%",2020
"ResNeSt: Split-Attention Networks@@@ResNeSt-101","Jonas Mueller","",2020-04-19,"83.0%",2020
"ResNeSt: Split-Attention Networks@@@ResNeSt-101","R. Manmatha","",2020-04-19,"83.0%",2020
"ResNeSt: Split-Attention Networks@@@ResNeSt-101","Mu Li","",2020-04-19,"83.0%",2020
"ResNeSt: Split-Attention Networks@@@ResNeSt-101","Alexander J. Smola","",2020-04-19,"83.0%",2020
"Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution@@@Oct-ResNet-152 (SE)","Yunpeng Chen","National University of Singapore",2019-04-10,"82.9%",2019
"Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution@@@Oct-ResNet-152 (SE)","Haoqi Fan","Facebook",2019-04-10,"82.9%",2019
"Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution@@@Oct-ResNet-152 (SE)","Bing Xu","Facebook",2019-04-10,"82.9%",2019
"Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution@@@Oct-ResNet-152 (SE)","Zhicheng Yan","Facebook",2019-04-10,"82.9%",2019
"Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution@@@Oct-ResNet-152 (SE)","Yannis Kalantidis","Facebook",2019-04-10,"82.9%",2019
"Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution@@@Oct-ResNet-152 (SE)","Marcus Rohrbach","Facebook",2019-04-10,"82.9%",2019
"Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution@@@Oct-ResNet-152 (SE)","Shuicheng Yan","National University of Singapore",2019-04-10,"82.9%",2019
"Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution@@@Oct-ResNet-152 (SE)","Yan Shuicheng","National University of Singapore",2019-04-10,"82.9%",2019
"Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution@@@Oct-ResNet-152 (SE)","Jiashi Feng","National University of Singapore",2019-04-10,"82.9%",2019
"Progressive Neural Architecture Search@@@PNASNet-5","Chenxi Liu","Johns Hopkins University",2017-12-02,"82.9%",2017
"Progressive Neural Architecture Search@@@PNASNet-5","Barret Zoph","Google",2017-12-02,"82.9%",2017
"Progressive Neural Architecture Search@@@PNASNet-5","Maxim Neumann","Google",2017-12-02,"82.9%",2017
"Progressive Neural Architecture Search@@@PNASNet-5","Jonathon Shlens","Google",2017-12-02,"82.9%",2017
"Progressive Neural Architecture Search@@@PNASNet-5","Wei Hua","Google",2017-12-02,"82.9%",2017
"Progressive Neural Architecture Search@@@PNASNet-5","Li-Jia Li","Google",2017-12-02,"82.9%",2017
"Progressive Neural Architecture Search@@@PNASNet-5","Li Fei-Fei","Stanford University",2017-12-02,"82.9%",2017
"Progressive Neural Architecture Search@@@PNASNet-5","Alan L. Yuille","Johns Hopkins University",2017-12-02,"82.9%",2017
"Progressive Neural Architecture Search@@@PNASNet-5","Jonathan Huang","Google",2017-12-02,"82.9%",2017
"Progressive Neural Architecture Search@@@PNASNet-5","Kevin Murphy","Google",2017-12-02,"82.9%",2017
"Learning Transferable Architectures for Scalable Image Recognition@@@NASNET-A(6)","Barret Zoph","Google",2017-07-21,"82.7%",2017
"Learning Transferable Architectures for Scalable Image Recognition@@@NASNET-A(6)","Vijay K. Vasudevan","Google",2017-07-21,"82.7%",2017
"Learning Transferable Architectures for Scalable Image Recognition@@@NASNET-A(6)","Jonathon Shlens","Google",2017-07-21,"82.7%",2017
"Learning Transferable Architectures for Scalable Image Recognition@@@NASNET-A(6)","Quoc V. Le","Google",2017-07-21,"82.7%",2017
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain SENet154 (500px)","Maxim Berman","",2019-02-14,"82.7%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain SENet154 (500px)","Hervé Jégou","",2019-02-14,"82.7%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain SENet154 (500px)","Andrea Vedaldi","",2019-02-14,"82.7%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain SENet154 (500px)","Iasonas Kokkinos","",2019-02-14,"82.7%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain SENet154 (500px)","Matthijs Douze","",2019-02-14,"82.7%",2019
"Harmonic Convolutional Networks based on Discrete Cosine Transform@@@Harm-SE-RNX-101 64x4d (320x320, Mean-Max Pooling)","Matej Ulicny","",2020-01-18,"82.66%",2020
"Harmonic Convolutional Networks based on Discrete Cosine Transform@@@Harm-SE-RNX-101 64x4d (320x320, Mean-Max Pooling)","Vladimir A. Krylov","",2020-01-18,"82.66%",2020
"Harmonic Convolutional Networks based on Discrete Cosine Transform@@@Harm-SE-RNX-101 64x4d (320x320, Mean-Max Pooling)","Rozenn Dahyot","",2020-01-18,"82.66%",2020
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNet-B1","Hugo Touvron","",2020-03-18,"82.6%",2020
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNet-B1","Andrea Vedaldi","",2020-03-18,"82.6%",2020
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNet-B1","Matthijs Douze","",2020-03-18,"82.6%",2020
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNet-B1","Hervé Jégou","",2020-03-18,"82.6%",2020
"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks@@@EfficientNet-B4","Mingxing Tan","Google",2019-05-28,"82.6%",2019
"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks@@@EfficientNet-B4","Quoc V. Le","Google",2019-05-28,"82.6%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain PNASNet (400px)","Maxim Berman","",2019-02-14,"82.6%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain PNASNet (400px)","Hervé Jégou","",2019-02-14,"82.6%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain PNASNet (400px)","Andrea Vedaldi","",2019-02-14,"82.6%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain PNASNet (400px)","Iasonas Kokkinos","",2019-02-14,"82.6%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain PNASNet (400px)","Matthijs Douze","",2019-02-14,"82.6%",2019
"Fixing the train-test resolution discrepancy@@@FixResNet-50 Billion","Hugo Touvron","Facebook",2019-06-14,"82.5%",2019
"Fixing the train-test resolution discrepancy@@@FixResNet-50 Billion","Andrea Vedaldi","University of Oxford",2019-06-14,"82.5%",2019
"Fixing the train-test resolution discrepancy@@@FixResNet-50 Billion","Matthijs Douze","Facebook",2019-06-14,"82.5%",2019
"Fixing the train-test resolution discrepancy@@@FixResNet-50 Billion","Hervé Jégou","Facebook",2019-06-14,"82.5%",2019
"Self-training with Noisy Student improves ImageNet classification@@@NoisyStudent (EfficientNet-B2)","Qizhe Xie","Carnegie Mellon University",2019-11-11,"82.4%",2019
"Self-training with Noisy Student improves ImageNet classification@@@NoisyStudent (EfficientNet-B2)","Minh-Thang Luong","Google",2019-11-11,"82.4%",2019
"Self-training with Noisy Student improves ImageNet classification@@@NoisyStudent (EfficientNet-B2)","Eduard Hovy","Carnegie Mellon University",2019-11-11,"82.4%",2019
"Self-training with Noisy Student improves ImageNet classification@@@NoisyStudent (EfficientNet-B2)","Quoc V. Le","Google",2019-11-11,"82.4%",2019
"ColorNet: Investigating the importance of color spaces for image classification@@@ColorNet","Shreyank N Gowda","Tsinghua University",2019-02-01,"82.35%",2019
"ColorNet: Investigating the importance of color spaces for image classification@@@ColorNet","Shreyank N. Gowda","Tsinghua University",2019-02-01,"82.35%",2019
"ColorNet: Investigating the importance of color spaces for image classification@@@ColorNet","Chun Yuan","Tsinghua University",2019-02-01,"82.35%",2019
"SCARLET-NAS: Bridging the gap between Stability and Scalability in Weight-sharing Neural Architecture Search@@@SCARLET-A4","Xiangxiang Chu","Xiaomi",2019-08-16,"82.3%",2019
"SCARLET-NAS: Bridging the gap between Stability and Scalability in Weight-sharing Neural Architecture Search@@@SCARLET-A4","Bo Zhang","Xiaomi",2019-08-16,"82.3%",2019
"SCARLET-NAS: Bridging the gap between Stability and Scalability in Weight-sharing Neural Architecture Search@@@SCARLET-A4","Li Jixiang","Xiaomi",2019-08-16,"82.3%",2019
"SCARLET-NAS: Bridging the gap between Stability and Scalability in Weight-sharing Neural Architecture Search@@@SCARLET-A4","Li Qingyuan","Xiaomi",2019-08-16,"82.3%",2019
"SCARLET-NAS: Bridging the gap between Stability and Scalability in Weight-sharing Neural Architecture Search@@@SCARLET-A4","Xu Ruijun","Xiaomi",2019-08-16,"82.3%",2019
"Exploring the Limits of Weakly Supervised Pretraining@@@ResNeXt-101 32x8d","Dhruv Mahajan","Facebook",2018-05-02,"82.2%",2018
"Exploring the Limits of Weakly Supervised Pretraining@@@ResNeXt-101 32x8d","Ross Girshick","Facebook",2018-05-02,"82.2%",2018
"Exploring the Limits of Weakly Supervised Pretraining@@@ResNeXt-101 32x8d","Vignesh Ramanathan","Facebook",2018-05-02,"82.2%",2018
"Exploring the Limits of Weakly Supervised Pretraining@@@ResNeXt-101 32x8d","Kaiming He","Facebook",2018-05-02,"82.2%",2018
"Exploring the Limits of Weakly Supervised Pretraining@@@ResNeXt-101 32x8d","Manohar Paluri","Facebook",2018-05-02,"82.2%",2018
"Exploring the Limits of Weakly Supervised Pretraining@@@ResNeXt-101 32x8d","Yixuan Li","Facebook",2018-05-02,"82.2%",2018
"Exploring the Limits of Weakly Supervised Pretraining@@@ResNeXt-101 32x8d","Ashwin Bharambe","Facebook",2018-05-02,"82.2%",2018
"Exploring the Limits of Weakly Supervised Pretraining@@@ResNeXt-101 32x8d","Laurens van der Maaten","Facebook",2018-05-02,"82.2%",2018
"SplitNet: Divide and Co-training@@@ResNeXt-101, 64x4d, S=2(224px)","Jianbo Dong","Alibaba Group",2020-11-30,"82.13%",2020
"SplitNet: Divide and Co-training@@@ResNeXt-101, 64x4d, S=2(224px)","Zheng Cao","Alibaba Group",2020-11-30,"82.13%",2020
"SplitNet: Divide and Co-training@@@ResNeXt-101, 64x4d, S=2(224px)","Tao Zhang","Alibaba Group",2020-11-30,"82.13%",2020
"SplitNet: Divide and Co-training@@@ResNeXt-101, 64x4d, S=2(224px)","Jianxi Ye","Alibaba Group",2020-11-30,"82.13%",2020
"SplitNet: Divide and Co-training@@@ResNeXt-101, 64x4d, S=2(224px)","Shaochuang Wang","Alibaba Group",2020-11-30,"82.13%",2020
"SplitNet: Divide and Co-training@@@ResNeXt-101, 64x4d, S=2(224px)","Fei Feng","Alibaba Group",2020-11-30,"82.13%",2020
"SplitNet: Divide and Co-training@@@ResNeXt-101, 64x4d, S=2(224px)","Li Zhao","Alibaba Group",2020-11-30,"82.13%",2020
"SplitNet: Divide and Co-training@@@ResNeXt-101, 64x4d, S=2(224px)","Xiaoyong Liu","Alibaba Group",2020-11-30,"82.13%",2020
"SplitNet: Divide and Co-training@@@ResNeXt-101, 64x4d, S=2(224px)","Liuyihan Song","Alibaba Group",2020-11-30,"82.13%",2020
"SplitNet: Divide and Co-training@@@ResNeXt-101, 64x4d, S=2(224px)","Liwei Peng","Alibaba Group",2020-11-30,"82.13%",2020
"SplitNet: Divide and Co-training@@@ResNeXt-101, 64x4d, S=2(224px)","Pan Pan","Alibaba Group",2020-11-30,"82.13%",2020
"SplitNet: Divide and Co-training@@@ResNeXt-101, 64x4d, S=2(224px)","Yuan Xie","Alibaba Group",2020-11-30,"82.13%",2020
"Attentive Normalization@@@AOGNet-40M-AN","Yu-Tong Wang","The Chinese University of Hong Kong",2019-08-04,"81.87%",2019
"Attentive Normalization@@@AOGNet-40M-AN","Yi Wang","The Chinese University of Hong Kong",2019-08-04,"81.87%",2019
"Attentive Normalization@@@AOGNet-40M-AN","Ying-Cong Chen","The Chinese University of Hong Kong",2019-08-04,"81.87%",2019
"Attentive Normalization@@@AOGNet-40M-AN","Xiangyu Zhang","Tsinghua University",2019-08-04,"81.87%",2019
"Attentive Normalization@@@AOGNet-40M-AN","Jian Sun","Xi'an Jiaotong University",2019-08-04,"81.87%",2019
"Attentive Normalization@@@AOGNet-40M-AN","Jiaya Jia","The Chinese University of Hong Kong",2019-08-04,"81.87%",2019
"Sharpness-Aware Minimization for Efficiently Improving Generalization@@@ResNet-152 (SAM)","Pierre Foret","Google",2020-10-03,"81.6%",2020
"Sharpness-Aware Minimization for Efficiently Improving Generalization@@@ResNet-152 (SAM)","Ariel Kleiner","Google",2020-10-03,"81.6%",2020
"Sharpness-Aware Minimization for Efficiently Improving Generalization@@@ResNet-152 (SAM)","Hossein Mobahi","Google",2020-10-03,"81.6%",2020
"Sharpness-Aware Minimization for Efficiently Improving Generalization@@@ResNet-152 (SAM)","Behnam Neyshabur","Google",2020-10-03,"81.6%",2020
"Self-training with Noisy Student improves ImageNet classification@@@NoisyStudent (EfficientNet-B1)","Qizhe Xie","Carnegie Mellon University",2019-11-11,"81.5%",2019
"Self-training with Noisy Student improves ImageNet classification@@@NoisyStudent (EfficientNet-B1)","Minh-Thang Luong","Google",2019-11-11,"81.5%",2019
"Self-training with Noisy Student improves ImageNet classification@@@NoisyStudent (EfficientNet-B1)","Eduard Hovy","Carnegie Mellon University",2019-11-11,"81.5%",2019
"Self-training with Noisy Student improves ImageNet classification@@@NoisyStudent (EfficientNet-B1)","Quoc V. Le","Google",2019-11-11,"81.5%",2019
"Pyramidal Convolution: Rethinking Convolutional Neural Networks for Visual Recognition@@@PyConvResNet-101","Ionut Cosmin Duta","",2020-06-20,"81.49%",2020
"Pyramidal Convolution: Rethinking Convolutional Neural Networks for Visual Recognition@@@PyConvResNet-101","Li Liu","",2020-06-20,"81.49%",2020
"Pyramidal Convolution: Rethinking Convolutional Neural Networks for Visual Recognition@@@PyConvResNet-101","Fan Zhu","",2020-06-20,"81.49%",2020
"Pyramidal Convolution: Rethinking Convolutional Neural Networks for Visual Recognition@@@PyConvResNet-101","Ling Shao","",2020-06-20,"81.49%",2020
"Dual Path Networks@@@DPN-131 (320x320)","","",2017-07-06,"81.38%",2017
"Adversarial AutoAugment@@@ResNet-200 (Adversarial Autoaugment)","Xinyu Zhang","Huawei",2019-12-24,"81.32%",2019
"Adversarial AutoAugment@@@ResNet-200 (Adversarial Autoaugment)","Qiang Wang","Huawei",2019-12-24,"81.32%",2019
"Adversarial AutoAugment@@@ResNet-200 (Adversarial Autoaugment)","Jian Zhang","Huawei",2019-12-24,"81.32%",2019
"Adversarial AutoAugment@@@ResNet-200 (Adversarial Autoaugment)","Zhao Zhong","Huawei",2019-12-24,"81.32%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain PNASNet (300px)","Maxim Berman","",2019-02-14,"81.3%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain PNASNet (300px)","Hervé Jégou","",2019-02-14,"81.3%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain PNASNet (300px)","Andrea Vedaldi","",2019-02-14,"81.3%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain PNASNet (300px)","Iasonas Kokkinos","",2019-02-14,"81.3%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain PNASNet (300px)","Matthijs Douze","",2019-02-14,"81.3%",2019
"Dual Path Networks@@@DPN-98 (320x320, Mean-Max Pooling)","Yunpeng Chen","National University of Singapore",2017-07-06,"81.28%",2017
"Dual Path Networks@@@DPN-98 (320x320, Mean-Max Pooling)","Jianan Li","Beijing Institute of Technology",2017-07-06,"81.28%",2017
"Dual Path Networks@@@DPN-98 (320x320, Mean-Max Pooling)","Huaxin Xiao","National University of Defense Technology",2017-07-06,"81.28%",2017
"Dual Path Networks@@@DPN-98 (320x320, Mean-Max Pooling)","Xiaojie Jin","National University of Singapore",2017-07-06,"81.28%",2017
"Dual Path Networks@@@DPN-98 (320x320, Mean-Max Pooling)","Shuicheng Yan","National University of Singapore",2017-07-06,"81.28%",2017
"Dual Path Networks@@@DPN-98 (320x320, Mean-Max Pooling)","Jiashi Feng","National University of Singapore",2017-07-06,"81.28%",2017
"Res2Net: A New Multi-scale Backbone Architecture@@@Res2Net-101","Shanghua Gao","Nankai University",2019-04-02,"81.23%",2019
"Res2Net: A New Multi-scale Backbone Architecture@@@Res2Net-101","Ming-Ming Cheng","Nankai University",2019-04-02,"81.23%",2019
"Res2Net: A New Multi-scale Backbone Architecture@@@Res2Net-101","Kai Zhao","Nankai University",2019-04-02,"81.23%",2019
"Res2Net: A New Multi-scale Backbone Architecture@@@Res2Net-101","Xin-Yu Zhang","Nankai University",2019-04-02,"81.23%",2019
"Res2Net: A New Multi-scale Backbone Architecture@@@Res2Net-101","Ming-Hsuan Yang","University of California, Merced",2019-04-02,"81.23%",2019
"Res2Net: A New Multi-scale Backbone Architecture@@@Res2Net-101","Philip H. S. Torr","University of Oxford",2019-04-02,"81.23%",2019
"Billion-scale semi-supervised learning for image classification@@@ResNet-50 (semi-weakly sup.)","I. Zeki Yalniz","",2019-05-02,"81.2%",2019
"Billion-scale semi-supervised learning for image classification@@@ResNet-50 (semi-weakly sup.)","Hervé Jégou","",2019-05-02,"81.2%",2019
"Billion-scale semi-supervised learning for image classification@@@ResNet-50 (semi-weakly sup.)","Kan Chen","",2019-05-02,"81.2%",2019
"Billion-scale semi-supervised learning for image classification@@@ResNet-50 (semi-weakly sup.)","Manohar Paluri","",2019-05-02,"81.2%",2019
"Billion-scale semi-supervised learning for image classification@@@ResNet-50 (semi-weakly sup.)","Dhruv Mahajan","",2019-05-02,"81.2%",2019
"Shape-Texture Debiased Neural Network Training@@@ResNeXt-101 (Debiased+CutMix)","Yingwei Li","",2020-10-12,"81.2",2020
"Shape-Texture Debiased Neural Network Training@@@ResNeXt-101 (Debiased+CutMix)","Qihang Yu","",2020-10-12,"81.2",2020
"Shape-Texture Debiased Neural Network Training@@@ResNeXt-101 (Debiased+CutMix)","Mingxing Tan","",2020-10-12,"81.2",2020
"Shape-Texture Debiased Neural Network Training@@@ResNeXt-101 (Debiased+CutMix)","Jieru Mei","",2020-10-12,"81.2",2020
"Shape-Texture Debiased Neural Network Training@@@ResNeXt-101 (Debiased+CutMix)","Peng Tang","",2020-10-12,"81.2",2020
"Shape-Texture Debiased Neural Network Training@@@ResNeXt-101 (Debiased+CutMix)","Wei Shen","",2020-10-12,"81.2",2020
"Shape-Texture Debiased Neural Network Training@@@ResNeXt-101 (Debiased+CutMix)","Alan L. Yuille","",2020-10-12,"81.2",2020
"Shape-Texture Debiased Neural Network Training@@@ResNeXt-101 (Debiased+CutMix)","Cihang Xie","",2020-10-12,"81.2",2020
"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks@@@EfficientNet-B3","Mingxing Tan","Google",2019-05-28,"81.1%",2019
"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks@@@EfficientNet-B3","Quoc V. Le","Google",2019-05-28,"81.1%",2019
"Dual Path Networks@@@DPN-98 (320x320)","Yunpeng Chen","National University of Singapore",2017-07-06,"81.06%",2017
"Dual Path Networks@@@DPN-98 (320x320)","Jianan Li","Beijing Institute of Technology",2017-07-06,"81.06%",2017
"Dual Path Networks@@@DPN-98 (320x320)","Huaxin Xiao","National University of Defense Technology",2017-07-06,"81.06%",2017
"Dual Path Networks@@@DPN-98 (320x320)","Xiaojie Jin","National University of Singapore",2017-07-06,"81.06%",2017
"Dual Path Networks@@@DPN-98 (320x320)","Shuicheng Yan","National University of Singapore",2017-07-06,"81.06%",2017
"Dual Path Networks@@@DPN-98 (320x320)","Jiashi Feng","National University of Singapore",2017-07-06,"81.06%",2017
"Dual Path Networks@@@DPN-92 (320x320, Mean-Max Pooling)","Yunpeng Chen","National University of Singapore",2017-07-06,"80.96%",2017
"Dual Path Networks@@@DPN-92 (320x320, Mean-Max Pooling)","Jianan Li","Beijing Institute of Technology",2017-07-06,"80.96%",2017
"Dual Path Networks@@@DPN-92 (320x320, Mean-Max Pooling)","Huaxin Xiao","National University of Defense Technology",2017-07-06,"80.96%",2017
"Dual Path Networks@@@DPN-92 (320x320, Mean-Max Pooling)","Xiaojie Jin","National University of Singapore",2017-07-06,"80.96%",2017
"Dual Path Networks@@@DPN-92 (320x320, Mean-Max Pooling)","Shuicheng Yan","National University of Singapore",2017-07-06,"80.96%",2017
"Dual Path Networks@@@DPN-92 (320x320, Mean-Max Pooling)","Jiashi Feng","National University of Singapore",2017-07-06,"80.96%",2017
"Aggregated Residual Transformations for Deep Neural Networks@@@ResNeXt-101  64x4","Saining Xie","University of California, San Diego",2016-11-16,"80.9%",2016
"Aggregated Residual Transformations for Deep Neural Networks@@@ResNeXt-101  64x4","Ross Girshick","Facebook",2016-11-16,"80.9%",2016
"Aggregated Residual Transformations for Deep Neural Networks@@@ResNeXt-101  64x4","Piotr Dollar","Facebook",2016-11-16,"80.9%",2016
"Aggregated Residual Transformations for Deep Neural Networks@@@ResNeXt-101  64x4","Zhuowen Tu","University of California, San Diego",2016-11-16,"80.9%",2016
"Aggregated Residual Transformations for Deep Neural Networks@@@ResNeXt-101  64x4","Kaiming He","Microsoft",2016-11-16,"80.9%",2016
"Supervised Contrastive Learning@@@ResNet-200 (Supervised Contrastive)","Pierre Sermanet","Google",2020-04-23,"80.8%",2020
"Supervised Contrastive Learning@@@ResNet-200 (Supervised Contrastive)","Corey Lynch","Google",2020-04-23,"80.8%",2020
"Supervised Contrastive Learning@@@ResNet-200 (Supervised Contrastive)","Yevgen Chebotar","University of Southern California",2020-04-23,"80.8%",2020
"Supervised Contrastive Learning@@@ResNet-200 (Supervised Contrastive)","Jasmine Hsu","Google",2020-04-23,"80.8%",2020
"Supervised Contrastive Learning@@@ResNet-200 (Supervised Contrastive)","Eric Jang","Google",2020-04-23,"80.8%",2020
"Supervised Contrastive Learning@@@ResNet-200 (Supervised Contrastive)","Stefan Schaal","University of Southern California",2020-04-23,"80.8%",2020
"Supervised Contrastive Learning@@@ResNet-200 (Supervised Contrastive)","Sergey Levine","Google",2020-04-23,"80.8%",2020
"MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks@@@ResNet-50 + MEAL V2","","",2020-09-17,"80.67",2020
"Dual Path Networks@@@DPN-92 (320x320)","Yunpeng Chen","National University of Singapore",2017-07-06,"80.66%",2017
"Dual Path Networks@@@DPN-92 (320x320)","Jianan Li","Beijing Institute of Technology",2017-07-06,"80.66%",2017
"Dual Path Networks@@@DPN-92 (320x320)","Huaxin Xiao","National University of Defense Technology",2017-07-06,"80.66%",2017
"Dual Path Networks@@@DPN-92 (320x320)","Xiaojie Jin","National University of Singapore",2017-07-06,"80.66%",2017
"Dual Path Networks@@@DPN-92 (320x320)","Shuicheng Yan","National University of Singapore",2017-07-06,"80.66%",2017
"Dual Path Networks@@@DPN-92 (320x320)","Jiashi Feng","National University of Singapore",2017-07-06,"80.66%",2017
"Fast AutoAugment@@@ResNet-200 (Fast AA)","Sungbin Lim","",2019-05-01,"80.6%",2019
"Fast AutoAugment@@@ResNet-200 (Fast AA)","Ildoo Kim","",2019-05-01,"80.6%",2019
"Fast AutoAugment@@@ResNet-200 (Fast AA)","Taesup Kim","",2019-05-01,"80.6%",2019
"Fast AutoAugment@@@ResNet-200 (Fast AA)","Chiheon Kim","",2019-05-01,"80.6%",2019
"Fast AutoAugment@@@ResNet-200 (Fast AA)","Sungwoong Kim","",2019-05-01,"80.6%",2019
"CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features@@@ResNeXt-101 (CutMix)","Sangdoo Yun","Naver Corporation",2019-05-13,"80.53%",2019
"CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features@@@ResNeXt-101 (CutMix)","Dongyoon Han","Naver Corporation",2019-05-13,"80.53%",2019
"CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features@@@ResNeXt-101 (CutMix)","Sanghyuk Chun","Naver Corporation",2019-05-13,"80.53%",2019
"CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features@@@ResNeXt-101 (CutMix)","Seong Joon Oh","",2019-05-13,"80.53%",2019
"CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features@@@ResNeXt-101 (CutMix)","Youngjoon Yoo","Naver Corporation",2019-05-13,"80.53%",2019
"CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features@@@ResNeXt-101 (CutMix)","Junsuk Choe","Yonsei University",2019-05-13,"80.53%",2019
"Residual Attention Network for Image Classification@@@Attention-92","Fei Wang","SenseTime",2017-04-23,"80.5%",2017
"Residual Attention Network for Image Classification@@@Attention-92","Mengqing Jiang","",2017-04-23,"80.5%",2017
"Residual Attention Network for Image Classification@@@Attention-92","Chen Qian","SenseTime",2017-04-23,"80.5%",2017
"Residual Attention Network for Image Classification@@@Attention-92","Shuo Yang","The Chinese University of Hong Kong",2017-04-23,"80.5%",2017
"Residual Attention Network for Image Classification@@@Attention-92","Cheng Li","University of Illinois at UrbanaChampaign",2017-04-23,"80.5%",2017
"Residual Attention Network for Image Classification@@@Attention-92","Honggang Zhang","Beijing University of Posts and Telecommunications",2017-04-23,"80.5%",2017
"Residual Attention Network for Image Classification@@@Attention-92","Xiaogang Wang","The Chinese University of Hong Kong",2017-04-23,"80.5%",2017
"Residual Attention Network for Image Classification@@@Attention-92","Xiaoou Tang","SenseTime",2017-04-23,"80.5%",2017
"Neural Architecture Transfer@@@NAT-M4","Catherine Wong","Massachusetts Institute of Technology",2020-05-12,"80.5%",2020
"Neural Architecture Transfer@@@NAT-M4","Neil Houlsby","Google",2020-05-12,"80.5%",2020
"Neural Architecture Transfer@@@NAT-M4","Yifeng Lu","",2020-05-12,"80.5%",2020
"Neural Architecture Transfer@@@NAT-M4","Andrea Gesmundo","Google",2020-05-12,"80.5%",2020
"Attentional Feature Fusion@@@iAFF-ResNeXt-50-32x4d","Yimian Dai","",2020-09-29,"80.22%",2020
"Attentional Feature Fusion@@@iAFF-ResNeXt-50-32x4d","Fabian Gieseke","",2020-09-29,"80.22%",2020
"Attentional Feature Fusion@@@iAFF-ResNeXt-50-32x4d","Stefan Oehmcke","",2020-09-29,"80.22%",2020
"Attentional Feature Fusion@@@iAFF-ResNeXt-50-32x4d","Yiquan Wu","",2020-09-29,"80.22%",2020
"Attentional Feature Fusion@@@iAFF-ResNeXt-50-32x4d","Kobus Barnard","",2020-09-29,"80.22%",2020
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNet-B0","Hugo Touvron","",2020-03-18,"80.2%",2020
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNet-B0","Andrea Vedaldi","",2020-03-18,"80.2%",2020
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNet-B0","Matthijs Douze","",2020-03-18,"80.2%",2020
"Fixing the train-test resolution discrepancy: FixEfficientNet@@@FixEfficientNet-B0","Hervé Jégou","",2020-03-18,"80.2%",2020
"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning@@@Inception ResNet V2","Christian Szegedy","Google",2016-02-23,"80.1%",2016
"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning@@@Inception ResNet V2","Sergey Ioffe","Google",2016-02-23,"80.1%",2016
"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning@@@Inception ResNet V2","Vincent Vanhoucke","Google",2016-02-23,"80.1%",2016
"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning@@@Inception ResNet V2","Alexander A. Alemi","Google",2016-02-23,"80.1%",2016
"Exploring Randomly Wired Neural Networks for Image Recognition@@@RandWire-WS","Saining Xie","Facebook",2019-04-02,"80.1%",2019
"Exploring Randomly Wired Neural Networks for Image Recognition@@@RandWire-WS","Alexander Kirillov","Facebook",2019-04-02,"80.1%",2019
"Exploring Randomly Wired Neural Networks for Image Recognition@@@RandWire-WS","Ross Girshick","",2019-04-02,"80.1%",2019
"Exploring Randomly Wired Neural Networks for Image Recognition@@@RandWire-WS","Kaiming He","Facebook",2019-04-02,"80.1%",2019
"Dual Path Networks@@@DPN-131 (224x224) (80M)","Yunpeng Chen","National University of Singapore",2017-07-06,"80.07%",2017
"Dual Path Networks@@@DPN-131 (224x224) (80M)","Jianan Li","Beijing Institute of Technology",2017-07-06,"80.07%",2017
"Dual Path Networks@@@DPN-131 (224x224) (80M)","Huaxin Xiao","National University of Defense Technology",2017-07-06,"80.07%",2017
"Dual Path Networks@@@DPN-131 (224x224) (80M)","Xiaojie Jin","National University of Singapore",2017-07-06,"80.07%",2017
"Dual Path Networks@@@DPN-131 (224x224) (80M)","Shuicheng Yan","National University of Singapore",2017-07-06,"80.07%",2017
"Dual Path Networks@@@DPN-131 (224x224) (80M)","Jiashi Feng","National University of Singapore",2017-07-06,"80.07%",2017
"Dual Path Networks@@@DPN-98 (224x224)","Yunpeng Chen","National University of Singapore",2017-07-06,"79.95%",2017
"Dual Path Networks@@@DPN-98 (224x224)","Jianan Li","Beijing Institute of Technology",2017-07-06,"79.95%",2017
"Dual Path Networks@@@DPN-98 (224x224)","Huaxin Xiao","National University of Defense Technology",2017-07-06,"79.95%",2017
"Dual Path Networks@@@DPN-98 (224x224)","Xiaojie Jin","National University of Singapore",2017-07-06,"79.95%",2017
"Dual Path Networks@@@DPN-98 (224x224)","Shuicheng Yan","National University of Singapore",2017-07-06,"79.95%",2017
"Dual Path Networks@@@DPN-98 (224x224)","Jiashi Feng","National University of Singapore",2017-07-06,"79.95%",2017
"Data-Driven Neuron Allocation for Scale Aggregation Networks@@@ScaleNet-152","Li Yi","SenseTime",2019-04-20,"79.94%",2019
"Data-Driven Neuron Allocation for Scale Aggregation Networks@@@ScaleNet-152","Zhanghui Kuang","SenseTime",2019-04-20,"79.94%",2019
"Data-Driven Neuron Allocation for Scale Aggregation Networks@@@ScaleNet-152","Yimin Chen","",2019-04-20,"79.94%",2019
"Data-Driven Neuron Allocation for Scale Aggregation Networks@@@ScaleNet-152","Wayne Zhang","SenseTime",2019-04-20,"79.94%",2019
"Identity Mappings in Deep Residual Networks@@@ResNet-200","Kaiming He","Microsoft",2016-03-16,"79.9%",2016
"Identity Mappings in Deep Residual Networks@@@ResNet-200","Xiangyu Zhang","Microsoft",2016-03-16,"79.9%",2016
"Identity Mappings in Deep Residual Networks@@@ResNet-200","Shaoqing Ren","Microsoft",2016-03-16,"79.9%",2016
"Identity Mappings in Deep Residual Networks@@@ResNet-200","Jian Sun","Microsoft",2016-03-16,"79.9%",2016
"Designing Network Design Spaces@@@RegNetY-8.0GF","Ilija Radosavovic","Facebook",2020-03-30,"79.9%",2020
"Designing Network Design Spaces@@@RegNetY-8.0GF","Justin Johnson","Facebook",2020-03-30,"79.9%",2020
"Designing Network Design Spaces@@@RegNetY-8.0GF","Saining Xie","Facebook",2020-03-30,"79.9%",2020
"Designing Network Design Spaces@@@RegNetY-8.0GF","Wan-Yen Lo","Facebook",2020-03-30,"79.9%",2020
"Designing Network Design Spaces@@@RegNetY-8.0GF","Piotr Dollar","",2020-03-30,"79.9%",2020
"Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation@@@Modified Aligned Xception","Liang-Chieh Chen","Google",2018-02-07,"79.81%",2018
"Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation@@@Modified Aligned Xception","Yukun Zhu","Google",2018-02-07,"79.81%",2018
"Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation@@@Modified Aligned Xception","George Papandreou","Google",2018-02-07,"79.81%",2018
"Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation@@@Modified Aligned Xception","Florian Schroff","Google",2018-02-07,"79.81%",2018
"Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation@@@Modified Aligned Xception","Hartwig Adam","Google",2018-02-07,"79.81%",2018
"Selective Kernel Networks@@@SKNet-101","","",2019-03-15,"79.81%",2019
"CSPNet: A New Backbone that can Enhance Learning Capability of CNN@@@CSPResNeXt-50 (Mish+Aug)","Chien-Yao Wang","Academia Sinica",2019-11-27,"79.8%",2019
"CSPNet: A New Backbone that can Enhance Learning Capability of CNN@@@CSPResNeXt-50 (Mish+Aug)","Hong-Yuan Mark Liao","Academia Sinica",2019-11-27,"79.8%",2019
"CSPNet: A New Backbone that can Enhance Learning Capability of CNN@@@CSPResNeXt-50 (Mish+Aug)","I-Hau Yeh","",2019-11-27,"79.8%",2019
"CSPNet: A New Backbone that can Enhance Learning Capability of CNN@@@CSPResNeXt-50 (Mish+Aug)","Yueh-Hua Wu","Academia Sinica",2019-11-27,"79.8%",2019
"CSPNet: A New Backbone that can Enhance Learning Capability of CNN@@@CSPResNeXt-50 (Mish+Aug)","Ping-Yang Chen","National Chiao Tung University",2019-11-27,"79.8%",2019
"CSPNet: A New Backbone that can Enhance Learning Capability of CNN@@@CSPResNeXt-50 (Mish+Aug)","Jun-Wei Hsieh","National Chiao Tung University",2019-11-27,"79.8%",2019
"Mish: A Self Regularized Non-Monotonic Activation Function@@@CSPResNeXt-50 + Mish","Diganta Misra","",2019-08-23,"79.8%",2019
"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks@@@EfficientNet-B2","Mingxing Tan","Google",2019-05-28,"79.8%",2019
"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks@@@EfficientNet-B2","Quoc V. Le","Google",2019-05-28,"79.8%",2019
"Fixing the train-test resolution discrepancy@@@FixResNet-50 CutMix","Hugo Touvron","Facebook",2019-06-14,"79.8%",2019
"Fixing the train-test resolution discrepancy@@@FixResNet-50 CutMix","Andrea Vedaldi","University of Oxford",2019-06-14,"79.8%",2019
"Fixing the train-test resolution discrepancy@@@FixResNet-50 CutMix","Matthijs Douze","Facebook",2019-06-14,"79.8%",2019
"Fixing the train-test resolution discrepancy@@@FixResNet-50 CutMix","Hervé Jégou","Facebook",2019-06-14,"79.8%",2019
"Grafit: Learning fine-grained image representations with coarse labels@@@Grafit (ResNet-50)","Jianlong Fu","Microsoft",2020-11-25,"79.6%",2020
"Grafit: Learning fine-grained image representations with coarse labels@@@Grafit (ResNet-50)","Heliang Zheng","University of Science and Technology of China",2020-11-25,"79.6%",2020
"Grafit: Learning fine-grained image representations with coarse labels@@@Grafit (ResNet-50)","Tao Mei","Microsoft",2020-11-25,"79.6%",2020
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain R50-AA-500","Maxim Berman","",2019-02-14,"79.4%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain R50-AA-500","Hervé Jégou","",2019-02-14,"79.4%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain R50-AA-500","Andrea Vedaldi","",2019-02-14,"79.4%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain R50-AA-500","Iasonas Kokkinos","",2019-02-14,"79.4%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain R50-AA-500","Matthijs Douze","",2019-02-14,"79.4%",2019
"Adversarial AutoAugment@@@ResNet-50 (Adversarial Autoaugment)","Xinyu Zhang","Huawei",2019-12-24,"79.4%",2019
"Adversarial AutoAugment@@@ResNet-50 (Adversarial Autoaugment)","Qiang Wang","Huawei",2019-12-24,"79.4%",2019
"Adversarial AutoAugment@@@ResNet-50 (Adversarial Autoaugment)","Jian Zhang","Huawei",2019-12-24,"79.4%",2019
"Adversarial AutoAugment@@@ResNet-50 (Adversarial Autoaugment)","Zhao Zhong","Huawei",2019-12-24,"79.4%",2019
"Designing Network Design Spaces@@@RegNetY-4.0GF","Ilija Radosavovic","Facebook",2020-03-30,"79.4%",2020
"Designing Network Design Spaces@@@RegNetY-4.0GF","Justin Johnson","Facebook",2020-03-30,"79.4%",2020
"Designing Network Design Spaces@@@RegNetY-4.0GF","Saining Xie","Facebook",2020-03-30,"79.4%",2020
"Designing Network Design Spaces@@@RegNetY-4.0GF","Wan-Yen Lo","Facebook",2020-03-30,"79.4%",2020
"Designing Network Design Spaces@@@RegNetY-4.0GF","Piotr Dollar","",2020-03-30,"79.4%",2020
"LIP: Local Importance-based Pooling@@@LIP-ResNet-101","Ziteng Gao","Nanjing University",2019-08-12,"79.33%",2019
"LIP: Local Importance-based Pooling@@@LIP-ResNet-101","Limin Wang","Nanjing University",2019-08-12,"79.33%",2019
"LIP: Local Importance-based Pooling@@@LIP-ResNet-101","Gangshan Wu","Nanjing University",2019-08-12,"79.33%",2019
"Dual Path Networks@@@DPN-92 (224x224)","Yunpeng Chen","National University of Singapore",2017-07-06,"79.27%",2017
"Dual Path Networks@@@DPN-92 (224x224)","Jianan Li","Beijing Institute of Technology",2017-07-06,"79.27%",2017
"Dual Path Networks@@@DPN-92 (224x224)","Huaxin Xiao","National University of Defense Technology",2017-07-06,"79.27%",2017
"Dual Path Networks@@@DPN-92 (224x224)","Xiaojie Jin","National University of Singapore",2017-07-06,"79.27%",2017
"Dual Path Networks@@@DPN-92 (224x224)","Shuicheng Yan","National University of Singapore",2017-07-06,"79.27%",2017
"Dual Path Networks@@@DPN-92 (224x224)","Jiashi Feng","National University of Singapore",2017-07-06,"79.27%",2017
"Revisiting Unreasonable Effectiveness of Data in Deep Learning Era@@@ResNet-101 (JFT-300M Finetuning)","Chen Sun","Google",2017-07-10,"79.2%",2017
"Revisiting Unreasonable Effectiveness of Data in Deep Learning Era@@@ResNet-101 (JFT-300M Finetuning)","Abhinav Shrivastava","Carnegie Mellon University",2017-07-10,"79.2%",2017
"Revisiting Unreasonable Effectiveness of Data in Deep Learning Era@@@ResNet-101 (JFT-300M Finetuning)","Saurabh Singh","Google",2017-07-10,"79.2%",2017
"Revisiting Unreasonable Effectiveness of Data in Deep Learning Era@@@ResNet-101 (JFT-300M Finetuning)","Abhinav Gupta","Carnegie Mellon University",2017-07-10,"79.2%",2017
"Multiscale Deep Equilibrium Models@@@Multiscale DEQ (MDEQ-XL)","Shaojie Bai","",2020-06-15,"79.2%",2020
"Multiscale Deep Equilibrium Models@@@Multiscale DEQ (MDEQ-XL)","V. Koltun","",2020-06-15,"79.2%",2020
"Multiscale Deep Equilibrium Models@@@Multiscale DEQ (MDEQ-XL)","Vladlen Koltun","",2020-06-15,"79.2%",2020
"Multiscale Deep Equilibrium Models@@@Multiscale DEQ (MDEQ-XL)","J. Zico Kolter","",2020-06-15,"79.2%",2020
"Data-Driven Neuron Allocation for Scale Aggregation Networks@@@ScaleNet-101","Li Yi","SenseTime",2019-04-20,"79.18%",2019
"Data-Driven Neuron Allocation for Scale Aggregation Networks@@@ScaleNet-101","Zhanghui Kuang","SenseTime",2019-04-20,"79.18%",2019
"Data-Driven Neuron Allocation for Scale Aggregation Networks@@@ScaleNet-101","Yimin Chen","",2019-04-20,"79.18%",2019
"Data-Driven Neuron Allocation for Scale Aggregation Networks@@@ScaleNet-101","Wayne Zhang","SenseTime",2019-04-20,"79.18%",2019
"Fixing the train-test resolution discrepancy@@@FixResNet-50","Hugo Touvron","Facebook",2019-06-14,"79.1%",2019
"Fixing the train-test resolution discrepancy@@@FixResNet-50","Andrea Vedaldi","University of Oxford",2019-06-14,"79.1%",2019
"Fixing the train-test resolution discrepancy@@@FixResNet-50","Matthijs Douze","Facebook",2019-06-14,"79.1%",2019
"Fixing the train-test resolution discrepancy@@@FixResNet-50","Hervé Jégou","Facebook",2019-06-14,"79.1%",2019
"Attention Augmented Convolutional Networks@@@AA-ResNet-152","Irwan Bello","Google",2019-04-22,"79.1%",2019
"Attention Augmented Convolutional Networks@@@AA-ResNet-152","Barret Zoph","Google",2019-04-22,"79.1%",2019
"Attention Augmented Convolutional Networks@@@AA-ResNet-152","Quoc V. Le","Google",2019-04-22,"79.1%",2019
"Attention Augmented Convolutional Networks@@@AA-ResNet-152","Ashish Vaswani","Google",2019-04-22,"79.1%",2019
"Attention Augmented Convolutional Networks@@@AA-ResNet-152","Jonathon Shlens","Google",2019-04-22,"79.1%",2019
"Unsupervised Data Augmentation for Consistency Training@@@ResNet-50 (UDA)","Qizhe Xie","",2019-04-29,"79.04%",2019
"Unsupervised Data Augmentation for Consistency Training@@@ResNet-50 (UDA)","Zihang Dai","",2019-04-29,"79.04%",2019
"Unsupervised Data Augmentation for Consistency Training@@@ResNet-50 (UDA)","Eduard Hovy","",2019-04-29,"79.04%",2019
"Unsupervised Data Augmentation for Consistency Training@@@ResNet-50 (UDA)","Minh-Thang Luong","",2019-04-29,"79.04%",2019
"Unsupervised Data Augmentation for Consistency Training@@@ResNet-50 (UDA)","Quoc V. Le","",2019-04-29,"79.04%",2019
"Xception: Deep Learning with Depthwise Separable Convolutions@@@Xception","François Chollet","Google",2016-10-07,"79%",2016
"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset@@@MobileNetV3_large_x1_0_ssld","Cheng Cui","",2020-06-18,"79.0%",2020
"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset@@@MobileNetV3_large_x1_0_ssld","Zhi Ye","",2020-06-18,"79.0%",2020
"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset@@@MobileNetV3_large_x1_0_ssld","Yangxi Li","",2020-06-18,"79.0%",2020
"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset@@@MobileNetV3_large_x1_0_ssld","Xinjian Li","",2020-06-18,"79.0%",2020
"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset@@@MobileNetV3_large_x1_0_ssld","Min Yang","",2020-06-18,"79.0%",2020
"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset@@@MobileNetV3_large_x1_0_ssld","Kai Wei","",2020-06-18,"79.0%",2020
"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset@@@MobileNetV3_large_x1_0_ssld","Bing Dai","",2020-06-18,"79.0%",2020
"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset@@@MobileNetV3_large_x1_0_ssld","Yanmei Zhao","",2020-06-18,"79.0%",2020
"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset@@@MobileNetV3_large_x1_0_ssld","Zhongji Liu","",2020-06-18,"79.0%",2020
"Semi-Supervised Recognition under a Noisy and Fine-grained Dataset@@@MobileNetV3_large_x1_0_ssld","Rong Pang","",2020-06-18,"79.0%",2020
"Filter Response Normalization Layer: Eliminating Batch Dependence in the Training of Deep Neural Networks@@@InceptionV3 (FRN layer)","Saurabh Singh","Google",2019-11-21,"78.95%",2019
"Filter Response Normalization Layer: Eliminating Batch Dependence in the Training of Deep Neural Networks@@@InceptionV3 (FRN layer)","Shankar Krishnan","Google",2019-11-21,"78.95%",2019
"Averaging Weights Leads to Wider Optima and Better Generalization@@@ResNet-152 + SWA","Pavel Izmailov","Cornell University",2018-03-14,"78.94%",2018
"Averaging Weights Leads to Wider Optima and Better Generalization@@@ResNet-152 + SWA","Dmitrii Podoprikhin","National Research University  Higher School of Economics",2018-03-14,"78.94%",2018
"Averaging Weights Leads to Wider Optima and Better Generalization@@@ResNet-152 + SWA","D. A. Podoprikhin","National Research University  Higher School of Economics",2018-03-14,"78.94%",2018
"Averaging Weights Leads to Wider Optima and Better Generalization@@@ResNet-152 + SWA","Timur Garipov","Moscow State University",2018-03-14,"78.94%",2018
"Averaging Weights Leads to Wider Optima and Better Generalization@@@ResNet-152 + SWA","Dmitry Vetrov","National Research University  Higher School of Economics",2018-03-14,"78.94%",2018
"Averaging Weights Leads to Wider Optima and Better Generalization@@@ResNet-152 + SWA","Andrew Gordon Wilson","Cornell University",2018-03-14,"78.94%",2018
"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks@@@ECA-Net (ResNet-152)","Qilong Wang","Tianjin University",2019-10-08,"78.92%",2019
"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks@@@ECA-Net (ResNet-152)","Banggu Wu","",2019-10-08,"78.92%",2019
"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks@@@ECA-Net (ResNet-152)","Pengfei Zhu","Tianjin University",2019-10-08,"78.92%",2019
"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks@@@ECA-Net (ResNet-152)","Peihua Li","Dalian University of Technology",2019-10-08,"78.92%",2019
"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks@@@ECA-Net (ResNet-152)","Wangmeng Zuo","Harbin Institute of Technology",2019-10-08,"78.92%",2019
"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks@@@ECA-Net (ResNet-152)","Qinghua Hu","Tianjin University",2019-10-08,"78.92%",2019
"MixConv: Mixed Depthwise Convolutional Kernels@@@MixNet-L","Mingxing Tan","Google",2019-07-22,"78.9%",2019
"MixConv: Mixed Depthwise Convolutional Kernels@@@MixNet-L","Quoc V. Le","Google",2019-07-22,"78.9%",2019
"Self-training with Noisy Student improves ImageNet classification@@@NoisyStudent (EfficientNet-B0)","Qizhe Xie","Carnegie Mellon University",2019-11-11,"78.8%",2019
"Self-training with Noisy Student improves ImageNet classification@@@NoisyStudent (EfficientNet-B0)","Minh-Thang Luong","Google",2019-11-11,"78.8%",2019
"Self-training with Noisy Student improves ImageNet classification@@@NoisyStudent (EfficientNet-B0)","Eduard Hovy","Carnegie Mellon University",2019-11-11,"78.8%",2019
"Self-training with Noisy Student improves ImageNet classification@@@NoisyStudent (EfficientNet-B0)","Quoc V. Le","Google",2019-11-11,"78.8%",2019
"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks@@@EfficientNet-B1","Mingxing Tan","Google",2019-05-28,"78.8%",2019
"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks@@@EfficientNet-B1","Quoc V. Le","Google",2019-05-28,"78.8%",2019
"Designing Network Design Spaces@@@RegNetY-1.6GF","Ilija Radosavovic","Facebook",2020-03-30,"78.8%",2020
"Designing Network Design Spaces@@@RegNetY-1.6GF","Justin Johnson","Facebook",2020-03-30,"78.8%",2020
"Designing Network Design Spaces@@@RegNetY-1.6GF","Saining Xie","Facebook",2020-03-30,"78.8%",2020
"Designing Network Design Spaces@@@RegNetY-1.6GF","Wan-Yen Lo","Facebook",2020-03-30,"78.8%",2020
"Designing Network Design Spaces@@@RegNetY-1.6GF","Piotr Dollar","",2020-03-30,"78.8%",2020
"Rethinking the Inception Architecture for Computer Vision@@@Inception V3","Christian Szegedy","Google",2015-12-02,"78.8%",2015
"Rethinking the Inception Architecture for Computer Vision@@@Inception V3","Vincent Vanhoucke","Google",2015-12-02,"78.8%",2015
"Rethinking the Inception Architecture for Computer Vision@@@Inception V3","Sergey Ioffe","Google",2015-12-02,"78.8%",2015
"Rethinking the Inception Architecture for Computer Vision@@@Inception V3","Jonathon Shlens","Google",2015-12-02,"78.8%",2015
"Rethinking the Inception Architecture for Computer Vision@@@Inception V3","Zbigniew Wojna","University College London",2015-12-02,"78.8%",2015
"Spatial Group-wise Enhance: Improving Semantic Feature Learning in Convolutional Networks@@@SGE-ResNet101","Xiang Li","Nanjing University of Science and Technology",2019-05-23,"78.7980%",2019
"Spatial Group-wise Enhance: Improving Semantic Feature Learning in Convolutional Networks@@@SGE-ResNet101","Xiaolin Hu","",2019-05-23,"78.7980%",2019
"Spatial Group-wise Enhance: Improving Semantic Feature Learning in Convolutional Networks@@@SGE-ResNet101","Jian Yang","",2019-05-23,"78.7980%",2019
"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks@@@ECA-Net (ResNet-101)","Qilong Wang","Tianjin University",2019-10-08,"78.65%",2019
"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks@@@ECA-Net (ResNet-101)","Banggu Wu","",2019-10-08,"78.65%",2019
"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks@@@ECA-Net (ResNet-101)","Pengfei Zhu","Tianjin University",2019-10-08,"78.65%",2019
"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks@@@ECA-Net (ResNet-101)","Peihua Li","Dalian University of Technology",2019-10-08,"78.65%",2019
"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks@@@ECA-Net (ResNet-101)","Wangmeng Zuo","Harbin Institute of Technology",2019-10-08,"78.65%",2019
"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks@@@ECA-Net (ResNet-101)","Qinghua Hu","Tianjin University",2019-10-08,"78.65%",2019
"Res2Net: A New Multi-scale Backbone Architecture@@@Res2Net-50-299","Shanghua Gao","Nankai University",2019-04-02,"78.59%",2019
"Res2Net: A New Multi-scale Backbone Architecture@@@Res2Net-50-299","Ming-Ming Cheng","Nankai University",2019-04-02,"78.59%",2019
"Res2Net: A New Multi-scale Backbone Architecture@@@Res2Net-50-299","Kai Zhao","Nankai University",2019-04-02,"78.59%",2019
"Res2Net: A New Multi-scale Backbone Architecture@@@Res2Net-50-299","Xin-Yu Zhang","Nankai University",2019-04-02,"78.59%",2019
"Res2Net: A New Multi-scale Backbone Architecture@@@Res2Net-50-299","Ming-Hsuan Yang","University of California, Merced",2019-04-02,"78.59%",2019
"Res2Net: A New Multi-scale Backbone Architecture@@@Res2Net-50-299","Philip H. S. Torr","University of Oxford",2019-04-02,"78.59%",2019
"Deep Residual Learning for Image Recognition@@@ResNet-152 (60M)","Kaiming He","Microsoft",2015-12-10,"78.57%",2015
"Deep Residual Learning for Image Recognition@@@ResNet-152 (60M)","Xiangyu Zhang","Xi'an Jiaotong University",2015-12-10,"78.57%",2015
"Deep Residual Learning for Image Recognition@@@ResNet-152 (60M)","Shaoqing Ren","Microsoft",2015-12-10,"78.57%",2015
"Deep Residual Learning for Image Recognition@@@ResNet-152 (60M)","Jian Sun","Microsoft",2015-12-10,"78.57%",2015
"Gated Convolutional Networks with Hybrid Connectivity for Image Classification@@@HCGNet-B","Chuanguang Yang","Chinese Academy of Sciences",2019-08-26,"78.5%",2019
"Gated Convolutional Networks with Hybrid Connectivity for Image Classification@@@HCGNet-B","Zhulin An","Chinese Academy of Sciences",2019-08-26,"78.5%",2019
"Gated Convolutional Networks with Hybrid Connectivity for Image Classification@@@HCGNet-B","Hui Zhu","Chinese Academy of Sciences",2019-08-26,"78.5%",2019
"Gated Convolutional Networks with Hybrid Connectivity for Image Classification@@@HCGNet-B","Xiaolong Hu","Chinese Academy of Sciences",2019-08-26,"78.5%",2019
"Gated Convolutional Networks with Hybrid Connectivity for Image Classification@@@HCGNet-B","Kun Zhang","Chinese Academy of Sciences",2019-08-26,"78.5%",2019
"Gated Convolutional Networks with Hybrid Connectivity for Image Classification@@@HCGNet-B","Kaiqiang Xu","Chinese Academy of Sciences",2019-08-26,"78.5%",2019
"Gated Convolutional Networks with Hybrid Connectivity for Image Classification@@@HCGNet-B","Boyu Diao","",2019-08-26,"78.5%",2019
"Gated Convolutional Networks with Hybrid Connectivity for Image Classification@@@HCGNet-B","Chao Li","Chinese Academy of Sciences",2019-08-26,"78.5%",2019
"Gated Convolutional Networks with Hybrid Connectivity for Image Classification@@@HCGNet-B","Yongjun Xu","Chinese Academy of Sciences",2019-08-26,"78.5%",2019
"Deformable Kernels: Adapting Effective Receptive Fields for Object Deformation@@@ResNet-50-DW (Deformable Kernels)","Hang Gao","University of California, Berkeley",2019-10-07,"78.5%",2019
"Deformable Kernels: Adapting Effective Receptive Fields for Object Deformation@@@ResNet-50-DW (Deformable Kernels)","Xizhou Zhu","University of Science and Technology of China",2019-10-07,"78.5%",2019
"Deformable Kernels: Adapting Effective Receptive Fields for Object Deformation@@@ResNet-50-DW (Deformable Kernels)","Stephen Lin","Microsoft",2019-10-07,"78.5%",2019
"Deformable Kernels: Adapting Effective Receptive Fields for Object Deformation@@@ResNet-50-DW (Deformable Kernels)","Jifeng Dai","Microsoft",2019-10-07,"78.5%",2019
"Dual Path Networks@@@DPN-68 (320x320, Mean-Max Pooling)","Yunpeng Chen","National University of Singapore",2017-07-06,"78.49%",2017
"Dual Path Networks@@@DPN-68 (320x320, Mean-Max Pooling)","Jianan Li","Beijing Institute of Technology",2017-07-06,"78.49%",2017
"Dual Path Networks@@@DPN-68 (320x320, Mean-Max Pooling)","Huaxin Xiao","National University of Defense Technology",2017-07-06,"78.49%",2017
"Dual Path Networks@@@DPN-68 (320x320, Mean-Max Pooling)","Xiaojie Jin","National University of Singapore",2017-07-06,"78.49%",2017
"Dual Path Networks@@@DPN-68 (320x320, Mean-Max Pooling)","Shuicheng Yan","National University of Singapore",2017-07-06,"78.49%",2017
"Dual Path Networks@@@DPN-68 (320x320, Mean-Max Pooling)","Jiashi Feng","National University of Singapore",2017-07-06,"78.49%",2017
"SRM : A Style-based Recalibration Module for Convolutional Neural Networks@@@SRM-ResNet-101","HyunJae Lee","",2019-03-26,"78.47%",2019
"SRM : A Style-based Recalibration Module for Convolutional Neural Networks@@@SRM-ResNet-101","Hyo-Eun Kim","",2019-03-26,"78.47%",2019
"SRM : A Style-based Recalibration Module for Convolutional Neural Networks@@@SRM-ResNet-101","Hyeonseob Nam","",2019-03-26,"78.47%",2019
"Averaging Weights Leads to Wider Optima and Better Generalization@@@DenseNet-161 + SWA","Pavel Izmailov","Cornell University",2018-03-14,"78.44%",2018
"Averaging Weights Leads to Wider Optima and Better Generalization@@@DenseNet-161 + SWA","Dmitrii Podoprikhin","National Research University  Higher School of Economics",2018-03-14,"78.44%",2018
"Averaging Weights Leads to Wider Optima and Better Generalization@@@DenseNet-161 + SWA","D. A. Podoprikhin","National Research University  Higher School of Economics",2018-03-14,"78.44%",2018
"Averaging Weights Leads to Wider Optima and Better Generalization@@@DenseNet-161 + SWA","Timur Garipov","Moscow State University",2018-03-14,"78.44%",2018
"Averaging Weights Leads to Wider Optima and Better Generalization@@@DenseNet-161 + SWA","Dmitry Vetrov","National Research University  Higher School of Economics",2018-03-14,"78.44%",2018
"Averaging Weights Leads to Wider Optima and Better Generalization@@@DenseNet-161 + SWA","Andrew Gordon Wilson","Cornell University",2018-03-14,"78.44%",2018
"CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features@@@ResNet-50 (CutMix)","Sangdoo Yun","Naver Corporation",2019-05-13,"78.4%",2019
"CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features@@@ResNet-50 (CutMix)","Dongyoon Han","Naver Corporation",2019-05-13,"78.4%",2019
"CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features@@@ResNet-50 (CutMix)","Sanghyuk Chun","Naver Corporation",2019-05-13,"78.4%",2019
"CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features@@@ResNet-50 (CutMix)","Seong Joon Oh","",2019-05-13,"78.4%",2019
"CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features@@@ResNet-50 (CutMix)","Youngjoon Yoo","Naver Corporation",2019-05-13,"78.4%",2019
"CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features@@@ResNet-50 (CutMix)","Junsuk Choe","Yonsei University",2019-05-13,"78.4%",2019
"CondConv: Conditionally Parameterized Convolutions for Efficient Inference@@@EfficientNet-B0 (CondConv)","Brandon Yang","Google",2019-04-10,"78.3%",2019
"CondConv: Conditionally Parameterized Convolutions for Efficient Inference@@@EfficientNet-B0 (CondConv)","Gabriel Bender","Google",2019-04-10,"78.3%",2019
"CondConv: Conditionally Parameterized Convolutions for Efficient Inference@@@EfficientNet-B0 (CondConv)","Quoc V. Le","Google",2019-04-10,"78.3%",2019
"CondConv: Conditionally Parameterized Convolutions for Efficient Inference@@@EfficientNet-B0 (CondConv)","Jiquan Ngiam","Google",2019-04-10,"78.3%",2019
"Deep Residual Learning for Image Recognition@@@ResNet-101","Kaiming He","Microsoft",2015-12-10,"78.25%",2015
"Deep Residual Learning for Image Recognition@@@ResNet-101","Xiangyu Zhang","Xi'an Jiaotong University",2015-12-10,"78.25%",2015
"Deep Residual Learning for Image Recognition@@@ResNet-101","Shaoqing Ren","Microsoft",2015-12-10,"78.25%",2015
"Deep Residual Learning for Image Recognition@@@ResNet-101","Jian Sun","Microsoft",2015-12-10,"78.25%",2015
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain R50-AA-224","Maxim Berman","",2019-02-14,"78.2%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain R50-AA-224","Hervé Jégou","",2019-02-14,"78.2%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain R50-AA-224","Andrea Vedaldi","",2019-02-14,"78.2%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain R50-AA-224","Iasonas Kokkinos","",2019-02-14,"78.2%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain R50-AA-224","Matthijs Douze","",2019-02-14,"78.2%",2019
"LIP: Local Importance-based Pooling@@@ResNet-50 (LIP Bottleneck-256)","Ziteng Gao","Nanjing University",2019-08-12,"78.15%",2019
"LIP: Local Importance-based Pooling@@@ResNet-50 (LIP Bottleneck-256)","Limin Wang","Nanjing University",2019-08-12,"78.15%",2019
"LIP: Local Importance-based Pooling@@@ResNet-50 (LIP Bottleneck-256)","Gangshan Wu","Nanjing University",2019-08-12,"78.15%",2019
"Wide Residual Networks@@@WRN-50-2-bottleneck","Sergey Zagoruyko","",2016-05-23,"78.1%",2016
"Wide Residual Networks@@@WRN-50-2-bottleneck","Nikos Komodakis","",2016-05-23,"78.1%",2016
"Dual Path Networks@@@DPN-68 (320x320)","Yunpeng Chen","National University of Singapore",2017-07-06,"77.85%",2017
"Dual Path Networks@@@DPN-68 (320x320)","Jianan Li","Beijing Institute of Technology",2017-07-06,"77.85%",2017
"Dual Path Networks@@@DPN-68 (320x320)","Huaxin Xiao","National University of Defense Technology",2017-07-06,"77.85%",2017
"Dual Path Networks@@@DPN-68 (320x320)","Xiaojie Jin","National University of Singapore",2017-07-06,"77.85%",2017
"Dual Path Networks@@@DPN-68 (320x320)","Shuicheng Yan","National University of Singapore",2017-07-06,"77.85%",2017
"Dual Path Networks@@@DPN-68 (320x320)","Jiashi Feng","National University of Singapore",2017-07-06,"77.85%",2017
"Densely Connected Convolutional Networks@@@DenseNet-264","Gao Huang","Cornell University",2016-08-25,"77.85%",2016
"Densely Connected Convolutional Networks@@@DenseNet-264","Zhuang Liu","Tsinghua University",2016-08-25,"77.85%",2016
"Densely Connected Convolutional Networks@@@DenseNet-264","Laurens van der Maaten","Facebook",2016-08-25,"77.85%",2016
"Densely Connected Convolutional Networks@@@DenseNet-264","Kilian Q. Weinberger","Cornell University",2016-08-25,"77.85%",2016
"Fast AutoAugment@@@ResNet-50 (Fast AA)","Sungbin Lim","",2019-05-01,"77.6%",2019
"Fast AutoAugment@@@ResNet-50 (Fast AA)","Ildoo Kim","",2019-05-01,"77.6%",2019
"Fast AutoAugment@@@ResNet-50 (Fast AA)","Taesup Kim","",2019-05-01,"77.6%",2019
"Fast AutoAugment@@@ResNet-50 (Fast AA)","Chiheon Kim","",2019-05-01,"77.6%",2019
"Fast AutoAugment@@@ResNet-50 (Fast AA)","Sungwoong Kim","",2019-05-01,"77.6%",2019
"Adaptively Connected Neural Networks@@@ACNet (ResNet-50)","Guangrun Wang","Sun Yat-sen University",2019-04-07,"77.5%",2019
"Adaptively Connected Neural Networks@@@ACNet (ResNet-50)","Keze Wang","University of California, Los Angeles",2019-04-07,"77.5%",2019
"Adaptively Connected Neural Networks@@@ACNet (ResNet-50)","Liang Lin","Sun Yat-sen University",2019-04-07,"77.5%",2019
"Puzzle Mix: Exploiting Saliency and Local Statistics for Optimal Mixup@@@ResNet-50","Janghyun Kim","Seoul National University",2020-09-15,"77.5%",2020
"Puzzle Mix: Exploiting Saliency and Local Statistics for Optimal Mixup@@@ResNet-50","Jang Hyun Kim","Seoul National University",2020-09-15,"77.5%",2020
"Puzzle Mix: Exploiting Saliency and Local Statistics for Optimal Mixup@@@ResNet-50","Wonho Choo","Seoul National University",2020-09-15,"77.5%",2020
"Puzzle Mix: Exploiting Saliency and Local Statistics for Optimal Mixup@@@ResNet-50","Hyun Oh Song","Seoul National University",2020-09-15,"77.5%",2020
"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks@@@ECA-Net (ResNet-50)","Qilong Wang","Tianjin University",2019-10-08,"77.48%",2019
"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks@@@ECA-Net (ResNet-50)","Banggu Wu","",2019-10-08,"77.48%",2019
"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks@@@ECA-Net (ResNet-50)","Pengfei Zhu","Tianjin University",2019-10-08,"77.48%",2019
"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks@@@ECA-Net (ResNet-50)","Peihua Li","Dalian University of Technology",2019-10-08,"77.48%",2019
"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks@@@ECA-Net (ResNet-50)","Wangmeng Zuo","Harbin Institute of Technology",2019-10-08,"77.48%",2019
"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks@@@ECA-Net (ResNet-50)","Qinghua Hu","Tianjin University",2019-10-08,"77.48%",2019
"Densely Connected Convolutional Networks@@@DenseNet-201","Gao Huang","Cornell University",2016-08-25,"77.42%",2016
"Densely Connected Convolutional Networks@@@DenseNet-201","Zhuang Liu","Tsinghua University",2016-08-25,"77.42%",2016
"Densely Connected Convolutional Networks@@@DenseNet-201","Laurens van der Maaten","Facebook",2016-08-25,"77.42%",2016
"Densely Connected Convolutional Networks@@@DenseNet-201","Kilian Q. Weinberger","Cornell University",2016-08-25,"77.42%",2016
"Filter Response Normalization Layer: Eliminating Batch Dependence in the Training of Deep Neural Networks@@@ResnetV2 50 (FRN layer)","Saurabh Singh","Google",2019-11-21,"77.21%",2019
"Filter Response Normalization Layer: Eliminating Batch Dependence in the Training of Deep Neural Networks@@@ResnetV2 50 (FRN layer)","Shankar Krishnan","Google",2019-11-21,"77.21%",2019
"Bag of Tricks for Image Classification with Convolutional Neural Networks@@@ResNet-50-D","Tong He","Amazon.com",2018-12-04,"77.16%",2018
"Bag of Tricks for Image Classification with Convolutional Neural Networks@@@ResNet-50-D","Zhi Zhang","Amazon.com",2018-12-04,"77.16%",2018
"Bag of Tricks for Image Classification with Convolutional Neural Networks@@@ResNet-50-D","Hang Zhang","Amazon.com",2018-12-04,"77.16%",2018
"Bag of Tricks for Image Classification with Convolutional Neural Networks@@@ResNet-50-D","Zhongyue Zhang","Amazon.com",2018-12-04,"77.16%",2018
"Bag of Tricks for Image Classification with Convolutional Neural Networks@@@ResNet-50-D","Junyuan Xie","Amazon.com",2018-12-04,"77.16%",2018
"Bag of Tricks for Image Classification with Convolutional Neural Networks@@@ResNet-50-D","Mu Li","Amazon.com",2018-12-04,"77.16%",2018
"Deep Residual Learning for Image Recognition@@@ResNet-50","Kaiming He","Microsoft",2015-12-10,"77.15%",2015
"Deep Residual Learning for Image Recognition@@@ResNet-50","Xiangyu Zhang","Xi'an Jiaotong University",2015-12-10,"77.15%",2015
"Deep Residual Learning for Image Recognition@@@ResNet-50","Shaoqing Ren","Microsoft",2015-12-10,"77.15%",2015
"Deep Residual Learning for Image Recognition@@@ResNet-50","Jian Sun","Microsoft",2015-12-10,"77.15%",2015
"GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet@@@GreedyNAS-A","Shan You","Tsinghua University",2020-03-25,"77.1%",2020
"GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet@@@GreedyNAS-A","Tao Huang","",2020-03-25,"77.1%",2020
"GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet@@@GreedyNAS-A","Mingmin Yang","",2020-03-25,"77.1%",2020
"GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet@@@GreedyNAS-A","Fei Wang","",2020-03-25,"77.1%",2020
"GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet@@@GreedyNAS-A","Chen Qian","",2020-03-25,"77.1%",2020
"GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet@@@GreedyNAS-A","Changshui Zhang","Tsinghua University",2020-03-25,"77.1%",2020
"MixConv: Mixed Depthwise Convolutional Kernels@@@MixNet-M","Mingxing Tan","Google",2019-07-22,"77%",2019
"MixConv: Mixed Depthwise Convolutional Kernels@@@MixNet-M","Quoc V. Le","Google",2019-07-22,"77%",2019
"SCARLET-NAS: Bridging the gap between Stability and Scalability in Weight-sharing Neural Architecture Search@@@SCARLET-A","Xiangxiang Chu","Xiaomi",2019-08-16,"76.9%",2019
"SCARLET-NAS: Bridging the gap between Stability and Scalability in Weight-sharing Neural Architecture Search@@@SCARLET-A","Bo Zhang","Xiaomi",2019-08-16,"76.9%",2019
"SCARLET-NAS: Bridging the gap between Stability and Scalability in Weight-sharing Neural Architecture Search@@@SCARLET-A","Li Jixiang","Xiaomi",2019-08-16,"76.9%",2019
"SCARLET-NAS: Bridging the gap between Stability and Scalability in Weight-sharing Neural Architecture Search@@@SCARLET-A","Li Qingyuan","Xiaomi",2019-08-16,"76.9%",2019
"SCARLET-NAS: Bridging the gap between Stability and Scalability in Weight-sharing Neural Architecture Search@@@SCARLET-A","Xu Ruijun","Xiaomi",2019-08-16,"76.9%",2019
"GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet@@@GreedyNAS-B","Shan You","Tsinghua University",2020-03-25,"76.8%",2020
"GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet@@@GreedyNAS-B","Tao Huang","",2020-03-25,"76.8%",2020
"GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet@@@GreedyNAS-B","Mingmin Yang","",2020-03-25,"76.8%",2020
"GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet@@@GreedyNAS-B","Fei Wang","",2020-03-25,"76.8%",2020
"GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet@@@GreedyNAS-B","Chen Qian","",2020-03-25,"76.8%",2020
"GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet@@@GreedyNAS-B","Changshui Zhang","Tsinghua University",2020-03-25,"76.8%",2020
"MnasNet: Platform-Aware Neural Architecture Search for Mobile@@@MnasNet-A3","Mingxing Tan","Google",2018-07-31,"76.7%",2018
"MnasNet: Platform-Aware Neural Architecture Search for Mobile@@@MnasNet-A3","Bo Chen","Google",2018-07-31,"76.7%",2018
"MnasNet: Platform-Aware Neural Architecture Search for Mobile@@@MnasNet-A3","Ruoming Pang","Google",2018-07-31,"76.7%",2018
"MnasNet: Platform-Aware Neural Architecture Search for Mobile@@@MnasNet-A3","Vijay K. Vasudevan","Google",2018-07-31,"76.7%",2018
"MnasNet: Platform-Aware Neural Architecture Search for Mobile@@@MnasNet-A3","Mark Sandler","Google",2018-07-31,"76.7%",2018
"MnasNet: Platform-Aware Neural Architecture Search for Mobile@@@MnasNet-A3","Andrew Howard","Google",2018-07-31,"76.7%",2018
"MnasNet: Platform-Aware Neural Architecture Search for Mobile@@@MnasNet-A3","Quoc V. Le","Google",2018-07-31,"76.7%",2018
"LIP: Local Importance-based Pooling@@@LIP-DenseNet-BC-121","Ziteng Gao","Nanjing University",2019-08-12,"76.64%",2019
"LIP: Local Importance-based Pooling@@@LIP-DenseNet-BC-121","Limin Wang","Nanjing University",2019-08-12,"76.64%",2019
"LIP: Local Importance-based Pooling@@@LIP-DenseNet-BC-121","Gangshan Wu","Nanjing University",2019-08-12,"76.64%",2019
"MUXConv: Information Multiplexing in Convolutional Neural Networks@@@MUXNet-l","Zhichao Lu","Michigan State University",2020-03-31,"76.6%",2020
"MUXConv: Information Multiplexing in Convolutional Neural Networks@@@MUXNet-l","Kalyanmoy Deb","Michigan State University",2020-03-31,"76.6%",2020
"MUXConv: Information Multiplexing in Convolutional Neural Networks@@@MUXNet-l","Vishnu Naresh Boddeti","Michigan State University",2020-03-31,"76.6%",2020
"Dual Path Networks@@@DPN-68 (224x224)","Yunpeng Chen","National University of Singapore",2017-07-06,"76.43%",2017
"Dual Path Networks@@@DPN-68 (224x224)","Jianan Li","Beijing Institute of Technology",2017-07-06,"76.43%",2017
"Dual Path Networks@@@DPN-68 (224x224)","Huaxin Xiao","National University of Defense Technology",2017-07-06,"76.43%",2017
"Dual Path Networks@@@DPN-68 (224x224)","Xiaojie Jin","National University of Singapore",2017-07-06,"76.43%",2017
"Dual Path Networks@@@DPN-68 (224x224)","Shuicheng Yan","National University of Singapore",2017-07-06,"76.43%",2017
"Dual Path Networks@@@DPN-68 (224x224)","Jiashi Feng","National University of Singapore",2017-07-06,"76.43%",2017
"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks@@@EfficientNet-B0","Mingxing Tan","Google",2019-05-28,"76.3%",2019
"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks@@@EfficientNet-B0","Quoc V. Le","Google",2019-05-28,"76.3%",2019
"SCARLET-NAS: Bridging the gap between Stability and Scalability in Weight-sharing Neural Architecture Search@@@SCARLET-B","Xiangxiang Chu","Xiaomi",2019-08-16,"76.3%",2019
"SCARLET-NAS: Bridging the gap between Stability and Scalability in Weight-sharing Neural Architecture Search@@@SCARLET-B","Bo Zhang","Xiaomi",2019-08-16,"76.3%",2019
"SCARLET-NAS: Bridging the gap between Stability and Scalability in Weight-sharing Neural Architecture Search@@@SCARLET-B","Li Jixiang","Xiaomi",2019-08-16,"76.3%",2019
"SCARLET-NAS: Bridging the gap between Stability and Scalability in Weight-sharing Neural Architecture Search@@@SCARLET-B","Li Qingyuan","Xiaomi",2019-08-16,"76.3%",2019
"SCARLET-NAS: Bridging the gap between Stability and Scalability in Weight-sharing Neural Architecture Search@@@SCARLET-B","Xu Ruijun","Xiaomi",2019-08-16,"76.3%",2019
"Designing Network Design Spaces@@@RegNetY-800MF","Ilija Radosavovic","Facebook",2020-03-30,"76.3%",2020
"Designing Network Design Spaces@@@RegNetY-800MF","Justin Johnson","Facebook",2020-03-30,"76.3%",2020
"Designing Network Design Spaces@@@RegNetY-800MF","Saining Xie","Facebook",2020-03-30,"76.3%",2020
"Designing Network Design Spaces@@@RegNetY-800MF","Wan-Yen Lo","Facebook",2020-03-30,"76.3%",2020
"Designing Network Design Spaces@@@RegNetY-800MF","Piotr Dollar","",2020-03-30,"76.3%",2020
"Densely Connected Convolutional Networks@@@DenseNet-169","Gao Huang","Cornell University",2016-08-25,"76.2%",2016
"Densely Connected Convolutional Networks@@@DenseNet-169","Zhuang Liu","Tsinghua University",2016-08-25,"76.2%",2016
"Densely Connected Convolutional Networks@@@DenseNet-169","Laurens van der Maaten","Facebook",2016-08-25,"76.2%",2016
"Densely Connected Convolutional Networks@@@DenseNet-169","Kilian Q. Weinberger","Cornell University",2016-08-25,"76.2%",2016
"GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet@@@GreedyNAS-C","Shan You","Tsinghua University",2020-03-25,"76.2%",2020
"GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet@@@GreedyNAS-C","Tao Huang","",2020-03-25,"76.2%",2020
"GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet@@@GreedyNAS-C","Mingmin Yang","",2020-03-25,"76.2%",2020
"GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet@@@GreedyNAS-C","Fei Wang","",2020-03-25,"76.2%",2020
"GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet@@@GreedyNAS-C","Chen Qian","",2020-03-25,"76.2%",2020
"GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet@@@GreedyNAS-C","Changshui Zhang","Tsinghua University",2020-03-25,"76.2%",2020
"MoGA: Searching Beyond MobileNetV3@@@MoGA-A","Xiangxiang Chu","Xiaomi",2019-08-04,"75.9%",2019
"MoGA: Searching Beyond MobileNetV3@@@MoGA-A","Bo Zhang","",2019-08-04,"75.9%",2019
"MoGA: Searching Beyond MobileNetV3@@@MoGA-A","Xu Ruijun","Xiaomi",2019-08-04,"75.9%",2019
"Densely Connected Search Space for More Flexible Neural Architecture Search@@@DenseNAS-A","Jiemin Fang","",2019-06-23,"75.9%",2019
"Densely Connected Search Space for More Flexible Neural Architecture Search@@@DenseNAS-A","Yuzhu Sun","",2019-06-23,"75.9%",2019
"Densely Connected Search Space for More Flexible Neural Architecture Search@@@DenseNAS-A","Qian Zhang","Samsung",2019-06-23,"75.9%",2019
"Densely Connected Search Space for More Flexible Neural Architecture Search@@@DenseNAS-A","Yuan Li","",2019-06-23,"75.9%",2019
"Densely Connected Search Space for More Flexible Neural Architecture Search@@@DenseNAS-A","Wenyu Liu","Huazhong University of Science and Technology",2019-06-23,"75.9%",2019
"Densely Connected Search Space for More Flexible Neural Architecture Search@@@DenseNAS-A","Xinggang Wang","Huazhong University of Science and Technology",2019-06-23,"75.9%",2019
"FractalNet: Ultra-Deep Neural Networks without Residuals@@@FractalNet-34","Gustav Larsson","University of Chicago",2016-05-24,"75.88%",2016
"FractalNet: Ultra-Deep Neural Networks without Residuals@@@FractalNet-34","Michael Maire","Toyota Technological Institute at Chicago",2016-05-24,"75.88%",2016
"FractalNet: Ultra-Deep Neural Networks without Residuals@@@FractalNet-34","Gregory Shakhnarovich","Toyota Technological Institute at Chicago",2016-05-24,"75.88%",2016
"MixConv: Mixed Depthwise Convolutional Kernels@@@MixNet-S","Mingxing Tan","Google",2019-07-22,"75.8%",2019
"MixConv: Mixed Depthwise Convolutional Kernels@@@MixNet-S","Quoc V. Le","Google",2019-07-22,"75.8%",2019
"An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution@@@CoordConv ResNet-50","Rosanne Liu","Uber ",2018-07-09,"75.74%",2018
"An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution@@@CoordConv ResNet-50","Joel Lehman","Uber ",2018-07-09,"75.74%",2018
"An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution@@@CoordConv ResNet-50","Piero Molino","Uber ",2018-07-09,"75.74%",2018
"An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution@@@CoordConv ResNet-50","Felipe Petroski Such","Uber ",2018-07-09,"75.74%",2018
"An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution@@@CoordConv ResNet-50","Eric Frank","Uber ",2018-07-09,"75.74%",2018
"An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution@@@CoordConv ResNet-50","Alex Sergeev","Uber ",2018-07-09,"75.74%",2018
"An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution@@@CoordConv ResNet-50","Jason Yosinski","Uber ",2018-07-09,"75.74%",2018
"GhostNet: More Features from Cheap Operations@@@GhostNet","Kai Han","Huawei",2019-11-27,"75.7%",2019
"GhostNet: More Features from Cheap Operations@@@GhostNet","Yunhe Wang","Peking University",2019-11-27,"75.7%",2019
"GhostNet: More Features from Cheap Operations@@@GhostNet","Qi Tian","University of Texas at San Antonio",2019-11-27,"75.7%",2019
"GhostNet: More Features from Cheap Operations@@@GhostNet","Jianyuan Guo","Huawei",2019-11-27,"75.7%",2019
"GhostNet: More Features from Cheap Operations@@@GhostNet","Chunjing Xu","Huawei",2019-11-27,"75.7%",2019
"GhostNet: More Features from Cheap Operations@@@GhostNet","Chang Xu","University of Sydney",2019-11-27,"75.7%",2019
"Local Relation Networks for Image Recognition@@@LR-Net-26","Han Hu","Microsoft",2019-04-25,"75.7%",2019
"Local Relation Networks for Image Recognition@@@LR-Net-26","Zheng Zhang","Microsoft",2019-04-25,"75.7%",2019
"Local Relation Networks for Image Recognition@@@LR-Net-26","Zhenda Xie","Tsinghua University",2019-04-25,"75.7%",2019
"Local Relation Networks for Image Recognition@@@LR-Net-26","Stephen Lin","Microsoft",2019-04-25,"75.7%",2019
"MnasNet: Platform-Aware Neural Architecture Search for Mobile@@@MnasNet-A2","Mingxing Tan","Google",2018-07-31,"75.6%",2018
"MnasNet: Platform-Aware Neural Architecture Search for Mobile@@@MnasNet-A2","Bo Chen","Google",2018-07-31,"75.6%",2018
"MnasNet: Platform-Aware Neural Architecture Search for Mobile@@@MnasNet-A2","Ruoming Pang","Google",2018-07-31,"75.6%",2018
"MnasNet: Platform-Aware Neural Architecture Search for Mobile@@@MnasNet-A2","Vijay K. Vasudevan","Google",2018-07-31,"75.6%",2018
"MnasNet: Platform-Aware Neural Architecture Search for Mobile@@@MnasNet-A2","Mark Sandler","Google",2018-07-31,"75.6%",2018
"MnasNet: Platform-Aware Neural Architecture Search for Mobile@@@MnasNet-A2","Andrew Howard","Google",2018-07-31,"75.6%",2018
"MnasNet: Platform-Aware Neural Architecture Search for Mobile@@@MnasNet-A2","Quoc V. Le","Google",2018-07-31,"75.6%",2018
"SCARLET-NAS: Bridging the gap between Stability and Scalability in Weight-sharing Neural Architecture Search@@@SCARLET-C","Xiangxiang Chu","Xiaomi",2019-08-16,"75.6%",2019
"SCARLET-NAS: Bridging the gap between Stability and Scalability in Weight-sharing Neural Architecture Search@@@SCARLET-C","Bo Zhang","Xiaomi",2019-08-16,"75.6%",2019
"SCARLET-NAS: Bridging the gap between Stability and Scalability in Weight-sharing Neural Architecture Search@@@SCARLET-C","Li Jixiang","Xiaomi",2019-08-16,"75.6%",2019
"SCARLET-NAS: Bridging the gap between Stability and Scalability in Weight-sharing Neural Architecture Search@@@SCARLET-C","Li Qingyuan","Xiaomi",2019-08-16,"75.6%",2019
"SCARLET-NAS: Bridging the gap between Stability and Scalability in Weight-sharing Neural Architecture Search@@@SCARLET-C","Xu Ruijun","Xiaomi",2019-08-16,"75.6%",2019
"Designing Network Design Spaces@@@RegNetY-600MF","Ilija Radosavovic","Facebook",2020-03-30,"75.5%",2020
"Designing Network Design Spaces@@@RegNetY-600MF","Justin Johnson","Facebook",2020-03-30,"75.5%",2020
"Designing Network Design Spaces@@@RegNetY-600MF","Saining Xie","Facebook",2020-03-30,"75.5%",2020
"Designing Network Design Spaces@@@RegNetY-600MF","Wan-Yen Lo","Facebook",2020-03-30,"75.5%",2020
"Designing Network Design Spaces@@@RegNetY-600MF","Piotr Dollar","",2020-03-30,"75.5%",2020
"ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design@@@ShuffleNet V2","Ningning Ma","Tsinghua University",2018-07-30,"75.4%",2018
"ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design@@@ShuffleNet V2","Xiangyu Zhang","",2018-07-30,"75.4%",2018
"ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design@@@ShuffleNet V2","Hai-Tao Zheng","Tsinghua University",2018-07-30,"75.4%",2018
"ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design@@@ShuffleNet V2","Jian Sun","",2018-07-30,"75.4%",2018
"MUXConv: Information Multiplexing in Convolutional Neural Networks@@@MUXNet-m","Zhichao Lu","Michigan State University",2020-03-31,"75.3%",2020
"MUXConv: Information Multiplexing in Convolutional Neural Networks@@@MUXNet-m","Kalyanmoy Deb","Michigan State University",2020-03-31,"75.3%",2020
"MUXConv: Information Multiplexing in Convolutional Neural Networks@@@MUXNet-m","Vishnu Naresh Boddeti","Michigan State University",2020-03-31,"75.3%",2020
"MnasNet: Platform-Aware Neural Architecture Search for Mobile@@@MnasNet-A1","Mingxing Tan","Google",2018-07-31,"75.2%",2018
"MnasNet: Platform-Aware Neural Architecture Search for Mobile@@@MnasNet-A1","Bo Chen","Google",2018-07-31,"75.2%",2018
"MnasNet: Platform-Aware Neural Architecture Search for Mobile@@@MnasNet-A1","Ruoming Pang","Google",2018-07-31,"75.2%",2018
"MnasNet: Platform-Aware Neural Architecture Search for Mobile@@@MnasNet-A1","Vijay K. Vasudevan","Google",2018-07-31,"75.2%",2018
"MnasNet: Platform-Aware Neural Architecture Search for Mobile@@@MnasNet-A1","Mark Sandler","Google",2018-07-31,"75.2%",2018
"MnasNet: Platform-Aware Neural Architecture Search for Mobile@@@MnasNet-A1","Andrew Howard","Google",2018-07-31,"75.2%",2018
"MnasNet: Platform-Aware Neural Architecture Search for Mobile@@@MnasNet-A1","Quoc V. Le","Google",2018-07-31,"75.2%",2018
"Searching for MobileNetV3@@@MobileNet V3-Large 1.0","Andrew Howard","Google",2019-05-06,"75.2%",2019
"Searching for MobileNetV3@@@MobileNet V3-Large 1.0","Ruoming Pang","Google",2019-05-06,"75.2%",2019
"Searching for MobileNetV3@@@MobileNet V3-Large 1.0","Hartwig Adam","Google",2019-05-06,"75.2%",2019
"Searching for MobileNetV3@@@MobileNet V3-Large 1.0","Quoc V. Le","Google",2019-05-06,"75.2%",2019
"Searching for MobileNetV3@@@MobileNet V3-Large 1.0","Mark Sandler","Google",2019-05-06,"75.2%",2019
"Searching for MobileNetV3@@@MobileNet V3-Large 1.0","Bo Chen","Google",2019-05-06,"75.2%",2019
"Searching for MobileNetV3@@@MobileNet V3-Large 1.0","Weijun Wang","Google",2019-05-06,"75.2%",2019
"Searching for MobileNetV3@@@MobileNet V3-Large 1.0","Liang-Chieh Chen","Google",2019-05-06,"75.2%",2019
"Searching for MobileNetV3@@@MobileNet V3-Large 1.0","Yukun Zhu","Google",2019-05-06,"75.2%",2019
"Searching for MobileNetV3@@@MobileNet V3-Large 1.0","Mingxing Tan","Google",2019-05-06,"75.2%",2019
"Searching for MobileNetV3@@@MobileNet V3-Large 1.0","Vijay K. Vasudevan","University of Cincinnati",2019-05-06,"75.2%",2019
"Searching for MobileNetV3@@@MobileNet V3-Large 1.0","Grace Chu","Google",2019-05-06,"75.2%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain NASNet-A-Mobile (350px)","Maxim Berman","",2019-02-14,"75.1%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain NASNet-A-Mobile (350px)","Hervé Jégou","",2019-02-14,"75.1%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain NASNet-A-Mobile (350px)","Andrea Vedaldi","",2019-02-14,"75.1%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain NASNet-A-Mobile (350px)","Iasonas Kokkinos","",2019-02-14,"75.1%",2019
"MultiGrain: a unified image embedding for classes and instances@@@MultiGrain NASNet-A-Mobile (350px)","Matthijs Douze","",2019-02-14,"75.1%",2019
"DiCENet: Dimension-wise Convolutions for Efficient Networks@@@DiCENet","Sachin Mehta","",2019-06-08,"75.1%",2019
"DiCENet: Dimension-wise Convolutions for Efficient Networks@@@DiCENet","Hannaneh Hajishirzi","",2019-06-08,"75.1%",2019
"DiCENet: Dimension-wise Convolutions for Efficient Networks@@@DiCENet","Mohammad Rastegari","",2019-06-08,"75.1%",2019
"Densely Connected Convolutional Networks@@@DenseNet-121","Gao Huang","Cornell University",2016-08-25,"74.98%",2016
"Densely Connected Convolutional Networks@@@DenseNet-121","Zhuang Liu","Tsinghua University",2016-08-25,"74.98%",2016
"Densely Connected Convolutional Networks@@@DenseNet-121","Laurens van der Maaten","Facebook",2016-08-25,"74.98%",2016
"Densely Connected Convolutional Networks@@@DenseNet-121","Kilian Q. Weinberger","Cornell University",2016-08-25,"74.98%",2016
"Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours@@@Single-Path NAS","Dimitrios Stamoulis","Carnegie Mellon University",2019-04-05,"74.96%",2019
"Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours@@@Single-Path NAS","Ruizhou Ding","Carnegie Mellon University",2019-04-05,"74.96%",2019
"Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours@@@Single-Path NAS","Di Wang","Microsoft",2019-04-05,"74.96%",2019
"Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours@@@Single-Path NAS","Dimitrios Lymberopoulos","Microsoft",2019-04-05,"74.96%",2019
"Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours@@@Single-Path NAS","Bodhi Priyantha","Microsoft",2019-04-05,"74.96%",2019
"Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours@@@Single-Path NAS","Jie Liu","Harbin Institute of Technology",2019-04-05,"74.96%",2019
"Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours@@@Single-Path NAS","Diana Marculescu","Carnegie Mellon University",2019-04-05,"74.96%",2019
"FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search@@@FBNet-C","Bichen Wu","University of California, Berkeley",2018-12-09,"74.9%",2018
"FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search@@@FBNet-C","Kurt Keutzer","University of California, Berkeley",2018-12-09,"74.9%",2018
"FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search@@@FBNet-C","Xiaoliang Dai","Princeton University",2018-12-09,"74.9%",2018
"FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search@@@FBNet-C","Peizhao Zhang","Facebook",2018-12-09,"74.9%",2018
"FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search@@@FBNet-C","Yanghan Wang","Facebook",2018-12-09,"74.9%",2018
"FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search@@@FBNet-C","Fei Sun","Facebook",2018-12-09,"74.9%",2018
"FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search@@@FBNet-C","Yiming Wu","Facebook",2018-12-09,"74.9%",2018
"FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search@@@FBNet-C","Yuandong Tian","Facebook",2018-12-09,"74.9%",2018
"FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search@@@FBNet-C","Peter Vajda","Facebook",2018-12-09,"74.9%",2018
"FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search@@@FBNet-C","Yangqing Jia","Facebook",2018-12-09,"74.9%",2018
"ESPNetv2: A Light-weight, Power Efficient, and General Purpose Convolutional Neural Network@@@ESPNetv2","Sachin Mehta","University of Washington",2018-11-28,"74.9%",2018
"ESPNetv2: A Light-weight, Power Efficient, and General Purpose Convolutional Neural Network@@@ESPNetv2","Mohammad Rastegari","Allen Institute for Artificial Intelligence",2018-11-28,"74.9%",2018
"ESPNetv2: A Light-weight, Power Efficient, and General Purpose Convolutional Neural Network@@@ESPNetv2","Linda G. Shapiro","University of Washington",2018-11-28,"74.9%",2018
"ESPNetv2: A Light-weight, Power Efficient, and General Purpose Convolutional Neural Network@@@ESPNetv2","Hannaneh Hajishirzi","University of Washington",2018-11-28,"74.9%",2018
"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift@@@Inception V2","Sergey Ioffe","Google",2015-02-11,"74.8%",2015
"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift@@@Inception V2","Christian Szegedy","Google",2015-02-11,"74.8%",2015
"MobileNetV2: Inverted Residuals and Linear Bottlenecks@@@MobileNetV2 (1.4)","Mark Sandler","Google",2018-01-13,"74.7%",2018
"MobileNetV2: Inverted Residuals and Linear Bottlenecks@@@MobileNetV2 (1.4)","Andrew Howard","Google",2018-01-13,"74.7%",2018
"MobileNetV2: Inverted Residuals and Linear Bottlenecks@@@MobileNetV2 (1.4)","Menglong Zhu","Georgia Institute of Technology",2018-01-13,"74.7%",2018
"MobileNetV2: Inverted Residuals and Linear Bottlenecks@@@MobileNetV2 (1.4)","Andrey Zhmoginov","Google",2018-01-13,"74.7%",2018
"MobileNetV2: Inverted Residuals and Linear Bottlenecks@@@MobileNetV2 (1.4)","Liang-Chieh Chen","Google",2018-01-13,"74.7%",2018
"ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware@@@Proxyless","Han Cai","Shanghai Jiao Tong University",2018-12-02,"74.6%",2018
"ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware@@@Proxyless","Ligeng Zhu","Massachusetts Institute of Technology",2018-12-02,"74.6%",2018
"ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware@@@Proxyless","Song Han","Massachusetts Institute of Technology",2018-12-02,"74.6%",2018
"Very Deep Convolutional Networks for Large-Scale Image Recognition@@@VGG-19","Karen Simonyan","University of Oxford",2014-09-04,"74.5%",2014
"Very Deep Convolutional Networks for Large-Scale Image Recognition@@@VGG-19","Andrew Zisserman","University of Oxford",2014-09-04,"74.5%",2014
"Very Deep Convolutional Networks for Large-Scale Image Recognition@@@VGG-16","Karen Simonyan","University of Oxford",2014-09-04,"74.4%",2014
"Very Deep Convolutional Networks for Large-Scale Image Recognition@@@VGG-16","Andrew Zisserman","University of Oxford",2014-09-04,"74.4%",2014
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-MobileNetV2 ×1.0","Yinpeng Chen","Microsoft",2019-12-07,"74.4%",2019
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-MobileNetV2 ×1.0","Xiyang Dai","Microsoft",2019-12-07,"74.4%",2019
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-MobileNetV2 ×1.0","Mengchen Liu","Microsoft",2019-12-07,"74.4%",2019
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-MobileNetV2 ×1.0","Dongdong Chen","Microsoft",2019-12-07,"74.4%",2019
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-MobileNetV2 ×1.0","Lu Yuan","Microsoft",2019-12-07,"74.4%",2019
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-MobileNetV2 ×1.0","Zicheng Liu","Microsoft",2019-12-07,"74.4%",2019
"What's Hidden in a Randomly Weighted Neural Network?@@@Wide ResNet-50 (edge-popup)","Vivek Ramanujan","Allen Institute for Artificial Intelligence",2019-11-29,"73.3%",2019
"What's Hidden in a Randomly Weighted Neural Network?@@@Wide ResNet-50 (edge-popup)","Mitchell Wortsman","University of Washington",2019-11-29,"73.3%",2019
"What's Hidden in a Randomly Weighted Neural Network?@@@Wide ResNet-50 (edge-popup)","Aniruddha Kembhavi","University of Washington",2019-11-29,"73.3%",2019
"What's Hidden in a Randomly Weighted Neural Network?@@@Wide ResNet-50 (edge-popup)","Ali Farhadi","Allen Institute for Artificial Intelligence",2019-11-29,"73.3%",2019
"What's Hidden in a Randomly Weighted Neural Network?@@@Wide ResNet-50 (edge-popup)","Mohammad Rastegari","Allen Institute for Artificial Intelligence",2019-11-29,"73.3%",2019
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-MobileNetV2 ×0.75","Yinpeng Chen","Microsoft",2019-12-07,"72.8%",2019
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-MobileNetV2 ×0.75","Xiyang Dai","Microsoft",2019-12-07,"72.8%",2019
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-MobileNetV2 ×0.75","Mengchen Liu","Microsoft",2019-12-07,"72.8%",2019
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-MobileNetV2 ×0.75","Dongdong Chen","Microsoft",2019-12-07,"72.8%",2019
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-MobileNetV2 ×0.75","Lu Yuan","Microsoft",2019-12-07,"72.8%",2019
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-MobileNetV2 ×0.75","Zicheng Liu","Microsoft",2019-12-07,"72.8%",2019
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-ResNet-18","Yinpeng Chen","Microsoft",2019-12-07,"72.7%",2019
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-ResNet-18","Xiyang Dai","Microsoft",2019-12-07,"72.7%",2019
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-ResNet-18","Mengchen Liu","Microsoft",2019-12-07,"72.7%",2019
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-ResNet-18","Dongdong Chen","Microsoft",2019-12-07,"72.7%",2019
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-ResNet-18","Lu Yuan","Microsoft",2019-12-07,"72.7%",2019
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-ResNet-18","Zicheng Liu","Microsoft",2019-12-07,"72.7%",2019
"Compact Global Descriptor for Neural Networks@@@MobileNet-224 (CGD)","Xiangyu He","",2019-07-23,"72.56%",2019
"Compact Global Descriptor for Neural Networks@@@MobileNet-224 (CGD)","Ke Cheng","Chinese Academy of Sciences",2019-07-23,"72.56%",2019
"Compact Global Descriptor for Neural Networks@@@MobileNet-224 (CGD)","Qiang Chen","",2019-07-23,"72.56%",2019
"Compact Global Descriptor for Neural Networks@@@MobileNet-224 (CGD)","Qinghao Hu","",2019-07-23,"72.56%",2019
"Compact Global Descriptor for Neural Networks@@@MobileNet-224 (CGD)","Peisong Wang","",2019-07-23,"72.56%",2019
"Compact Global Descriptor for Neural Networks@@@MobileNet-224 (CGD)","Jian Cheng","",2019-07-23,"72.56%",2019
"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks@@@ECA-Net (MobileNetV2)","Qilong Wang","Tianjin University",2019-10-08,"72.56%",2019
"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks@@@ECA-Net (MobileNetV2)","Banggu Wu","",2019-10-08,"72.56%",2019
"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks@@@ECA-Net (MobileNetV2)","Pengfei Zhu","Tianjin University",2019-10-08,"72.56%",2019
"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks@@@ECA-Net (MobileNetV2)","Peihua Li","Dalian University of Technology",2019-10-08,"72.56%",2019
"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks@@@ECA-Net (MobileNetV2)","Wangmeng Zuo","Harbin Institute of Technology",2019-10-08,"72.56%",2019
"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks@@@ECA-Net (MobileNetV2)","Qinghua Hu","Tianjin University",2019-10-08,"72.56%",2019
"Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition@@@SPPNet","Kaiming He","Microsoft",2014-06-18,"72.14%",2014
"Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition@@@SPPNet","Xiangyu Zhang","Xi'an Jiaotong University",2014-06-18,"72.14%",2014
"Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition@@@SPPNet","Shaoqing Ren","University of Science and Technology of China",2014-06-18,"72.14%",2014
"Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition@@@SPPNet","Jian Sun","Microsoft",2014-06-18,"72.14%",2014
"On the adequacy of untuned warmup for adaptive optimization@@@ResNet-50","Jerry Ma","Facebook",2019-10-09,"72.1%",2019
"On the adequacy of untuned warmup for adaptive optimization@@@ResNet-50","Denis Yarats","",2019-10-09,"72.1%",2019
"torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation@@@ResNet-18 (PAD-L2 w/ ResNet-34 teacher)","","",2020-11-25,"71.71%",2020
"MUXConv: Information Multiplexing in Convolutional Neural Networks@@@MUXNet-s","Zhichao Lu","Michigan State University",2020-03-31,"71.6%",2020
"MUXConv: Information Multiplexing in Convolutional Neural Networks@@@MUXNet-s","Kalyanmoy Deb","Michigan State University",2020-03-31,"71.6%",2020
"MUXConv: Information Multiplexing in Convolutional Neural Networks@@@MUXNet-s","Vishnu Naresh Boddeti","Michigan State University",2020-03-31,"71.6%",2020
"torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation@@@ResNet-18 (KD w/ ResNet-34 teacher)","","",2020-11-25,"71.37%",2020
"Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition@@@MSRA","Kaiming He","Microsoft",2014-06-18,"71.32%",2014
"Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition@@@MSRA","Xiangyu Zhang","Xi'an Jiaotong University",2014-06-18,"71.32%",2014
"Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition@@@MSRA","Shaoqing Ren","University of Science and Technology of China",2014-06-18,"71.32%",2014
"Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition@@@MSRA","Jian Sun","Microsoft",2014-06-18,"71.32%",2014
"torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation@@@ResNet-18 (L2 w/ ResNet-34 teacher)","","",2020-11-25,"71.08%",2020
"torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation@@@ResNet-18 (CRD w/ ResNet-34 teacher)","","",2020-11-25,"70.93%",2020
"ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices@@@ShuffleNet","Xiangyu Zhang","",2017-07-04,"70.9%",2017
"ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices@@@ShuffleNet","Xinyu Zhou","",2017-07-04,"70.9%",2017
"ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices@@@ShuffleNet","Mengxiao Lin","",2017-07-04,"70.9%",2017
"ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices@@@ShuffleNet","Jian Sun","Xi'an Jiaotong University",2017-07-04,"70.9%",2017
"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications@@@MobileNet-224","Andrew Howard","",2017-04-17,"70.6%",2017
"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications@@@MobileNet-224","Menglong Zhu","",2017-04-17,"70.6%",2017
"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications@@@MobileNet-224","Bo Chen","",2017-04-17,"70.6%",2017
"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications@@@MobileNet-224","Dmitry Kalenichenko","",2017-04-17,"70.6%",2017
"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications@@@MobileNet-224","Weijun Wang","",2017-04-17,"70.6%",2017
"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications@@@MobileNet-224","Tobias Weyand","",2017-04-17,"70.6%",2017
"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications@@@MobileNet-224","M. Andreetto","",2017-04-17,"70.6%",2017
"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications@@@MobileNet-224","Hartwig Adam","",2017-04-17,"70.6%",2017
"torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation@@@ResNet-18 (tf-KD w/ ResNet-18 teacher)","","",2020-11-25,"70.52%",2020
"torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation@@@ResNet-18 (FT w/ ResNet-34 teacher)","","",2020-11-25,"70.45%",2020
"torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation@@@ResNet-18 (SSKD w/ ResNet-34 teacher)","","",2020-11-25,"70.09%",2020
"Going Deeper with Convolutions@@@Inception V1","Christian Szegedy","Google",2014-09-17,"69.8%",2014
"Going Deeper with Convolutions@@@Inception V1","Wei Liu","University of North Carolina at Chapel Hill",2014-09-17,"69.8%",2014
"Going Deeper with Convolutions@@@Inception V1","Yangqing Jia","Google",2014-09-17,"69.8%",2014
"Going Deeper with Convolutions@@@Inception V1","Pierre Sermanet","Google",2014-09-17,"69.8%",2014
"Going Deeper with Convolutions@@@Inception V1","Scott Reed","University of Michigan",2014-09-17,"69.8%",2014
"Going Deeper with Convolutions@@@Inception V1","Dragomir Anguelov","Google",2014-09-17,"69.8%",2014
"Going Deeper with Convolutions@@@Inception V1","Dumitru Erhan","Google",2014-09-17,"69.8%",2014
"Going Deeper with Convolutions@@@Inception V1","Vincent Vanhoucke","Google",2014-09-17,"69.8%",2014
"Going Deeper with Convolutions@@@Inception V1","Andrew Rabinovich","",2014-09-17,"69.8%",2014
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-MobileNetV3-Small","Yinpeng Chen","Microsoft",2019-12-07,"69.7%",2019
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-MobileNetV3-Small","Xiyang Dai","Microsoft",2019-12-07,"69.7%",2019
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-MobileNetV3-Small","Mengchen Liu","Microsoft",2019-12-07,"69.7%",2019
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-MobileNetV3-Small","Dongdong Chen","Microsoft",2019-12-07,"69.7%",2019
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-MobileNetV3-Small","Lu Yuan","Microsoft",2019-12-07,"69.7%",2019
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-MobileNetV3-Small","Zicheng Liu","Microsoft",2019-12-07,"69.7%",2019
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-MobileNetV2 ×0.5","Yinpeng Chen","Microsoft",2019-12-07,"69.4%",2019
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-MobileNetV2 ×0.5","Xiyang Dai","Microsoft",2019-12-07,"69.4%",2019
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-MobileNetV2 ×0.5","Mengchen Liu","Microsoft",2019-12-07,"69.4%",2019
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-MobileNetV2 ×0.5","Dongdong Chen","Microsoft",2019-12-07,"69.4%",2019
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-MobileNetV2 ×0.5","Lu Yuan","Microsoft",2019-12-07,"69.4%",2019
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-MobileNetV2 ×0.5","Zicheng Liu","Microsoft",2019-12-07,"69.4%",2019
"Graph-RISE: Graph-Regularized Image Semantic Embedding@@@Graph-RISE (40M)","Aleksei Timofeev","Google",2019-02-14,"68.29%",2019
"Graph-RISE: Graph-Regularized Image Semantic Embedding@@@Graph-RISE (40M)","Andrew Tomkins","Google",2019-02-14,"68.29%",2019
"Graph-RISE: Graph-Regularized Image Semantic Embedding@@@Graph-RISE (40M)","Zhen Li","Google",2019-02-14,"68.29%",2019
"Graph-RISE: Graph-Regularized Image Semantic Embedding@@@Graph-RISE (40M)","Chun-Ta Lu","Google",2019-02-14,"68.29%",2019
"Graph-RISE: Graph-Regularized Image Semantic Embedding@@@Graph-RISE (40M)","Futang Peng","Google",2019-02-14,"68.29%",2019
"Graph-RISE: Graph-Regularized Image Semantic Embedding@@@Graph-RISE (40M)","Da-Cheng Juan","Google",2019-02-14,"68.29%",2019
"Graph-RISE: Graph-Regularized Image Semantic Embedding@@@Graph-RISE (40M)","Krishnamurthy Viswanathan","Google",2019-02-14,"68.29%",2019
"Graph-RISE: Graph-Regularized Image Semantic Embedding@@@Graph-RISE (40M)","Yi-Ting Chen","Google",2019-02-14,"68.29%",2019
"Graph-RISE: Graph-Regularized Image Semantic Embedding@@@Graph-RISE (40M)","Lucy Gao","Google",2019-02-14,"68.29%",2019
"Graph-RISE: Graph-Regularized Image Semantic Embedding@@@Graph-RISE (40M)","Yaxi Gao","Google",2019-02-14,"68.29%",2019
"Graph-RISE: Graph-Regularized Image Semantic Embedding@@@Graph-RISE (40M)","Sujith Ravi","Google",2019-02-14,"68.29%",2019
"Graph-RISE: Graph-Regularized Image Semantic Embedding@@@Graph-RISE (40M)","Tom Duerig","Google",2019-02-14,"68.29%",2019
"Graph-RISE: Graph-Regularized Image Semantic Embedding@@@Graph-RISE (40M)","Yi-ting Chen","Google",2019-02-14,"68.29%",2019
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-ResNet-10","Yinpeng Chen","Microsoft",2019-12-07,"67.7%",2019
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-ResNet-10","Xiyang Dai","Microsoft",2019-12-07,"67.7%",2019
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-ResNet-10","Mengchen Liu","Microsoft",2019-12-07,"67.7%",2019
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-ResNet-10","Dongdong Chen","Microsoft",2019-12-07,"67.7%",2019
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-ResNet-10","Lu Yuan","Microsoft",2019-12-07,"67.7%",2019
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-ResNet-10","Zicheng Liu","Microsoft",2019-12-07,"67.7%",2019
"MUXConv: Information Multiplexing in Convolutional Neural Networks@@@MUXNet-xs","Zhichao Lu","Michigan State University",2020-03-31,"66.7%",2020
"MUXConv: Information Multiplexing in Convolutional Neural Networks@@@MUXNet-xs","Kalyanmoy Deb","Michigan State University",2020-03-31,"66.7%",2020
"MUXConv: Information Multiplexing in Convolutional Neural Networks@@@MUXNet-xs","Vishnu Naresh Boddeti","Michigan State University",2020-03-31,"66.7%",2020
"Some Improvements on Deep Convolutional Neural Network Based Image Classification@@@Five Base + Five HiRes","Andrew Howard","",2013-12-19,"66.3%",2013
"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks@@@OverFeat - 7 accurate models","Pierre Sermanet","Courant Institute of Mathematical Sciences",2013-12-21,"66.04%",2013
"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks@@@OverFeat - 7 accurate models","David Eigen","Courant Institute of Mathematical Sciences",2013-12-21,"66.04%",2013
"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks@@@OverFeat - 7 accurate models","Xiang Zhang","Courant Institute of Mathematical Sciences",2013-12-21,"66.04%",2013
"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks@@@OverFeat - 7 accurate models","X.-C. Zhang","",2013-12-21,"66.04%",2013
"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks@@@OverFeat - 7 accurate models","Michael Mathieu","Courant Institute of Mathematical Sciences",2013-12-21,"66.04%",2013
"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks@@@OverFeat - 7 accurate models","Rob Fergus","Courant Institute of Mathematical Sciences",2013-12-21,"66.04%",2013
"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks@@@OverFeat - 7 accurate models","Yann LeCun","Courant Institute of Mathematical Sciences",2013-12-21,"66.04%",2013
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-MobileNetV2 ×0.35","Yinpeng Chen","Microsoft",2019-12-07,"64.9%",2019
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-MobileNetV2 ×0.35","Xiyang Dai","Microsoft",2019-12-07,"64.9%",2019
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-MobileNetV2 ×0.35","Mengchen Liu","Microsoft",2019-12-07,"64.9%",2019
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-MobileNetV2 ×0.35","Dongdong Chen","Microsoft",2019-12-07,"64.9%",2019
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-MobileNetV2 ×0.35","Lu Yuan","Microsoft",2019-12-07,"64.9%",2019
"Dynamic Convolution: Attention over Convolution Kernels@@@DY-MobileNetV2 ×0.35","Zicheng Liu","Microsoft",2019-12-07,"64.9%",2019
"Visualizing and Understanding Convolutional Networks@@@ZFNet (ensemble, 6 convnets)","Matthew D. Zeiler","New York University",2013-11-12,"64%",2013
"Visualizing and Understanding Convolutional Networks@@@ZFNet (ensemble, 6 convnets)","Rob Fergus","New York University",2013-11-12,"64%",2013
"ImageNet Classification with Deep Convolutional Neural Networks@@@AlexNet","Alex Krizhevsky","University of Toronto",2012-12-01,"63.3%",2012
"ImageNet Classification with Deep Convolutional Neural Networks@@@AlexNet","Ilya Sutskever","University of Toronto",2012-12-01,"63.3%",2012
"ImageNet Classification with Deep Convolutional Neural Networks@@@AlexNet","Geoffrey E. Hinton","University of Toronto",2012-12-01,"63.3%",2012
"Balanced Binary Neural Networks with Gated Residual@@@BBG (ResNet-34)","Mingzhu Shen","Beihang University",2019-09-26,"62.6%",2019
"Balanced Binary Neural Networks with Gated Residual@@@BBG (ResNet-34)","Xianglong Liu","",2019-09-26,"62.6%",2019
"Balanced Binary Neural Networks with Gated Residual@@@BBG (ResNet-34)","Ruihao Gong","",2019-09-26,"62.6%",2019
"Balanced Binary Neural Networks with Gated Residual@@@BBG (ResNet-34)","Kai Han","",2019-09-26,"62.6%",2019
"Balanced Binary Neural Networks with Gated Residual@@@BBG (ResNet-34)","Yunhe Wang","",2019-09-26,"62.6%",2019
"Balanced Binary Neural Networks with Gated Residual@@@BBG (ResNet-34)","Chang Xu","",2019-09-26,"62.6%",2019
"Visualizing and Understanding Convolutional Networks@@@ZFNet (1 convnet, 512,1024,512 maps)","Matthew D. Zeiler","New York University",2013-11-12,"62.5%",2013
"Visualizing and Understanding Convolutional Networks@@@ZFNet (1 convnet, 512,1024,512 maps)","Rob Fergus","New York University",2013-11-12,"62.5%",2013
"Balanced Binary Neural Networks with Gated Residual@@@BBG (ResNet-18)","Mingzhu Shen","Beihang University",2019-09-26,"59.4%",2019
"Balanced Binary Neural Networks with Gated Residual@@@BBG (ResNet-18)","Xianglong Liu","Beihang University",2019-09-26,"59.4%",2019
"Balanced Binary Neural Networks with Gated Residual@@@BBG (ResNet-18)","Ruihao Gong","",2019-09-26,"59.4%",2019
"Balanced Binary Neural Networks with Gated Residual@@@BBG (ResNet-18)","Kai Han","",2019-09-26,"59.4%",2019
"Balanced Binary Neural Networks with Gated Residual@@@BBG (ResNet-18)","Yunhe Wang","",2019-09-26,"59.4%",2019
"Balanced Binary Neural Networks with Gated Residual@@@BBG (ResNet-18)","Chang Xu","",2019-09-26,"59.4%",2019
"@@@SIFT + FVs","","",NA,"50.9%",NA
"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification@@@PReLU-Net","Kaiming He","Microsoft",2015-02-06,"not available",2015
"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification@@@PReLU-Net","Xiangyu Zhang","Xi'an Jiaotong University",2015-02-06,"not available",2015
"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification@@@PReLU-Net","Shaoqing Ren","Microsoft",2015-02-06,"not available",2015
"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification@@@PReLU-Net","Jian Sun","Microsoft",2015-02-06,"not available",2015
"Look-into-Object: Self-supervised Structure Modeling for Object Recognition@@@LIO/ResNet-50 (multi-stage)","Mohan Zhou","",2020-03-31,"not available",2020
"Look-into-Object: Self-supervised Structure Modeling for Object Recognition@@@LIO/ResNet-50 (multi-stage)","Yalong Bai","Harbin Institute of Technology",2020-03-31,"not available",2020
"Look-into-Object: Self-supervised Structure Modeling for Object Recognition@@@LIO/ResNet-50 (multi-stage)","Wei Zhang","Chinese Academy of Sciences",2020-03-31,"not available",2020
"Look-into-Object: Self-supervised Structure Modeling for Object Recognition@@@LIO/ResNet-50 (multi-stage)","Tiejun Zhao","Harbin Institute of Technology",2020-03-31,"not available",2020
"Look-into-Object: Self-supervised Structure Modeling for Object Recognition@@@LIO/ResNet-50 (multi-stage)","Zhao Tiejun","Harbin Institute of Technology",2020-03-31,"not available",2020
"Look-into-Object: Self-supervised Structure Modeling for Object Recognition@@@LIO/ResNet-50 (multi-stage)","Tao Mei","Association for Computing Machinery",2020-03-31,"not available",2020
"Knapsack Pruning with Inner Distillation@@@ResNet50 2.5 GFLOPS","Yonathan Aflalo","",2020-02-19,"not available",2020
"Knapsack Pruning with Inner Distillation@@@ResNet50 2.5 GFLOPS","Asaf Noy","",2020-02-19,"not available",2020
"Knapsack Pruning with Inner Distillation@@@ResNet50 2.5 GFLOPS","Ming Lin","",2020-02-19,"not available",2020
"Knapsack Pruning with Inner Distillation@@@ResNet50 2.5 GFLOPS","Itamar Friedman","",2020-02-19,"not available",2020
"Knapsack Pruning with Inner Distillation@@@ResNet50 2.5 GFLOPS","Lihi Zelnik-Manor","",2020-02-19,"not available",2020
"Knapsack Pruning with Inner Distillation@@@ResNet50 2.5 GFLOPS","Lihi Zelnik","",2020-02-19,"not available",2020
"Knapsack Pruning with Inner Distillation@@@ResNet50 2.0 GFLOPS","Yonathan Aflalo","",2020-02-19,"not available",2020
"Knapsack Pruning with Inner Distillation@@@ResNet50 2.0 GFLOPS","Asaf Noy","",2020-02-19,"not available",2020
"Knapsack Pruning with Inner Distillation@@@ResNet50 2.0 GFLOPS","Ming Lin","",2020-02-19,"not available",2020
"Knapsack Pruning with Inner Distillation@@@ResNet50 2.0 GFLOPS","Itamar Friedman","",2020-02-19,"not available",2020
"Knapsack Pruning with Inner Distillation@@@ResNet50 2.0 GFLOPS","Lihi Zelnik-Manor","",2020-02-19,"not available",2020
"Knapsack Pruning with Inner Distillation@@@ResNet50 2.0 GFLOPS","Lihi Zelnik","",2020-02-19,"not available",2020
"EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning@@@ResNet50-3G FLOPs","Bailin Li","",2020-07-06,"not available",2020
"EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning@@@ResNet50-3G FLOPs","Bowen Wu","Sun Yat-sen University",2020-07-06,"not available",2020
"EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning@@@ResNet50-3G FLOPs","Jiang Su","Sun Yat-sen University",2020-07-06,"not available",2020
"EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning@@@ResNet50-3G FLOPs","Guangrun Wang","",2020-07-06,"not available",2020
"EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning@@@ResNet50-3G FLOPs","Liang Lin","",2020-07-06,"not available",2020
"EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning@@@ResNet50-2G FLOPs","Bailin Li","",2020-07-06,"not available",2020
"EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning@@@ResNet50-2G FLOPs","Bowen Wu","Sun Yat-sen University",2020-07-06,"not available",2020
"EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning@@@ResNet50-2G FLOPs","Jiang Su","Sun Yat-sen University",2020-07-06,"not available",2020
"EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning@@@ResNet50-2G FLOPs","Guangrun Wang","",2020-07-06,"not available",2020
"EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning@@@ResNet50-2G FLOPs","Liang Lin","",2020-07-06,"not available",2020
"Network Pruning via Transformable Architecture Search@@@TAS-pruned ResNet-50","Xuanyi Dong","University of Technology, Sydney",2019-05-23,"not available",2019
"Network Pruning via Transformable Architecture Search@@@TAS-pruned ResNet-50","Yi Yang","University of Technology, Sydney",2019-05-23,"not available",2019
"EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning@@@ResNet50-1G FLOPs","Bailin Li","",2020-07-06,"not available",2020
"EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning@@@ResNet50-1G FLOPs","Bowen Wu","Sun Yat-sen University",2020-07-06,"not available",2020
"EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning@@@ResNet50-1G FLOPs","Jiang Su","Sun Yat-sen University",2020-07-06,"not available",2020
"EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning@@@ResNet50-1G FLOPs","Guangrun Wang","",2020-07-06,"not available",2020
"EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning@@@ResNet50-1G FLOPs","Liang Lin","",2020-07-06,"not available",2020
"EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning@@@ResNet50-1G FLOPs","Bailin Li","",2020-07-06,"not available",2020
"EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning@@@ResNet50-1G FLOPs","Bowen Wu","Sun Yat-sen University",2020-07-06,"not available",2020
"EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning@@@ResNet50-1G FLOPs","Jiang Su","Sun Yat-sen University",2020-07-06,"not available",2020
"EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning@@@ResNet50-1G FLOPs","Guangrun Wang","",2020-07-06,"not available",2020
"EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning@@@ResNet50-1G FLOPs","Liang Lin","",2020-07-06,"not available",2020
"EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning@@@MobileNetV1-50% FLOPs","Bailin Li","",2020-07-06,"not available",2020
"EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning@@@MobileNetV1-50% FLOPs","Bowen Wu","Sun Yat-sen University",2020-07-06,"not available",2020
"EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning@@@MobileNetV1-50% FLOPs","Jiang Su","Sun Yat-sen University",2020-07-06,"not available",2020
"EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning@@@MobileNetV1-50% FLOPs","Guangrun Wang","",2020-07-06,"not available",2020
"EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning@@@MobileNetV1-50% FLOPs","Liang Lin","",2020-07-06,"not available",2020
"HMQ: Hardware Friendly Mixed Precision Quantization Block for CNNs@@@EfficientNet-B0-W8A8","Hai Victor Habi","",2020-07-20,"not available",2020
"HMQ: Hardware Friendly Mixed Precision Quantization Block for CNNs@@@EfficientNet-B0-W8A8","Roy H. Jennings","",2020-07-20,"not available",2020
"HMQ: Hardware Friendly Mixed Precision Quantization Block for CNNs@@@EfficientNet-B0-W8A8","Arnon Netzer","",2020-07-20,"not available",2020
"HMQ: Hardware Friendly Mixed Precision Quantization Block for CNNs@@@EfficientNet-B0-W4A4","Hai Victor Habi","",2020-07-20,"not available",2020
"HMQ: Hardware Friendly Mixed Precision Quantization Block for CNNs@@@EfficientNet-B0-W4A4","Roy H. Jennings","",2020-07-20,"not available",2020
"HMQ: Hardware Friendly Mixed Precision Quantization Block for CNNs@@@EfficientNet-B0-W4A4","Arnon Netzer","",2020-07-20,"not available",2020
"HMQ: Hardware Friendly Mixed Precision Quantization Block for CNNs@@@ResNet50-W3A4","Hai Victor Habi","",2020-07-20,"not available",2020
"HMQ: Hardware Friendly Mixed Precision Quantization Block for CNNs@@@ResNet50-W3A4","Roy H. Jennings","",2020-07-20,"not available",2020
"HMQ: Hardware Friendly Mixed Precision Quantization Block for CNNs@@@ResNet50-W3A4","Arnon Netzer","",2020-07-20,"not available",2020
"LSQ+: Improving low-bit quantization through learnable offsets and better initialization@@@EfficientNet-W4A4","Yash Bhalgat","Qualcomm",2020-04-20,"not available",2020
"LSQ+: Improving low-bit quantization through learnable offsets and better initialization@@@EfficientNet-W4A4","Jinwon Lee","Qualcomm",2020-04-20,"not available",2020
"LSQ+: Improving low-bit quantization through learnable offsets and better initialization@@@EfficientNet-W4A4","Markus Nagel","Qualcomm",2020-04-20,"not available",2020
"LSQ+: Improving low-bit quantization through learnable offsets and better initialization@@@EfficientNet-W4A4","Tijmen Blankevoort","Qualcomm",2020-04-20,"not available",2020
"LSQ+: Improving low-bit quantization through learnable offsets and better initialization@@@EfficientNet-W4A4","Nojun Kwak","Seoul National University",2020-04-20,"not available",2020
"LSQ+: Improving low-bit quantization through learnable offsets and better initialization@@@MixNet-W4A4","Yash Bhalgat","Qualcomm",2020-04-20,"not available",2020
"LSQ+: Improving low-bit quantization through learnable offsets and better initialization@@@MixNet-W4A4","Jinwon Lee","Qualcomm",2020-04-20,"not available",2020
"LSQ+: Improving low-bit quantization through learnable offsets and better initialization@@@MixNet-W4A4","Markus Nagel","Qualcomm",2020-04-20,"not available",2020
"LSQ+: Improving low-bit quantization through learnable offsets and better initialization@@@MixNet-W4A4","Tijmen Blankevoort","Qualcomm",2020-04-20,"not available",2020
"LSQ+: Improving low-bit quantization through learnable offsets and better initialization@@@MixNet-W4A4","Nojun Kwak","Seoul National University",2020-04-20,"not available",2020
"HMQ: Hardware Friendly Mixed Precision Quantization Block for CNNs@@@MobileNetV2","Hai Victor Habi","",2020-07-20,"not available",2020
"HMQ: Hardware Friendly Mixed Precision Quantization Block for CNNs@@@MobileNetV2","Roy H. Jennings","",2020-07-20,"not available",2020
"HMQ: Hardware Friendly Mixed Precision Quantization Block for CNNs@@@MobileNetV2","Arnon Netzer","",2020-07-20,"not available",2020
"Merging $K$-means with hierarchical clustering for identifying general-shaped groups@@@means","Anna D. Peterson","Iowa State University",2017-12-23,"not available",2017
"Merging $K$-means with hierarchical clustering for identifying general-shaped groups@@@means","Arka P. Ghosh","Iowa State University",2017-12-23,"not available",2017
"Merging $K$-means with hierarchical clustering for identifying general-shaped groups@@@means","Ranjan Maitra","Iowa State University",2017-12-23,"not available",2017
"Rigging the Lottery: Making All Tickets Winners@@@Resnet-50: 80% Sparse","Utku Evci","Google",2019-11-25,"not available",2019
"Rigging the Lottery: Making All Tickets Winners@@@Resnet-50: 80% Sparse","Trevor Gale","Google",2019-11-25,"not available",2019
"Rigging the Lottery: Making All Tickets Winners@@@Resnet-50: 80% Sparse","Jacob Menick","",2019-11-25,"not available",2019
"Rigging the Lottery: Making All Tickets Winners@@@Resnet-50: 80% Sparse","Pablo Samuel Castro","Google",2019-11-25,"not available",2019
"Rigging the Lottery: Making All Tickets Winners@@@Resnet-50: 80% Sparse","Erich Elsen","Google",2019-11-25,"not available",2019
"Rigging the Lottery: Making All Tickets Winners@@@Resnet-50: 90% Sparse","Utku Evci","Google",2019-11-25,"not available",2019
"Rigging the Lottery: Making All Tickets Winners@@@Resnet-50: 90% Sparse","Trevor Gale","Google",2019-11-25,"not available",2019
"Rigging the Lottery: Making All Tickets Winners@@@Resnet-50: 90% Sparse","Jacob Menick","",2019-11-25,"not available",2019
"Rigging the Lottery: Making All Tickets Winners@@@Resnet-50: 90% Sparse","Pablo Samuel Castro","Google",2019-11-25,"not available",2019
"Rigging the Lottery: Making All Tickets Winners@@@Resnet-50: 90% Sparse","Erich Elsen","Google",2019-11-25,"not available",2019
"Rigging the Lottery: Making All Tickets Winners@@@MobileNet-v1: 75% Sparse","Utku Evci","Google",2019-11-25,"not available",2019
"Rigging the Lottery: Making All Tickets Winners@@@MobileNet-v1: 75% Sparse","Trevor Gale","Google",2019-11-25,"not available",2019
"Rigging the Lottery: Making All Tickets Winners@@@MobileNet-v1: 75% Sparse","Jacob Menick","",2019-11-25,"not available",2019
"Rigging the Lottery: Making All Tickets Winners@@@MobileNet-v1: 75% Sparse","Pablo Samuel Castro","Google",2019-11-25,"not available",2019
"Rigging the Lottery: Making All Tickets Winners@@@MobileNet-v1: 75% Sparse","Erich Elsen","Google",2019-11-25,"not available",2019
"Rigging the Lottery: Making All Tickets Winners@@@MobileNet-v1: 90% Sparse","Utku Evci","Google",2019-11-25,"not available",2019
"Rigging the Lottery: Making All Tickets Winners@@@MobileNet-v1: 90% Sparse","Trevor Gale","Google",2019-11-25,"not available",2019
"Rigging the Lottery: Making All Tickets Winners@@@MobileNet-v1: 90% Sparse","Jacob Menick","",2019-11-25,"not available",2019
"Rigging the Lottery: Making All Tickets Winners@@@MobileNet-v1: 90% Sparse","Pablo Samuel Castro","Google",2019-11-25,"not available",2019
"Rigging the Lottery: Making All Tickets Winners@@@MobileNet-v1: 90% Sparse","Erich Elsen","Google",2019-11-25,"not available",2019
