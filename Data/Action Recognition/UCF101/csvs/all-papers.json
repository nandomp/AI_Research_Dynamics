[{"title": "Late Temporal Modeling in 3D CNN Architectures with BERT for Action Recognition@@@R2+1D-BERT", "authors": ["M. Esat Kalfaoglu", "Sinan Kalkan", "A. Aydin Alatan"], "affiliations": ["Middle East Technical University", "Middle East Technical University", "Middle East Technical University"], "paper_date": "2020-08-03", "metric": "98.69"}, {"title": "Omni-sourced Webly-supervised Learning for Video Recognition@@@OmniSource (SlowOnly-8x8-R101-RGB + I3D-Flow)", "authors": ["Haodong Duan", "Yue Zhao", "Yuanjun Xiong", "Wentao Liu", "Dahua Lin"], "affiliations": ["", "", "", "", ""], "paper_date": "2020-03-29", "metric": "98.6"}, {"title": "Learning Spatio-Temporal Representation with Local and Global Diffusion@@@LGD-3D Two-stream", "authors": ["Zhaofan Qiu", "Ting Yao", "Chong-Wah Ngo", "Xinmei Tian", "Tao Mei"], "affiliations": ["University of Science and Technology of China", "", "City University of Hong Kong", "", ""], "paper_date": "2019-06-13", "metric": "98.2"}, {"title": "Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset@@@Two-stream I3D (on pre-trained)", "authors": ["Joao Carreira", "Andrew Zisserman"], "affiliations": ["Google", "University of Oxford"], "paper_date": "2017-05-22", "metric": "98.0"}, {"title": "MARS: Motion-Augmented RGB Stream for Action Recognition@@@MARS+RGB+Flow (64 frames, Kinetics pretrained)", "authors": ["Nieves Crasto", "Philippe Weinzaepfel", "Karteek Alahari", "Cordelia Schmid"], "affiliations": ["", "", "", ""], "paper_date": "2019-06-01", "metric": "97.8"}, {"title": "Large Scale Holistic Video Understanding@@@HATNet (32 frames)", "authors": ["Ali Diba", "Mohsen Fayyaz", "Vivek Sharma", "Manohar Paluri", "J\u00fcrgen Gall", "Rainer Stiefelhagen", "Luc Van Gool"], "affiliations": ["", "", "", "", "ETH Zurich", "", ""], "paper_date": "2019-04-25", "metric": "97.8"}, {"title": "Cooperative Cross-Stream Network for Discriminative Action Representation@@@CCS + TSN (ImageNet+Kinetics pretrained)", "authors": ["Jingran Zhang", "Fumin Shen", "Xing Xu", "Heng Tao Shen"], "affiliations": ["", "", "University of Electronic Science and Technology of China", ""], "paper_date": "2019-08-27", "metric": "97.4"}, {"title": "Contextual Action Cues from Camera Sensor for Multi-Stream Action Recognition@@@Multi-stream I3D ", "authors": ["Jongkwang Hong", "Bora Cho", "Yong Won Hong", "Hyeran Byun"], "affiliations": ["Yonsei University", "Yonsei University", "Yonsei University", "Yonsei University"], "paper_date": "2019-03-20", "metric": "97.2"}, {"title": "Hidden Two-Stream Convolutional Networks for Action Recognition@@@Hidden Two-Stream", "authors": ["Yi Zhu", "Zhenzhong Lan", "Shawn Newsam", "Alexander G. Hauptmann"], "affiliations": ["University of California, Merced", "Carnegie Mellon University", "University of California, Merced", "Carnegie Mellon University"], "paper_date": "2017-04-02", "metric": "97.1"}, {"title": "Two-Stream Video Classification with Cross-Modality Attention@@@CMA iter1-S", "authors": ["Lu Chi", "Guiyu Tian", "Yadong Mu", "Qi Tian"], "affiliations": ["Peking University", "Peking University", "Peking University", "Huawei"], "paper_date": "2019-08-01", "metric": "96.5"}, {"title": "STM: SpatioTemporal and Motion Encoding for Action Recognition@@@STM (ImageNet+Kinetics pretrain)", "authors": ["Boyuan Jiang", "Mengmeng Wang", "Weihao Gan", "Wei Wu", "Junjie Yan"], "affiliations": ["Zhejiang University", "SenseTime", "SenseTime", "SenseTime", "SenseTime"], "paper_date": "2019-08-07", "metric": "96.2"}, {"title": "Multi-Fiber Networks for Video Recognition@@@MF-Net, RGB only (ImageNet+Kinetics pretrained)", "authors": ["Yunpeng Chen", "Yannis Kalantidis", "Jianshu Li", "Shuicheng Yan", "Jiashi Feng"], "affiliations": ["National University of Singapore", "Facebook", "National University of Singapore", "National University of Singapore", "National University of Singapore"], "paper_date": "2018-07-30", "metric": "96.0"}, {"title": "Optical Flow Guided Feature: A Fast and Robust Motion Representation for Video Action Recognition@@@Optical Flow Guided Feature", "authors": ["Shuyang Sun", "Zhanghui Kuang", "Lu Sheng", "Wanli Ouyang", "Wei Zhang"], "affiliations": ["University of Sydney", "", "The Chinese University of Hong Kong", "University of Sydney", "SenseTime"], "paper_date": "2017-11-29", "metric": "96"}, {"title": "MARS: Motion-Augmented RGB Stream for Action Recognition@@@MARS+RGB+Flow (16 frames)", "authors": ["Nieves Crasto", "Philippe Weinzaepfel", "Karteek Alahari", "Cordelia Schmid"], "affiliations": ["", "", "", ""], "paper_date": "2019-06-01", "metric": "95.8"}, {"title": "Attention Distillation for Learning Video Representations@@@Prob-Distill", "authors": ["Miao Liu", "Xin Chen", "Yun Zhang", "Yin Li", "James M. Rehg"], "affiliations": ["Georgia Institute of Technology", "", "Georgia Institute of Technology", "University of Wisconsin-Madison", "Georgia Institute of Technology"], "paper_date": "2019-04-05", "metric": "95.7"}, {"title": "Learning spatio-temporal representations with temporal squeeze pooling@@@TesNet (ImageNet pretrained)", "authors": ["Guoxi Huang", "Adrian G. Bors"], "affiliations": ["University of York", "University of York"], "paper_date": "2020-02-11", "metric": "95.2"}, {"title": "I3D-LSTM: A New Model for Human Action Recognition@@@I3D-LSTM", "authors": ["Xianyuan Wang", "Zhenjiang Miao", "Ruyi Zhang", "Shanshan Hao"], "affiliations": ["", "", "", ""], "paper_date": "2019-08-09", "metric": "95.1"}, {"title": "Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?@@@ResNeXt-101 (64f)", "authors": ["Kensho Hara", "Hirokatsu Kataoka", "Yutaka Satoh"], "affiliations": ["National Institute of Advanced Industrial Science and Technology", "National Institute of Advanced Industrial Science and Technology", "National Institute of Advanced Industrial Science and Technology"], "paper_date": "2017-11-27", "metric": "94.5"}, {"title": "R-STAN: Residual Spatial-Temporal Attention Network for Action Recognition@@@R-STAN-101", "authors": ["Quanle Liu", "Xiangjiu Che", "Mei Bie"], "affiliations": ["Jilin University", "Jilin University", "Jilin University"], "paper_date": "2019-06-19", "metric": "94.5"}, {"title": "Temporal-Spatial Mapping for Action Recognition@@@TSN+TSM", "authors": ["Xiaolin Song", "Cuiling Lan", "Wenjun Zeng", "Junliang Xing", "Xiaoyan Sun", "Jingyu Yang"], "affiliations": ["Tianjin University", "Microsoft", "Microsoft", "Chinese Academy of Sciences", "Microsoft", "Tianjin University"], "paper_date": "2018-09-11", "metric": "94.3"}, {"title": "TS-LSTM and Temporal-Inception: Exploiting Spatiotemporal Dynamics for Activity Recognition@@@TS-LSTM", "authors": ["Chih-Yao Ma", "Min-Hung Chen", "Zsolt Kira", "Ghassan AlRegib"], "affiliations": ["Georgia Institute of Technology", "Georgia Institute of Technology", "Georgia Tech Research Institute", "Georgia Institute of Technology"], "paper_date": "2017-03-30", "metric": "94.1"}, {"title": "Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset@@@Two-stream I3D", "authors": ["Joao Carreira", "Andrew Zisserman"], "affiliations": ["Google", "University of Oxford"], "paper_date": "2017-05-22", "metric": "93.4"}, {"title": "Convolutional Two-Stream Network Fusion for Video Action Recognition@@@S:VGG-16, T:VGG-16", "authors": ["Christoph Feichtenhofer", "Axel Pinz", "Andrew Zisserman"], "affiliations": ["Graz University of Technology", "Graz University of Technology", "University of Oxford"], "paper_date": "2016-04-22", "metric": "92.5"}, {"title": "Dance with Flow: Two-in-One Stream Action Detection@@@two-in-one two stream", "authors": ["Jiaojiao Zhao", "Cees G. M. Snoek"], "affiliations": ["University of Amsterdam", "University of Amsterdam"], "paper_date": "2019-04-01", "metric": "92"}, {"title": "Long-term Temporal Convolutions for Action Recognition@@@LTC", "authors": ["G\u00fcl Varol", "Ivan Laptev", "Cordelia Schmid"], "affiliations": ["\u00c9cole Normale Sup\u00e9rieure", "\u00c9cole Normale Sup\u00e9rieure", "French Institute for Research in Computer Science and Automation"], "paper_date": "2016-04-15", "metric": "91.7"}, {"title": "R-STAN: Residual Spatial-Temporal Attention Network for Action Recognition@@@R-STAN-50", "authors": ["Quanle Liu", "Xiangjiu Che", "Mei Bie"], "affiliations": ["Jilin University", "Jilin University", "Jilin University"], "paper_date": "2019-06-19", "metric": "91.5"}, {"title": "Towards Good Practices for Very Deep Two-Stream ConvNets@@@Very deep two-stream ConvNet", "authors": ["Limin Wang", "Yuanjun Xiong", "Zhe Wang", "Yu Qiao"], "affiliations": ["", "", "", "Chinese Academy of Sciences"], "paper_date": "2015-07-08", "metric": "91.4"}, {"title": "Multi-region two-stream R-CNN for action detection@@@MR Two-Sream R-CNN", "authors": ["Xiaojiang Peng", "Cordelia Schmid"], "affiliations": ["", ""], "paper_date": "2016-09-17", "metric": "91.1"}, {"title": "Beyond Short Snippets: Deep Networks for Video Classification@@@Two-stream+LSTM", "authors": ["Joe Yue-Hei Ng", "Matthew Hausknecht", "Sudheendra Vijayanarasimhan", "Oriol Vinyals", "Rajat Monga", "George Toderici"], "affiliations": ["University of Maryland, College Park", "University of Texas at Austin", "Google", "Google", "Google", "Google"], "paper_date": "2015-03-31", "metric": "88.6"}, {"title": "Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks@@@P3D", "authors": ["Zhaofan Qiu", "Ting Yao", "Tao Mei"], "affiliations": ["University of Science and Technology of China", "Microsoft", "Microsoft"], "paper_date": "2017-11-28", "metric": "88.6"}, {"title": "Two-Stream Convolutional Networks for Action Recognition in Videos@@@Two-stream", "authors": ["Karen Simonyan", "Andrew Zisserman"], "affiliations": ["University of Oxford", "University of Oxford"], "paper_date": "2014-06-09", "metric": "88.0"}, {"title": "Bag of Visual Words and Fusion Methods for Action Recognition: Comprehensive Study and Good Practice@@@iDT+HSV", "authors": ["Xiaojiang Peng", "Limin Wang", "Xingxing Wang", "Yu Qiao"], "affiliations": ["", "", "", ""], "paper_date": "2014-05-18", "metric": "87.9"}, {"title": "DistInit: Learning Video Representations Without a Single Labeled Video@@@R(2+1)D-18 (DistInit pretraining)", "authors": ["Rohit Girdhar", "Du Tran", "Lorenzo Torresani", "Deva Ramanan"], "affiliations": ["Carnegie Mellon University", "Facebook", "Facebook", "Carnegie Mellon University"], "paper_date": "2019-01-26", "metric": "85.8"}, {"title": "Resource Efficient 3D Convolutional Neural Networks@@@3D-SqueezeNet", "authors": ["Okan K\u00f6p\u00fckl\u00fc", "Neslihan Kose", "Ahmet Gunduz", "Gerhard Rigoll"], "affiliations": ["Technische Universit\u00e4t M\u00fcnchen", "Intel", "Technische Universit\u00e4t M\u00fcnchen", "Technische Universit\u00e4t M\u00fcnchen"], "paper_date": "2019-04-04", "metric": "74.94"}, {"title": "Large-Scale Video Classification with Convolutional Neural Networks@@@Slow Fusion + Finetune top 3 layers", "authors": ["Andrej Karpathy", "George Toderici", "Sanketh Shetty", "Thomas Leung", "Rahul Sukthankar", "Li Fei-Fei"], "affiliations": ["Stanford University", "Stanford University", "Stanford University", "Stanford University", "Stanford University", "Stanford University"], "paper_date": "2014-06-23", "metric": "65.4"}, {"title": "MLGCN: Multi-Laplacian Graph Convolutional Networks for Human Action Recognition@@@MLGCN", "authors": ["Ahmed Mazari", "Hichem Sahbi"], "affiliations": ["", ""], "paper_date": "2019-09-11", "metric": "63.27"}, {"title": "Self-supervised Spatio-temporal Representation Learning for Videos by Predicting Motion and Appearance Statistics@@@Pretrained on Kinetics", "authors": ["Jiangliu Wang", "Jianbo Jiao", "Linchao Bao", "Shengfeng He", "Yunhui Liu", "Yun-Hui Liu", "Wei Liu"], "affiliations": ["The Chinese University of Hong Kong", "University of Oxford", "Tencent", "South China University of Technology", "The Chinese University of Hong Kong", "The Chinese University of Hong Kong", "Tencent"], "paper_date": "2019-04-07", "metric": "61.2"}, {"title": "Resource Efficient 3D Convolutional Neural Networks@@@3D-ShuffleNetV2 0.25x", "authors": ["Okan K\u00f6p\u00fckl\u00fc", "Neslihan Kose", "Ahmet Gunduz", "Gerhard Rigoll"], "affiliations": ["Technische Universit\u00e4t M\u00fcnchen", "Intel", "Technische Universit\u00e4t M\u00fcnchen", "Technische Universit\u00e4t M\u00fcnchen"], "paper_date": "2019-04-04", "metric": "56.52"}, {"title": "Resource Efficient 3D Convolutional Neural Networks@@@3D-MobileNetV2 0.2x", "authors": ["Okan K\u00f6p\u00fckl\u00fc", "Neslihan Kose", "Ahmet Gunduz", "Gerhard Rigoll"], "affiliations": ["Technische Universit\u00e4t M\u00fcnchen", "Intel", "Technische Universit\u00e4t M\u00fcnchen", "Technische Universit\u00e4t M\u00fcnchen"], "paper_date": "2019-04-04", "metric": "55.56"}, {"title": "UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild@@@Baseline UCF101", "authors": ["Khurram Soomro", "Amir Roshan Zamir", "Mubarak Shah"], "affiliations": ["", "", ""], "paper_date": "2012-12-03", "metric": "43.9"}, {"title": "Towards Universal Representation for Unseen Action Recognition@@@CD-UAR", "authors": ["Yi Zhu", "Yang Long", "Yu Guan", "Shawn Newsam", "Ling Shao"], "affiliations": ["University of California, Merced", "Newcastle University", "Newcastle University", "University of California, Merced", "University of East Anglia"], "paper_date": "2018-03-22", "metric": "42.5"}, {"title": "PoTion: Pose MoTion Representation for Action Recognition@@@I3D + PoTion", "authors": ["Vasileios Choutas", "Philippe Weinzaepfel", "Jerome Revaud", "Cordelia Schmid"], "affiliations": ["", "", "", "French Institute for Research in Computer Science and Automation"], "paper_date": "2018-06-01", "metric": "29.3"}]